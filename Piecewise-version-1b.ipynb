{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Package Import and Simple Module definition\n",
    "\n",
    "Modified from ZFTurbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodolfoxps/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.io import loadmat\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import Process\n",
    "import copy\n",
    "\n",
    "#Importing old and new Kfold\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold as NewKF\n",
    "\n",
    "#Importing GroupKfold, only available since version 0.18\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "#Importing function for scaling data before PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Importing PCA packages\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "#Importing FFT package\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "#Importing crossvalidation metrics and Gridsearch\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Importing wrapper to use XGB with Gridsearch\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#Importing plotting packages (optional)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "#Oversampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "\n",
    "#Defining general modules used in the classification\n",
    "\n",
    "random.seed(2016)\n",
    "np.random.seed(2016)\n",
    "\n",
    "\n",
    "def natural_key(string_):\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_)]\n",
    "\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def get_importance(gbm, features):\n",
    "    create_feature_map(features)\n",
    "    importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "    return importance\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "\n",
    "def print_features_importance(imp):\n",
    "    for i in range(len(imp)):\n",
    "        print(\"# \" + str(imp[i][1]))\n",
    "        print('output.remove(\\'' + imp[i][0] + '\\')')\n",
    "\n",
    "\n",
    "def mat_to_pandas(path):\n",
    "    mat = loadmat(path)\n",
    "    names = mat['dataStruct'].dtype.names\n",
    "    ndata = {n: mat['dataStruct'][n][0, 0] for n in names}\n",
    "    sequence = -1\n",
    "    if 'sequence' in names:\n",
    "        sequence = mat['dataStruct']['sequence']\n",
    "    return pd.DataFrame(ndata['data'], columns=ndata['channelIndices'][0]), sequence\n",
    "\n",
    "def create_submission(score, test, prediction):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('File,Class\\n')\n",
    "    total = 0\n",
    "    for id in test['Id']:\n",
    "        patient = id // 100000\n",
    "        fid = id % 100000\n",
    "        str1 = str(patient) + '_' + str(fid) + '.mat' + ',' + str(prediction[total])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values)\n",
    "    testval = list(test.columns.values)\n",
    "    output = intersect(trainval, testval)\n",
    "    output.remove('Id')\n",
    "    # output.remove('file_size')\n",
    "    return sorted(output)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Creating Features and Saving to CSV Files\n",
    "\n",
    "#### One file per patient per test/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modules to read train and test data.\n",
    "#Short_dataset can be False or TRue. It decides whether to use the lon or short sample size.\n",
    "\n",
    "def create_simple_csv_train(patient_id, feature_model, short_dataset=False):\n",
    "    \n",
    "    feature_file='_'+str(feature_model)+'_'+'short'+'_'+str(short_dataset)\n",
    "    \n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/train_\"\n",
    "    else:\n",
    "        source_dir=\"./train_\"\n",
    "        \n",
    "    \n",
    "\n",
    "    out = open(\"simple_train_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,sequence_id,patient_id\")\n",
    "    for i in range(16):\n",
    "        out.write(\",avg_\" + str(i))\n",
    "    out.write(\",file_size,result\\n\")\n",
    "\n",
    "    # TRAIN (0)\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "    print ('train files'+ str(patient_id), len(files))    \n",
    "    pos1=0\n",
    "    neg1=0\n",
    "    sequence_id = int(patient_id)*1000\n",
    "    total = 0\n",
    "    seq1=0\n",
    "    for fl in files:\n",
    "        total += 1\n",
    "        # print('Go for ' + fl)\n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        result = int(arr[2])\n",
    "        new_id = patient*100000 + id\n",
    "        try:\n",
    "            tables, sequence_from_mat = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        out_str += str(new_id) + \",\" + str(sequence_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])       \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f],out_str,feature_model, sizesignal)\n",
    "            \n",
    "            \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \",\" + str(result) + \"\\n\"\n",
    "        #print(sequence_from_mat)\n",
    "        #print(type(sequence_from_mat))\n",
    "        seq1=int(sequence_from_mat[0][0][0][0])\n",
    "        print(total, seq1)\n",
    "        if (total % 6 == 0) and (seq1==6):\n",
    "            if result != 0:\n",
    "                pos1 += 1\n",
    "                print('Positive ocurrence sequence finished', pos1)\n",
    "            else:\n",
    "                neg1 += 1\n",
    "                print('Negative ocurrence sequence finished', neg1)\n",
    "                \n",
    "            sequence_id += 1\n",
    "            print ('sequence',sequence_id)\n",
    "\n",
    "    out.write(out_str)\n",
    "    \n",
    "    out.close()\n",
    "    print('Train CSV for patient {} has been completed...'.format(patient_id))\n",
    "\n",
    "\n",
    "def create_simple_csv_test(patient_id, feature_model, short_dataset=False):\n",
    "    \n",
    "    feature_file='_'+str(feature_model)+'_'+'short'+'_'+str(short_dataset)\n",
    "    \n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/test_\"\n",
    "    else:\n",
    "        source_dir=\"./test_\"\n",
    "\n",
    "    # TEST\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "    print ('test files'+ str(patient_id), len(files))    \n",
    "    out = open(\"simple_test_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,patient_id\")\n",
    "    for i in range(16):\n",
    "        out.write(\",avg_\" + str(i))\n",
    "    out.write(\",file_size\\n\")\n",
    "    for fl in files:\n",
    "        # print('Go for ' + fl)\n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        new_id = patient*100000 + id\n",
    "        try:\n",
    "            tables, sequence_from_mat = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        out_str += str(new_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f],out_str,feature_model, sizesignal)\n",
    "                        \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \"\\n\"\n",
    "        # break\n",
    "\n",
    "    out.write(out_str)\n",
    "    out.close()\n",
    "    print('Test CSV for patient {} has been completed...'.format(patient_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#eng_number is the feature_value that has values 0,1,2,3... infinite, given by the list below.\n",
    "\n",
    "def feature_eng(data_sensor, out_str, eng_number, sizesignal):\n",
    "\n",
    "                \n",
    "    yf1 = fft(data_sensor)\n",
    "    fftpeak=2/sizesignal * np.abs(yf1[0:sizesignal/2])\n",
    " \n",
    "    numberofbands=4\n",
    "\n",
    "    sizeband=20/numberofbands\n",
    "\n",
    "\n",
    "    if eng_number==2:\n",
    "        \n",
    "        mean = data_sensor.mean()\n",
    "        \n",
    "        peak1=fftpeak[0:3].mean()            \n",
    "        peak2=fftpeak[3:6].mean()          \n",
    "        peak3=fftpeak[6:9].mean()\n",
    "        peak4=fftpeak[9:12].mean()\n",
    "        peak5=fftpeak[12:15].mean()            \n",
    "        peak6=fftpeak[15:18].mean()          \n",
    "        peak7=fftpeak[18:21].mean()\n",
    "        peak8=fftpeak[21:24].mean()\n",
    "        peak9=fftpeak[24:27].mean()            \n",
    "        peak10=fftpeak[27:30].mean()          \n",
    "        peak11=fftpeak[30:33].mean()\n",
    "        peak12=fftpeak[33:36].mean()\n",
    "            \n",
    "        out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4) \\\n",
    "                    +\",\" + str(peak5) + \",\" + str(peak6) + \",\" + str(peak7) +\",\" + str(peak8)+ \",\" + str(peak9) \\\n",
    "                    +\",\" + str(peak10) + \",\" + str(peak11) +\",\" + str(peak12)\n",
    "    \n",
    "    elif eng_number==1:\n",
    "            \n",
    "        mean = data_sensor.mean()   \n",
    "        \n",
    "        peak1=fftpeak[0:5].mean()            \n",
    "        peak2=fftpeak[5:10].mean()          \n",
    "        peak3=fftpeak[10:15].mean()\n",
    "        peak4=fftpeak[15:20].mean()\n",
    "        \n",
    "        out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4)\n",
    "    \n",
    "    elif eng_number==0:\n",
    "            \n",
    "        mean = data_sensor.mean()\n",
    "    \n",
    "        out_str += \",\" + str(mean)\n",
    "    \n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Reading Test and Train Feature Files and Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_test_train(feature_model, short_size=False):\n",
    "    feature_file='_'+str(feature_model)\n",
    "    print(\"Load train.csv...\")\n",
    "    train1 = pd.read_csv('simple_train_1'+feature_file+'_short_'+str(short_size)+'.csv')\n",
    "    train2 = pd.read_csv('simple_train_2'+feature_file+'_short_'+str(short_size)+'.csv')\n",
    "    train3 = pd.read_csv('simple_train_3'+feature_file+'_short_'+str(short_size)+'.csv')\n",
    "    train = pd.concat([train1, train2, train3])\n",
    "    # Remove all zeroes files\n",
    "    train = train[train['file_size'] > 55000].copy()\n",
    "    # Shuffle rows since they are ordered\n",
    "    train = train.iloc[np.random.permutation(len(train))]\n",
    "    # Reset broken index\n",
    "    train = train.reset_index()\n",
    "    print(\"Load test.csv...\")\n",
    "    test1 = pd.read_csv('simple_test_1'+feature_file+'_short_'+str(short_size)+'.csv')\n",
    "    test2 = pd.read_csv('simple_test_2'+feature_file+'_short_'+str(short_size)+'.csv')\n",
    "    test3 = pd.read_csv('simple_test_3'+feature_file+'_short_'+str(short_size)+'.csv')\n",
    "    test = pd.concat([test1, test2, test3])\n",
    "    print(\"Process tables...\")\n",
    "    features = get_features(train, test)\n",
    "    return train, test, features\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Creation of Feature Files (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.6\n",
      "test files1 1584\n",
      "test files3 2286\n",
      "test files2 2256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodolfoxps/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/rodolfoxps/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/rodolfoxps/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CSV for patient 1 has been completed...\n",
      "Test CSV for patient 2 has been completed...\n",
      "Test CSV for patient 3 has been completed...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_model=0\n",
    "short_size=False\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    if 1:\n",
    "        # Do reading and processing of MAT files in parallel\n",
    "        p = dict()\n",
    "#        p[1] = Process(target=create_simple_csv_train, args=(1,feature_model,short_size))\n",
    "#        p[1].start()\n",
    "#        p[2] = Process(target=create_simple_csv_train, args=(2,feature_model,short_size))\n",
    "#        p[2].start()\n",
    "#        p[3] = Process(target=create_simple_csv_train, args=(3,feature_model,short_size))\n",
    "#        p[3].start()\n",
    "        p[4] = Process(target=create_simple_csv_test, args=(1,feature_model,short_size))\n",
    "        p[4].start()\n",
    "        p[5] = Process(target=create_simple_csv_test, args=(2,feature_model,short_size))\n",
    "        p[5].start()\n",
    "        p[6] = Process(target=create_simple_csv_test, args=(3,feature_model,short_size))\n",
    "        p[6].start()\n",
    "#        p[1].join()\n",
    "#        p[2].join()\n",
    "#        p[3].join()\n",
    "        p[4].join()\n",
    "        p[5].join()\n",
    "        p[6].join()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.6\n",
      "Load train.csv...\n",
      "Load test.csv...\n",
      "Process tables...\n",
      "Length of train:  5970\n",
      "Length of test:  6126\n",
      "Features [18]: ['avg_0', 'avg_1', 'avg_10', 'avg_11', 'avg_12', 'avg_13', 'avg_14', 'avg_15', 'avg_2', 'avg_3', 'avg_4', 'avg_5', 'avg_6', 'avg_7', 'avg_8', 'avg_9', 'file_size', 'patient_id']\n"
     ]
    }
   ],
   "source": [
    "feature_model=0\n",
    "short_size=False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    \n",
    "    train, test, features = read_test_train(feature_model, short_size)\n",
    "    print('Length of train: ', len(train))\n",
    "    print('Length of test: ', len(test))\n",
    "    print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "    \n",
    "#   print ('train',train['sequence_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_train_predict(nfolds, train, test, features, target, random_state=2016, PCAkey=False, SEQoriginal=False,\n",
    "                     Oversampling=False):\n",
    "\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "    print('train pre', train.shape) \n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = copy.deepcopy(test[['Id']].astype(object))\n",
    "\n",
    "    unique_sequences = np.array(train['sequence_id'].unique())\n",
    "    print('unique sequences pre', unique_sequences.shape)\n",
    "\n",
    "    groups1=np.fix(unique_sequences/1000)\n",
    "    \n",
    "    groups2=groups1.astype(int)\n",
    "#    print('groups', groups2)\n",
    "    \n",
    "\n",
    "    \n",
    "    if SEQoriginal:\n",
    "        sequences_full=np.mod(train['sequence_id'].values,1000)\n",
    "        print('sequences full', sequences_full.shape)\n",
    "        unique_sequences2=np.mod(unique_sequences,1000)\n",
    "        unique_sequences_fold=pd.Series(sequences_full, index=train['sequence_id'].index)\n",
    "#        print('unique_sequences_fold', unique_sequences_fold)\n",
    "    \n",
    "        unique_sequences = np.unique(unique_sequences2)\n",
    "        print('unique sequences pre', unique_sequences.shape)\n",
    "\n",
    "    else:\n",
    "        unique_sequences_fold=pd.Series(train['sequence_id'], index=train['sequence_id'].index)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    test1=gkf.split(unique_sequences, groups=groups2)\n",
    "    test2=gkf.split(unique_sequences, groups=groups2)\n",
    "    \n",
    "    #random_state=random_state\n",
    "    print('unique sequences', unique_sequences.shape)\n",
    "    kf = KFold(len(unique_sequences), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    num_fold1=0\n",
    "    \n",
    "    eta = 0.2\n",
    "    max_depth = 3\n",
    "    subsample = 0.9\n",
    "    colsample_bytree = 0.9\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": eta,\n",
    "        \"tree_method\": 'exact',\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "#        \"gamma\": 0,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"scale_pos_weight\":1\n",
    "    }\n",
    "\n",
    "    \n",
    "#   Using best parameters to train model    \n",
    "    \n",
    "    \n",
    "    for train_seq_index, test_seq_index in kf:\n",
    "        num_fold += 1\n",
    "        print('Start fold {} from {}'.format(num_fold, nfolds))\n",
    "        train_seq = unique_sequences[train_seq_index]\n",
    "        valid_seq = unique_sequences[test_seq_index]\n",
    "        print('Length of train people: {}'.format(len(train_seq)))\n",
    "        print('Length of valid people: {}'.format(len(valid_seq)))\n",
    "        \n",
    "#        print('train_seq',train_seq)\n",
    "#        print('valid_seq',valid_seq)\n",
    "\n",
    "        X_train, X_valid = train[unique_sequences_fold.isin(train_seq)][features], train[unique_sequences_fold.isin(valid_seq)][features]\n",
    "        y_train, y_valid = train[unique_sequences_fold.isin(train_seq)][target], train[unique_sequences_fold.isin(valid_seq)][target]\n",
    "        X_test = test[features]\n",
    "        \n",
    "#        print('X_train',X_train)\n",
    "#        print('y_train',y_train)\n",
    "\n",
    "        print('Length train:', len(X_train))\n",
    "        print('Length valid:', len(X_valid))\n",
    "        \n",
    "#       Scaling for PCA\n",
    "\n",
    "        if PCAkey:\n",
    "        \n",
    "            scaler = MinMaxScaler()   \n",
    "        \n",
    "            Xtrain_scaled=pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "            Xvalid_scaled=pd.DataFrame(scaler.fit_transform(X_valid), columns=X_valid.columns, index=X_valid.index )\n",
    "\n",
    "            Xtest_scaled=pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "        \n",
    "\n",
    "        \n",
    "#       PCA transformation \n",
    "            pcatest=PCA(n_components=40)\n",
    "            X_train_f=pd.DataFrame(pcatest.fit_transform(Xtrain_scaled), index=Xtrain_scaled.index)\n",
    "            X_valid_f=pd.DataFrame(pcatest.fit_transform(Xvalid_scaled), index=Xvalid_scaled.index)\n",
    "\n",
    "            X_test_f=pd.DataFrame(pcatest.fit_transform(Xtest_scaled), index=Xtest_scaled.index)\n",
    "\n",
    "        else:\n",
    "            X_train_f=X_train\n",
    "            X_valid_f=X_valid\n",
    "            X_test_f=X_test\n",
    "                \n",
    "        \n",
    "        y_train_f=y_train\n",
    "        y_valid_f=y_valid\n",
    "    \n",
    "#       SMOTE oversampling\n",
    "        \n",
    "#        print('Original dataset shape {}'.format(Counter(y_train)))\n",
    "#        print('Original dataset shape {}'.format(Counter(X_train_f)))\n",
    "#        print(X_train_f)\n",
    "#        print(y_train)\n",
    "\n",
    "        if Oversampling:\n",
    "        \n",
    "            sm1 = SMOTETomek(random_state=42)\n",
    "            X_res,y_res = sm1.fit_sample(X_train_f,y_train)\n",
    "            X_train_f=pd.DataFrame(X_res, columns=X_train_f.columns)\n",
    "            y_train=pd.Series(y_res)\n",
    "        \n",
    "#        print('Resampled dataset shape {}'.format(Counter(y_train)))\n",
    "#        print(X_train_f)\n",
    "#        print(y_train)\n",
    "    \n",
    "    \n",
    "        \n",
    "#       Preparation for XGB training\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_f, y_train)\n",
    "        dvalid = xgb.DMatrix(X_valid_f, y_valid)\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]       \n",
    "        \n",
    "        gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n",
    "        yhat = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "\n",
    "#       Each time store portion of precicted data in train predicted values\n",
    "\n",
    "        for i in range(len(X_valid_f.index)):\n",
    "            yfull_train[X_valid_f.index[i]] = yhat[i]\n",
    "            \n",
    "        print(\"Validating...\")\n",
    "        check = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "        score = roc_auc_score(y_valid.tolist(), check)\n",
    "        print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "        print(\"Predict test set...\")\n",
    "        test_prediction1 = gbm.predict(xgb.DMatrix(X_test_f), ntree_limit=gbm.best_iteration+1)\n",
    "        yfull_test['kfold_' + str(num_fold)] = test_prediction1\n",
    "        \n",
    "              \n",
    "\n",
    "    print('iteration finished')\n",
    "    # Copy dict to list\n",
    "    train_res = []\n",
    "    \n",
    "    for i in range(len(train.index)):\n",
    "        train_res.append(yfull_train[i])\n",
    "\n",
    "    score = roc_auc_score(train[target], np.array(train_res))\n",
    "    print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "    # Find mean for KFolds on test\n",
    "    merge = []\n",
    "    for i in range(1, nfolds+1):\n",
    "        merge.append('kfold_' + str(i))\n",
    "    yfull_test['mean'] = yfull_test[merge].mean(axis=1)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return yfull_test['mean'].values, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_param_search(nfolds, train, test, features, target, random_state=2016,  PCAkey=False, SEQoriginal=False):\n",
    "\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "    \n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = copy.deepcopy(test[['Id']].astype(object))\n",
    "    print('train sequences',train['sequence_id'])\n",
    "    \n",
    "    \n",
    "    unique_sequences = np.array(train['sequence_id'].unique())\n",
    "#    print('unique sequences', unique_sequences, len(unique_sequences))\n",
    "    \n",
    "    groups1=np.fix(unique_sequences/1000)\n",
    "    \n",
    "    groups2=groups1.astype(int)\n",
    "#    print('groups', groups2)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    test1=gkf.split(unique_sequences, groups=groups2)\n",
    "    test2=gkf.split(unique_sequences, groups=groups2)\n",
    "    \n",
    "    \n",
    "    if SEQoriginal:\n",
    "        sequences_full=np.mod(train['sequence_id'].values,1000)\n",
    "        unique_sequences2=np.mod(unique_sequences,1000)\n",
    "        unique_sequences_fold=pd.Series(sequences_full, index=train['sequence_id'].index)\n",
    "#        print('unique_sequences_fold', unique_sequences_fold)\n",
    "    \n",
    "        unique_sequences = np.unique(unique_sequences2)\n",
    "\n",
    "    else:\n",
    "        unique_sequences_fold=pd.Series(train['sequence_id'], index=train['sequence_id'].index)\n",
    "\n",
    "    \n",
    "    kf = KFold(len(unique_sequences), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    num_fold1=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#   param_test1 = {'max_depth': [3,5]}\n",
    "#   param_test1 = {'max_depth': [1,3,5,7,9], 'min_child_weight':[1,3,5,7]}\n",
    "#   param_test1 = {'gamma':[i/10.0 for i in range(0,7)]}\n",
    "#   param_test1 = { 'subsample':[i/10.0 for i in range(6,10)],'colsample_bytree':[i/10.0 for i in range(6,10)]}  \n",
    "#   param_test1 = {'max_depth': [1,3], 'min_child_weight':[6,7,8,9,10]}\n",
    "    param_test1 = {'scale_pos_weight':[1,2,3,4,5], 'max_delta_step':[0,1,2,3,4,5]}\n",
    "\n",
    "    for train_seq_index1, test_seq_index1 in kf:\n",
    "        num_fold1 += 1\n",
    "        print('this is creation of Kfold iterator')\n",
    "        print('Start fold {} from {}'.format(num_fold1, nfolds))\n",
    "    \n",
    "        train_seq1 = unique_sequences[train_seq_index1]\n",
    "        valid_seq1 = unique_sequences[test_seq_index1]\n",
    "        \n",
    "        print(train_seq1)\n",
    "        print(valid_seq1)\n",
    "\n",
    "        train_index = train[unique_sequences_fold.isin(train_seq1)].index.values\n",
    "        test_index = train[unique_sequences_fold.isin(valid_seq1)].index.values\n",
    "\n",
    "        print(train_index, type(train_index))\n",
    "        print(test_index, type(test_index))\n",
    "       \n",
    "        train_index_group.append(train_index)\n",
    "        test_index_group.append(test_index)\n",
    "        \n",
    "    \n",
    "    print('train index group',train_index_group)\n",
    "\n",
    "    custom_cv = [(train_index_group[i], test_index_group[i]) for i in range(0,3) ]\n",
    "    \n",
    "#    custom_cv=GroupShuffleSplit(n_splits=nfolds, test_size=0.5, random_state=0)\n",
    "\n",
    "#    custom_cv = list(zip(train_index_group, test_index_group))\n",
    "\n",
    "    print('custom cv', custom_cv)\n",
    "                   \n",
    "\n",
    "#    Scaling and PCA\n",
    "\n",
    "    if PCAkey:\n",
    "\n",
    "        scaler1 = MinMaxScaler() \n",
    "    \n",
    "        train_features=train[features]\n",
    "        train_target=train[target]\n",
    "            \n",
    "        train_scaled=pd.DataFrame(scaler1.fit_transform(train_features), columns=train_features.columns, index=train_features.index)\n",
    "\n",
    "\n",
    "        pcatest=KernelPCA(kernel='poly')\n",
    "        train_features_f=pd.DataFrame(pcatest.fit_transform(train_scaled), index=train_scaled.index)\n",
    "\n",
    "        dmfeatures=train_features_f\n",
    "        dmtarget=train_target        \n",
    "\n",
    "    else:\n",
    "    \n",
    "        dmfeatures=train[features]\n",
    "        dmtarget=train[target]\n",
    "    \n",
    "#   GridSearch\n",
    "    \n",
    "    classifier1=XGBClassifier( learning_rate =0.2, n_estimators=1000, max_depth=1,\n",
    "        min_child_weight=3, gamma=0, subsample=0.6, colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic', nthread=4, scale_pos_weight=2, seed=27)\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = classifier1, param_grid = param_test1, scoring='roc_auc',n_jobs=-1,iid=False, cv=custom_cv)\n",
    "  \n",
    "    \n",
    "#    gsearch1.fit(dmfeatures,dmtarget,groups=train['sequence_id'])\n",
    "    gsearch1.fit(dmfeatures,dmtarget)\n",
    "\n",
    "    print('best parameters, scores')    \n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "#    classifier2 = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_param_search(3, train, test, features, 'result', SEQoriginal=True, PCAkey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Training, Prediction and Creating Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pre (5970, 22)\n",
      "unique sequences pre (932,)\n",
      "unique sequences (932,)\n",
      "XGBoost params. ETA: 0.2, MAX_DEPTH: 3, SUBSAMPLE: 0.9, COLSAMPLE_BY_TREE: 0.9\n",
      "Start fold 1 from 4\n",
      "Length of train people: 699\n",
      "Length of valid people: 233\n",
      "Length train: 4433\n",
      "Length valid: 1537\n",
      "[0]\ttrain-auc:0.753799\teval-auc:0.614343\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4]\ttrain-auc:0.858249\teval-auc:0.63688\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.636880\n",
      "Predict test set...\n",
      "Start fold 2 from 4\n",
      "Length of train people: 699\n",
      "Length of valid people: 233\n",
      "Length train: 4512\n",
      "Length valid: 1458\n",
      "[0]\ttrain-auc:0.767685\teval-auc:0.687644\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1]\ttrain-auc:0.810831\teval-auc:0.701816\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.701816\n",
      "Predict test set...\n",
      "Start fold 3 from 4\n",
      "Length of train people: 699\n",
      "Length of valid people: 233\n",
      "Length train: 4492\n",
      "Length valid: 1478\n",
      "[0]\ttrain-auc:0.754637\teval-auc:0.678179\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.754637\teval-auc:0.678179\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.678179\n",
      "Predict test set...\n",
      "Start fold 4 from 4\n",
      "Length of train people: 699\n",
      "Length of valid people: 233\n",
      "Length train: 4473\n",
      "Length valid: 1497\n",
      "[0]\ttrain-auc:0.765024\teval-auc:0.578643\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[34]\ttrain-auc:0.932299\teval-auc:0.623771\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.623771\n",
      "Predict test set...\n",
      "iteration finished\n",
      "Check error value: 0.592071\n",
      "\n",
      "Training time: 0.07 minutes\n",
      "Writing submission:  submission_0.592071210477_2016-10-31-16-31.csv\n"
     ]
    }
   ],
   "source": [
    "prediction, score = run_train_predict(4, train, test, features, 'result', SEQoriginal=False, PCAkey=False,\n",
    "                                      Oversampling=False)\n",
    "\n",
    "create_submission(score, test, prediction)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

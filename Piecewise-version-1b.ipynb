{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Package Import and Simple Module definition\n",
    "\n",
    "Modified from ZFTurbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.io import loadmat\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import Process\n",
    "import copy\n",
    "\n",
    "#importing CSP modules\n",
    "import mne\n",
    "from mne.decoding import CSP\n",
    "\n",
    "\n",
    "\n",
    "#Importing old and new Kfold\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold as NewKF\n",
    "from sklearn.model_selection import StratifiedKFold as StratKF\n",
    "\n",
    "#Importing GroupKfold, only available since version 0.18\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "#Importing function for scaling data before PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Importing PCA packages\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "#Importing FFT package\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "#Importing crossvalidation metrics and Gridsearch\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Importing wrapper to use XGB with Gridsearch\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#Importing plotting packages (optional)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "#Oversampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "###Band Frequency filtering###\n",
    "from scipy.signal import cheby2, butter, lfilter\n",
    "\n",
    "\n",
    "from numpy import inf\n",
    "\n",
    "\n",
    "#Defining general modules used in the classification\n",
    "\n",
    "random.seed(2016)\n",
    "np.random.seed(2016)\n",
    "\n",
    "\n",
    "def natural_key(string_):\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_)]\n",
    "\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def get_importance(gbm, features):\n",
    "    create_feature_map(features)\n",
    "    importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "    return importance\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "\n",
    "def print_features_importance(imp):\n",
    "    for i in range(len(imp)):\n",
    "        print(\"# \" + str(imp[i][1]))\n",
    "        print('output.remove(\\'' + imp[i][0] + '\\')')\n",
    "\n",
    "\n",
    "def mat_to_pandas(path):\n",
    "    mat = loadmat(path)\n",
    "    names = mat['dataStruct'].dtype.names\n",
    "    ndata = {n: mat['dataStruct'][n][0, 0] for n in names}\n",
    "    samp_freq = ndata['iEEGsamplingRate'][0, 0]\n",
    "    sequence = -1\n",
    "    if 'sequence' in names:\n",
    "        sequence = mat['dataStruct']['sequence']\n",
    "    return pd.DataFrame(ndata['data'], columns=ndata['channelIndices'][0]), sequence, samp_freq\n",
    "\n",
    "def mat_to_pandas_seq(path):\n",
    "    mat = loadmat(path)\n",
    "    names = mat['dataStruct'].dtype.names\n",
    "    sequence = -1\n",
    "    if 'sequence' in names:\n",
    "        sequence = mat['dataStruct']['sequence']\n",
    "    return sequence\n",
    "\n",
    "def create_submission(score, test, prediction, feature_model, short_size, new_test):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + 'model_'+str(feature_model)+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'_'+ str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('File,Class\\n')\n",
    "    total = 0\n",
    "    for id in test['Id']:\n",
    "        patient = id // 100000\n",
    "        fid = id % 100000\n",
    "        str1 = 'new_' + str(patient) + '_' + str(fid) + '.mat' + ',' + str(prediction[total])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values)\n",
    "    testval = list(test.columns.values)\n",
    "    output = intersect(trainval, testval)\n",
    "    output.remove('Id')\n",
    "    # output.remove('file_size')\n",
    "    return sorted(output)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Creating Features and Saving to CSV Files\n",
    "\n",
    "#### One file per patient per test/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modules to read train and test data.\n",
    "#Short_dataset can be False or TRue. It decides whether to use the long or short sample size.\n",
    "\n",
    "#patient_id:\n",
    "#fini\n",
    "#fend\n",
    "#fovr\n",
    "#feature_model\n",
    "\n",
    "\n",
    "def create_simple_csv_train(patient_id, feature_model, num_features, fini, fend, fovr,\n",
    "                            short_dataset=False, new_test=False):\n",
    "    \n",
    "    #Constructing main part of feature file name\n",
    "    feature_file='_'+str(feature_model)+'_short_'+str(short_dataset)+'_new_test_'+ str(new_test)\\\n",
    "            +'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)\n",
    "    \n",
    "    \n",
    "    #Deciding on short ot small dataset\n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/train_\"\n",
    "    else:\n",
    "        source_dir=\"./train_\"\n",
    "    \n",
    "    new_label=''\n",
    "    \n",
    "    #Using old or new test files\n",
    "    if new_test:\n",
    "        \n",
    "        new_label='_new'\n",
    "\n",
    "    out = open(\"simple_train_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,sequence_id,sequence_num,patient_id,\")\n",
    "  \n",
    "    #Generating column names\n",
    "\n",
    "    columns=''\n",
    "    for i in range(16):\n",
    "        for j in range(num_features):\n",
    "            columns+= 'ch_'+str(i)+'_'+\"band_\"+str(j)+\",\"        \n",
    "\n",
    "    out.write(columns+\"file_size,result\\n\")\n",
    "\n",
    "    # Generating features\n",
    "    \n",
    "    out_str = ''\n",
    "    \n",
    "    ##reading files\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "    print ('train files'+ str(patient_id), len(files))    \n",
    "    \n",
    "    ##Indicators for debugging\n",
    "    pos1=0\n",
    "    neg1=0\n",
    "    sequence_id_pre = int(patient_id)*1000\n",
    "    sequence_id_inter = int(patient_id)*1000\n",
    "    total_pre = 0\n",
    "    total_inter=0\n",
    "    seq1=0\n",
    "    \n",
    "    ##Selecting 'safe' files from old test\n",
    "    \n",
    "    new_train = pd.read_csv('train_and_test_data_labels_safe'+'.csv')\n",
    "    new_data = new_train['image']\n",
    "    \n",
    "    selection = new_train[new_train['safe'] == 1].drop('safe', axis=1)\n",
    "    \n",
    "    ## Iterating through file\n",
    "    \n",
    "    for fl in files:\n",
    "        \n",
    "        # print('Go for ' + fl)\n",
    "               \n",
    "        if os.path.basename(fl) not in selection['image'].values:\n",
    "            continue\n",
    "        \n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        result = int(arr[2])\n",
    "        \n",
    "        if result == 1:\n",
    "            \n",
    "            total_pre += 1\n",
    "            sequence_id=int(patient_id)*1000+int((total_pre-1) // 6) + int((total_inter-1) // 6) + 1\n",
    "\n",
    "            \n",
    "        elif result == 0:\n",
    "            \n",
    "            total_inter += 1            \n",
    "            sequence_id=int(patient_id)*1000+int((total_pre) // 6) + int((total_inter-1) // 6)\n",
    "\n",
    "        \n",
    "        new_id = int(patient*100000 + id)\n",
    "        try:\n",
    "            tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "            seq1=int(sequence_from_mat[0][0][0][0])\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if (new_id % 1000) % 6 == 0:\n",
    "            sequence_validator=6\n",
    "        else:\n",
    "            sequence_validator=(new_id % 1000) % 6\n",
    "        \n",
    "        \n",
    "        if seq1!=sequence_validator:\n",
    "            print('sequence mismatch!',seq1, sequence_validator)\n",
    "        else:\n",
    "            print('sequence match ',seq1, sequence_validator) \n",
    "        \n",
    "        \n",
    "        print(sequence_id)\n",
    "        out_str += str(new_id) + \",\" + str(sequence_id) + \",\" + str(seq1) + \",\"+str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])       \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f], out_str,feature_model, sizesignal, samp_freq,  fini, fend, fovr,)\n",
    "            \n",
    "            \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \",\" + str(result) + \"\\n\"\n",
    "        #print(sequence_from_mat)\n",
    "        #print(type(sequence_from_mat))\n",
    "       \n",
    "        print('total preictal: ', total_pre,' total interictal: ', total_inter,' sequence local: ', seq1)\n",
    "        if (total_pre % 6 == 0) and result == 1:\n",
    "                pos1 += 1\n",
    "                print('Positive ocurrence sequence finished', pos1)\n",
    "                if (seq1==6):\n",
    "                    sequence_id_pre += 1\n",
    "                    print ('sequence preictal next',sequence_id_pre)\n",
    "        \n",
    "        if (total_inter % 6 == 0) and result == 0:                \n",
    "                neg1 += 1\n",
    "                print('Negative ocurrence sequence finished', neg1)\n",
    "                if (seq1==6):\n",
    "                    sequence_id_inter += 1\n",
    "                    print ('sequence interictal next',sequence_id_inter)\n",
    "\n",
    "    out.write(out_str)\n",
    "    \n",
    "    out.close()\n",
    "    print('Train CSV for patient {} has been completed...'.format(patient_id))\n",
    "\n",
    "\n",
    "def create_simple_csv_test(patient_id, feature_model, num_features, fini, fend, fovr,\n",
    "                           short_dataset=False, new_test=False):\n",
    "    \n",
    "    feature_file='_'+str(feature_model)+'_short_'+str(short_dataset)+'_new_test_'+str(new_test)\\\n",
    "            +'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)\n",
    "    \n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/test_\"\n",
    "    else:\n",
    "        source_dir=\"./test_\"\n",
    "    \n",
    "    new_label=''\n",
    "    \n",
    "    if new_test:\n",
    "        \n",
    "        new_label=\"_new\"\n",
    "\n",
    "    # TEST\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + new_label + \"/*.mat\"), key=natural_key)\n",
    "    print ('test files'+ str(patient_id), len(files))    \n",
    "    out = open(\"simple_test_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,patient_id,\")\n",
    "    \n",
    "    columns=''\n",
    "    for i in range(16):\n",
    "        for j in range(num_features):\n",
    "            columns+= 'ch_'+str(i)+'_'+\"band_\"+str(j)+\",\"        \n",
    "    \n",
    "    out.write(columns+\"file_size\\n\")\n",
    "    \n",
    "        \n",
    "    for fl in files:\n",
    "        # print('Go for ' + fl)\n",
    "        id_str = os.path.basename(fl)[4:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        new_id = int(patient*100000 + id)\n",
    "        try:\n",
    "            tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        out_str += str(new_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])\n",
    "              \n",
    "               \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f], out_str,feature_model, sizesignal, samp_freq, fini, fend, fovr,)\n",
    "                        \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \"\\n\"\n",
    "        # break\n",
    "\n",
    "    out.write(out_str)\n",
    "    out.close()\n",
    "    print('Test CSV for patient {} has been completed...'.format(patient_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_subset_train(feature_model):\n",
    "    \n",
    "   \n",
    "    \n",
    "#    filestotal = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "\n",
    "    \n",
    "    folder_list=['train_1','train_2','train_3']\n",
    "    \n",
    "    \n",
    "    \n",
    "    for folder_label in folder_list:\n",
    "        \n",
    "        short_size=False\n",
    "        \n",
    "        patient_id=int(list(filter(str.isdigit, folder_label))[0])\n",
    "        \n",
    "        new_test=True      \n",
    "        if len(folder_label)>7:\n",
    "  \n",
    "            var=6\n",
    "            \n",
    "        else:\n",
    "            var=7\n",
    "        \n",
    "        files = pd.read_csv('simple_'+ folder_label[0:var]+'_'+str(feature_model)+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "\n",
    "        files_index=pd.Series(files['sequence_id'], index=files['sequence_id'].index)\n",
    "\n",
    "        \n",
    "        files_unique =files.drop_duplicates(subset=['sequence_id'])\n",
    "        \n",
    "#       print(files_unique)\n",
    "        \n",
    "\n",
    "        files_1=files_unique[files_unique['result'] == 1]['sequence_id']\n",
    "        files_0=files_unique[files_unique['result'] == 0]['sequence_id']\n",
    "        \n",
    "        print(files_1.shape)\n",
    "        print(files_0.shape)\n",
    "    \n",
    "        n_samples_1=int(len(files_1)/10)\n",
    "        n_samples_0=int(len(files_0)/10)\n",
    "        \n",
    "\n",
    "    \n",
    "        files_1_rand=np.random.choice(files_1, size=n_samples_1)\n",
    "        files_0_rand=np.random.choice(files_0, size=n_samples_0)\n",
    "        \n",
    "        \n",
    "        files_1_rand_seq=files[files_index.isin(files_1_rand)]['Id']\n",
    "        files_0_rand_seq=files[files_index.isin(files_0_rand)]['Id']\n",
    "        \n",
    "        for file in files_1_rand_seq:\n",
    "        \n",
    "            id_file=str(file % 1000)\n",
    "            source_file='./'+folder_label+'/'+str(patient_id)+'_'+id_file+'_1.mat'\n",
    "            target_file='./data_random/' + folder_label+'/'+str(patient_id)+'_'+id_file+'_1.mat'\n",
    "            print(target_file)\n",
    "    \n",
    "            shutil.copyfile(source_file, target_file) \n",
    "\n",
    "        for file in files_0_rand_seq:\n",
    "        \n",
    "            id_file=str(file % 1000)\n",
    "            source_file='./'+folder_label+'/'+str(patient_id)+'_'+id_file+'_0.mat'\n",
    "            target_file='./data_random/' + folder_label+'/'+str(patient_id)+'_'+id_file+'_0.mat'\n",
    "            print(target_file)\n",
    "    \n",
    "            shutil.copyfile(source_file, target_file)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_model=3\n",
    "short_size=False\n",
    "num_features=6\n",
    "new_test=True\n",
    "\n",
    "\n",
    "create_subset_train(feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_subset_test(feature_model):\n",
    "    \n",
    "   \n",
    "    \n",
    "#    filestotal = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "\n",
    "    \n",
    "    folder_list=['test_1_new','test_2_new','test_3_new']\n",
    "    \n",
    "    \n",
    "    \n",
    "    for folder_label in folder_list:\n",
    "        \n",
    "        short_size=False\n",
    "        \n",
    "        patient_id=int(list(filter(str.isdigit, folder_label))[0])\n",
    "        \n",
    "        new_test=True      \n",
    "        if len(folder_label)>7:\n",
    "  \n",
    "            var=6\n",
    "            \n",
    "        else:\n",
    "            var=7\n",
    "        \n",
    "        files = pd.read_csv('simple_'+ folder_label[0:var]+'_'+str(feature_model)+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "\n",
    "             \n",
    "\n",
    "        files_total=files['Id']\n",
    "\n",
    "    \n",
    "        n_samples=int(len(files_total)/10)\n",
    "\n",
    "    \n",
    "        files_rand=np.random.choice(files_total, size=n_samples)\n",
    "\n",
    "             \n",
    "        \n",
    "        for file in files_rand:\n",
    "        \n",
    "            id_file=str(file % 1000)\n",
    "            source_file='./'+folder_label+'/new_'+str(patient_id)+'_'+id_file+'.mat'\n",
    "            target_file='./data_random/' + folder_label+'/new_'+str(patient_id)+'_'+id_file+'.mat'\n",
    "            print(target_file)\n",
    "    \n",
    "            shutil.copyfile(source_file, target_file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_model=3\n",
    "short_size=False\n",
    "num_features=6\n",
    "new_test=True\n",
    "\n",
    "\n",
    "create_subset_test(feature_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#eng_number is the feature_value that has values 0,1,2,3... infinite, given by the list below.\n",
    "\n",
    "def feature_eng(data_sensor, out_str, eng_number, sizesignal, fs, fini5,fend5,fovr5):\n",
    "\n",
    "                \n",
    "    yf1 = fft(data_sensor)\n",
    "    fftpeak=2/sizesignal * np.abs(yf1[0:sizesignal/2])\n",
    " \n",
    "    numberofbands=4\n",
    "\n",
    "    sizeband=20/numberofbands\n",
    "    \n",
    "    if eng_number==5:\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "        ##Frequency parameters##\n",
    "        #Start frequency#\n",
    "        fini = fini5\n",
    "        #End frequency#\n",
    "        fend = fend5\n",
    "        #Frequency band range#\n",
    "        frng = 4\n",
    "        #Frequency overlap#\n",
    "        fovr = fovr5\n",
    "    \n",
    "        #Frequency band generator#\n",
    "        fbands = [[f, f + frng] for f in range(fini, fend - fovr, frng - fovr)]\n",
    "    \n",
    "        #Filter order#\n",
    "        order = 5\n",
    "        #Filter bandstop attenuation (dB)#\n",
    "        attenuation = 20.0\n",
    "        #Nyquist frequency#\n",
    "        fnyq = fs / 2.0\n",
    "        \n",
    "\n",
    "        for fb in fbands:\n",
    "        \n",
    "            #Create butterworth bandpass filter#\n",
    "            #b, a = butter(order, fb  / fnyq, btype='band')\n",
    "            b, a = cheby2(order, attenuation, fb  / fnyq, btype='band')\n",
    "            \n",
    "            #Apply filter#\n",
    "            data_filter = lfilter(b, a, data_sensor)\n",
    "            \n",
    "            #Band pass 'power'#\n",
    "            band_pwr = np.square(data_filter)\n",
    "            \n",
    "            avg_band_pwr = band_pwr.mean()\n",
    "            \n",
    "            out_str += \",\" + str(avg_band_pwr)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    elif eng_number==4:\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "        ##Frequency parameters##\n",
    "        #Start frequency#\n",
    "        fini = 4\n",
    "        #End frequency#\n",
    "        fend = 40\n",
    "        #Frequency band range#\n",
    "        frng = 4\n",
    "        #Frequency overlap#\n",
    "        fovr = 0\n",
    "    \n",
    "        #Frequency band generator#\n",
    "        fbands = [[f, f + frng] for f in range(fini, fend - fovr, frng - fovr)]\n",
    "    \n",
    "        #Filter order#\n",
    "        order = 5\n",
    "        #Filter bandstop attenuation (dB)#\n",
    "        attenuation = 20.0\n",
    "        #Nyquist frequency#\n",
    "        fnyq = fs / 2.0\n",
    "        \n",
    "\n",
    "        for fb in fbands:\n",
    "        \n",
    "            #Create butterworth bandpass filter#\n",
    "            #b, a = butter(order, fb  / fnyq, btype='band')\n",
    "            b, a = cheby2(order, attenuation, fb  / fnyq, btype='band')\n",
    "            \n",
    "            #Apply filter#\n",
    "            data_filter = lfilter(b, a, data_sensor)\n",
    "            \n",
    "            #Band pass 'power'#\n",
    "            band_pwr = np.square(data_filter)\n",
    "            \n",
    "            avg_band_pwr = band_pwr.mean()\n",
    "            \n",
    "            out_str += \",\" + str(avg_band_pwr)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    elif eng_number==3:\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "        ##Frequency parameters##\n",
    "        #Start frequency#\n",
    "        fini = 7\n",
    "        #End frequency#\n",
    "        fend = 30\n",
    "        #Frequency band range#\n",
    "        frng = 4\n",
    "        #Frequency overlap#\n",
    "        fovr = 0\n",
    "    \n",
    "        #Frequency band generator#\n",
    "        fbands = [[f, f + frng] for f in range(fini, fend - fovr, frng - fovr)]\n",
    "    \n",
    "        #Filter order#\n",
    "        order = 5\n",
    "        #Filter bandstop attenuation (dB)#\n",
    "        attenuation = 20.0\n",
    "        #Nyquist frequency#\n",
    "        fnyq = fs / 2.0\n",
    "        \n",
    "\n",
    "        for fb in fbands:\n",
    "        \n",
    "            #Create butterworth bandpass filter#\n",
    "            #b, a = butter(order, fb  / fnyq, btype='band')\n",
    "            b, a = cheby2(order, attenuation, fb  / fnyq, btype='band')\n",
    "            \n",
    "            #Apply filter#\n",
    "            data_filter = lfilter(b, a, data_sensor)\n",
    "            \n",
    "            #Band pass 'power'#\n",
    "            band_pwr = np.square(data_filter)\n",
    "            \n",
    "            avg_band_pwr = band_pwr.mean()\n",
    "            \n",
    "            out_str += \",\" + str(avg_band_pwr)\n",
    "            \n",
    "\n",
    "    elif eng_number==2:\n",
    "        \n",
    "        mean = data_sensor.mean()\n",
    "        \n",
    "        peak1=fftpeak[0:3].mean()            \n",
    "        peak2=fftpeak[3:6].mean()          \n",
    "        peak3=fftpeak[6:9].mean()\n",
    "        peak4=fftpeak[9:12].mean()\n",
    "        peak5=fftpeak[12:15].mean()            \n",
    "        peak6=fftpeak[15:18].mean()          \n",
    "        peak7=fftpeak[18:21].mean()\n",
    "        peak8=fftpeak[21:24].mean()\n",
    "        peak9=fftpeak[24:27].mean()            \n",
    "        peak10=fftpeak[27:30].mean()          \n",
    "        peak11=fftpeak[30:33].mean()\n",
    "        peak12=fftpeak[33:36].mean()\n",
    "            \n",
    "        out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4) \\\n",
    "                    +\",\" + str(peak5) + \",\" + str(peak6) + \",\" + str(peak7) +\",\" + str(peak8)+ \",\" + str(peak9) \\\n",
    "                    +\",\" + str(peak10) + \",\" + str(peak11) +\",\" + str(peak12)\n",
    "    \n",
    "    elif eng_number==1:\n",
    "            \n",
    "        mean = data_sensor.mean()   \n",
    "        \n",
    "        peak1=fftpeak[0:5].mean()            \n",
    "        peak2=fftpeak[5:10].mean()          \n",
    "        peak3=fftpeak[10:15].mean()\n",
    "        peak4=fftpeak[15:20].mean()\n",
    "        \n",
    "        out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4)\n",
    "    \n",
    "    elif eng_number==0:\n",
    "            \n",
    "        mean = data_sensor.mean()\n",
    "    \n",
    "        out_str += \",\" + str(mean)\n",
    "    \n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Reading Test and Train Feature Files and Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_test_train(feature_model, short_size=False, new_test=False):\n",
    "    feature_file='_'+str(feature_model)\n",
    "    print(\"Load train.csv...\")\n",
    "    train1 = pd.read_csv('simple_train_1'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    train2 = pd.read_csv('simple_train_2'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    train3 = pd.read_csv('simple_train_3'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    train = pd.concat([train1, train2, train3])\n",
    "    # Remove all zeroes files\n",
    "    train = train[train['file_size'] > 55000].copy()\n",
    "    # Shuffle rows since they are ordered\n",
    "    train = train.iloc[np.random.permutation(len(train))]\n",
    "    # Reset broken index\n",
    "    train = train.reset_index()\n",
    "    print(\"Load test.csv...\")\n",
    "    test1 = pd.read_csv('simple_test_1'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    test2 = pd.read_csv('simple_test_2'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    test3 = pd.read_csv('simple_test_3'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    test = pd.concat([test1, test2, test3])\n",
    "    print(\"Process tables...\")\n",
    "    features = get_features(train, test)\n",
    "    return train, test, features\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_test_train_per_patient(feature_model, short_size=False, new_test=False, fini=4, fend=40, fovr=0):\n",
    "    feature_file='_'+str(feature_model)\n",
    "    print(\"Load train.csv...\")\n",
    "    train1 = pd.read_csv('simple_train_1'+feature_file+'_short_'+str(short_size)+\n",
    "                        '_new_test_'+str(new_test)+'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)+'.csv')\n",
    "    train2 = pd.read_csv('simple_train_2'+feature_file+'_short_'+str(short_size)+\n",
    "                         '_new_test_'+str(new_test)+'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)+'.csv')\n",
    "    train3 = pd.read_csv('simple_train_3'+feature_file+'_short_'+str(short_size)+\n",
    "                         '_new_test_'+str(new_test)+'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)+'.csv')\n",
    "#    print(train1['Id'])\n",
    "    #train = pd.concat([train1, train2, train3])\n",
    "    # Remove all zeroes files\n",
    "    train1 = train1[train1['file_size'] > 55000].copy()\n",
    "    train2 = train2[train2['file_size'] > 55000].copy()\n",
    "    train3 = train3[train3['file_size'] > 55000].copy()\n",
    "    # Shuffle rows since they are ordered\n",
    "    train1 = train1.iloc[np.random.permutation(len(train1))]\n",
    "    train2 = train2.iloc[np.random.permutation(len(train2))]\n",
    "    train3 = train3.iloc[np.random.permutation(len(train3))]\n",
    "    # Reset broken index\n",
    "    train1 = train1.reset_index()\n",
    "    train2 = train2.reset_index()\n",
    "    train3 = train3.reset_index()\n",
    "    print(\"Load test.csv...\")\n",
    "    test1 = pd.read_csv('simple_test_1'+feature_file+'_short_'+str(short_size)+\n",
    "                        '_new_test_'+str(new_test)+'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)+'.csv')\n",
    "    test2 = pd.read_csv('simple_test_2'+feature_file+'_short_'+str(short_size)+\n",
    "                        '_new_test_'+str(new_test)+'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)+'.csv')\n",
    "    test3 = pd.read_csv('simple_test_3'+feature_file+'_short_'+str(short_size)+\n",
    "                        '_new_test_'+str(new_test)+'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)+'.csv')\n",
    "    #test = pd.concat([test1, test2, test3])\n",
    "    print(\"Process tables...\")\n",
    "    features1 = get_features(train1, test1)\n",
    "    features2 = get_features(train2, test2)\n",
    "    features3 = get_features(train3, test3)\n",
    "    return [[train1, test1, features1],[train2, test2, features2],[train3, test3, features3]]\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Creation of Feature Files (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    print('XGBoost: {}'.format(xgb.__version__))\n",
    "\n",
    "def generating_files(mode,feature_model, short_size, num_features, new_test,fini, fend, fovr):\n",
    "\n",
    "    if mode==0:\n",
    "        # Do reading and processing of MAT files in parallel\n",
    "        p = dict()\n",
    "        p[1] = Process(target=create_simple_csv_train, args=(1,feature_model,num_features,fini, fend, fovr,\n",
    "                                                             short_size,new_test))\n",
    "        p[1].start()\n",
    "        p[2] = Process(target=create_simple_csv_train, args=(2,feature_model,num_features,fini, fend, fovr,\n",
    "                                                             short_size,new_test))\n",
    "        p[2].start()\n",
    "        p[3] = Process(target=create_simple_csv_train, args=(3,feature_model,num_features,fini, fend, fovr,\n",
    "                                                             short_size,new_test))\n",
    "        p[3].start()\n",
    "        p[4] = Process(target=create_simple_csv_test, args=(1,feature_model,num_features,fini, fend, fovr,\n",
    "                                                            short_size,new_test))\n",
    "        p[4].start()\n",
    "        p[5] = Process(target=create_simple_csv_test, args=(2,feature_model,num_features,fini, fend, fovr,\n",
    "                                                            short_size,new_test))\n",
    "        p[5].start()\n",
    "        p[6] = Process(target=create_simple_csv_test, args=(3,feature_model,num_features,fini, fend, fovr,\n",
    "                                                            short_size,new_test))\n",
    "        p[6].start()\n",
    "        p[1].join()\n",
    "        p[2].join()\n",
    "        p[3].join()\n",
    "        p[4].join()\n",
    "        p[5].join()\n",
    "        p[6].join()\n",
    "    \n",
    "    elif mode==1:\n",
    "        p = dict()\n",
    "        \n",
    "        p[1] = Process(target=create_simple_csv_train, args=(1,feature_model,num_features,fini, fend, fovr,\n",
    "                                                             short_size,new_test))\n",
    "        p[1].start()\n",
    "                \n",
    "        p[2] = Process(target=create_simple_csv_test, args=(1,feature_model,num_features,fini, fend, fovr,\n",
    "                                                            short_size,new_test))\n",
    "        p[2].start()\n",
    "        \n",
    "        p[1].join()\n",
    "        p[2].join()\n",
    "        \n",
    "    elif mode==2:\n",
    "        \n",
    "        p = dict()\n",
    "        \n",
    "        p[1] = Process(target=create_simple_csv_train, args=(2,feature_model,num_features,fini, fend, fovr,\n",
    "                                                             short_size,new_test))\n",
    "        p[1].start()\n",
    "                \n",
    "        p[2] = Process(target=create_simple_csv_test, args=(2,feature_model,num_features,fini, fend, fovr,\n",
    "                                                            short_size,new_test))\n",
    "        p[2].start()\n",
    "        \n",
    "        p[1].join()\n",
    "        p[2].join()\n",
    "        \n",
    "          \n",
    "        \n",
    "    elif mode==3:\n",
    "    \n",
    "        p = dict()\n",
    "        \n",
    "        p[1] = Process(target=create_simple_csv_train, args=(3,feature_model,num_features,fini, fend, fovr,\n",
    "                                                             short_size,new_test))\n",
    "        p[1].start()\n",
    "                \n",
    "        p[2] = Process(target=create_simple_csv_test, args=(3,feature_model,num_features,fini, fend, fovr,\n",
    "                                                            short_size,new_test))\n",
    "        p[2].start()\n",
    "        \n",
    "        p[1].join()\n",
    "        p[2].join()\n",
    "\n",
    "#create_simple_csv_test(1,feature_model,num_features,short_size,new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_model=5\n",
    "short_size=False\n",
    "num_features=9\n",
    "new_test=True\n",
    "fini=4\n",
    "fend=40\n",
    "fovr=0\n",
    "mode=3\n",
    "\n",
    "generating_files(mode,feature_model, short_size, num_features, new_test,fini, fend, fovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_model=4\n",
    "short_size=False\n",
    "num_features=9\n",
    "new_test=True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    \n",
    "    train, test, features = read_test_train(feature_model, short_size, new_test)\n",
    "    print('Length of train: ', len(train))\n",
    "    print('Length of test: ', len(test))\n",
    "    print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "    \n",
    "#   print ('train',train['sequence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_train_predict(train, test, features, target,params_model, num_features, channels, csp_n=4,\n",
    "                      seq_number=1,csp_init=0,csp_end=0.1, \n",
    "                      nfolds=3, random_state=2016,\n",
    "                      mode=0, PCAkey=False, PCAgraph=False,\n",
    "                      PCAkeyGS=False, SEQoriginal=False,\n",
    "                     Oversampling=False, GridSearch=False, pred_per_patient=False, CSPkey=False, CSPkey1=False):\n",
    "    \n",
    "    \n",
    "#    print(train)\n",
    "#    print(train.shape)\n",
    "#    print(train.columns.values)\n",
    "#    print(type(train['result']))\n",
    "    \n",
    "    function_params = OrderedDict()\n",
    "    function_params[\"nfolds\"]=nfolds\n",
    "    function_params[\"random_state\"]= random_state\n",
    "    function_params[\"PCAkey\"] = PCAkey\n",
    "    function_params[\"PCAgraph\"]= PCAgraph\n",
    "    function_params[\"PCAkeyGS\"]= PCAkeyGS\n",
    "    function_params[\"SEQoriginal\"]= SEQoriginal\n",
    "    function_params[\"Oversampling\"]= Oversampling\n",
    "    function_params[\"GridSearch\"]= GridSearch\n",
    "    \n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    #print('train type', type(train),'train', train, 'train index', train.index.values)\n",
    "    #print('test type',type(test),'test', test, 'test index', test.index.values)\n",
    "\n",
    "    #train=train.iloc[0:120]\n",
    "    #test=test.iloc[0:100]\n",
    "    \n",
    "    #print('test',test)\n",
    "    \n",
    "\n",
    "    \n",
    "    #train_seq=train['Id']\n",
    "    \n",
    "    #if seq_number==6:\n",
    "    #    pandas_number=0\n",
    "    #else:\n",
    "    #    pandas_number=seq_number\n",
    "    \n",
    "    #print('pandas number', pandas_number)\n",
    "    \n",
    "    #train=train[(train_seq % 1000) % 6 == pandas_number]\n",
    "    \n",
    "    #train_seq_rev=train['Id'].values.tolist()#[0:120]\n",
    "    #result_seq_rev=np.int64(train[target].values.tolist())#[0:120])\n",
    "    \n",
    "    #for item111 in train_seq_rev:\n",
    "        \n",
    "    #    if (item111 % 1000) % 6 != pandas_number:\n",
    "            \n",
    "    #        print('error pandas numerb',(item111 % 1000) % 6, pandas_number)\n",
    "        \n",
    "    #    else:\n",
    "            \n",
    "    #        print('all good!', (item111 % 1000) % 6, pandas_number)\n",
    "    \n",
    "    #file_name_train_sequence=[]\n",
    "    #seq_number_list=[]\n",
    "   \n",
    "    #for i,f_id in enumerate(train_seq_rev):\n",
    "                           \n",
    "    #   real_f_id=f_id % 100000 \n",
    "    #   name_file1=\"./train_\"+str(mode)+'/'+str(mode)+'_'+str(real_f_id)+'_'+str(result_seq_rev[i])+'.mat'\n",
    "    #   file_name_train_sequence.append(name_file1)\n",
    "    #   print(name_file1)\n",
    "    #  \n",
    "    #   try:\n",
    "    #       sequence_from_mat_seq = mat_to_pandas_seq(name_file1)\n",
    "    #   except:\n",
    "    #       print('Some error here {}...'.format(name_file1))\n",
    "    #       seq_number_list.append(0)\n",
    "    #       continue\n",
    "       \n",
    "    #   seq_number_list.append(int(sequence_from_mat_seq[0][0][0][0]))\n",
    "    #   print('another done', i)\n",
    "\n",
    "#    pandas_array=np.array(list(np.loadtxt('pandas_sequences_train_3.txt', delimiter=',')))\n",
    "    \n",
    "#    print('pandas_array', len(pandas_array), pandas_array)\n",
    "\n",
    "#    total_sequences_lists=[result_seq_rev, seq_number_list]\n",
    "        \n",
    "#   pandas_sequences=pd.Dataframe(total_sequences_lists, columns=['Id_number', 'sequences'])\n",
    "    \n",
    "    \n",
    "    #pandas_sequences=pd.DataFrame(np.array(seq_number_list),columns=['sequences'], index=train.index)\n",
    "    \n",
    "    #print('pandas_sequences',pandas_sequences.shape, pandas_sequences)\n",
    "    print('train shape',train.shape)\n",
    "    train=train[train['sequence_num']==seq_number]\n",
    "    \n",
    "#    for i in pandas_sequences['Id_number'].values.tolist():\n",
    "        \n",
    "#        for j in train.index.values.tolist():\n",
    "            \n",
    "#            if \n",
    "#            train_list.append()\n",
    "    \n",
    "#    train=train[train['Id']==Id_number &\n",
    "                \n",
    "#                pandas_sequences[pandas_sequences.isin(train['Id'])]\n",
    "\n",
    "    \n",
    "#    np.savetxt('pandas_sequences.txt', pandas_sequences, delimiter=',')\n",
    "\n",
    "      \n",
    "    \n",
    "    print(#'train_seq',len(train_seq_rev),\n",
    "          'train shape',train.shape, 'train', train.shape)\n",
    "    \n",
    "    #print('train type', type(train),'train', train, 'train index', train.index.values)\n",
    "    #print('test type',type(test),'test', test, 'test index', test.index.values)\n",
    "    \n",
    "    unique_seq = train.drop_duplicates(subset=['sequence_id'])\n",
    "    unique_seq_y = unique_seq['result'].values\n",
    "    \n",
    "    print('unique seq y', len(unique_seq_y) )\n",
    "    \n",
    "    n_samples=len(unique_seq_y)\n",
    "    print('length',n_samples)\n",
    "    unique_seq_X = np.zeros(n_samples)\n",
    "    \n",
    "    print('unique seq X', len(unique_seq_X)  )\n",
    "    \n",
    "    \n",
    "    print('train pre', train.shape) \n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = copy.deepcopy(test[['Id']].astype(object))\n",
    "\n",
    "    unique_sequences = np.array(train['sequence_id'].unique())\n",
    "    print('unique sequences pre', unique_sequences.shape)\n",
    "\n",
    "    groups1=np.fix(unique_sequences/1000)\n",
    "    \n",
    "    groups2=groups1.astype(int)\n",
    "    #    print('groups', groups2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    test1=gkf.split(unique_sequences, groups=groups2)\n",
    "    test2=gkf.split(unique_sequences, groups=groups2)\n",
    "    \n",
    "    #random_state=random_state\n",
    "    print('unique sequences', unique_sequences.shape)\n",
    "    #    splitKF = KFold(len(unique_sequences), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    #    kf = NewKF(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    kf = StratKF(n_splits=nfolds, shuffle=False, random_state=random_state)\n",
    "    \n",
    "    num_fold = 0\n",
    "    num_fold1=0\n",
    "    \n",
    "\n",
    "    \n",
    "    if SEQoriginal:\n",
    "        sequences_full=np.mod(train['sequence_id'].values,1000)\n",
    "        print('sequences full', sequences_full.shape)\n",
    "        unique_sequences2=np.mod(unique_sequences,1000)\n",
    "        unique_sequences_fold=pd.Series(sequences_full, index=train['sequence_id'].index)\n",
    "#        print('unique_sequences_fold', unique_sequences_fold)\n",
    "    \n",
    "        unique_sequences = np.unique(unique_sequences2)\n",
    "        print('unique sequences pre', unique_sequences.shape)\n",
    "\n",
    "    else:\n",
    "        unique_sequences_fold=pd.Series(train['sequence_id'], index=train['sequence_id'].index)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    \n",
    "    eta = 0.1\n",
    "    max_depth = 4\n",
    "    subsample = 0.9\n",
    "    colsample_bytree = 0.9\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    \n",
    "    params = OrderedDict()\n",
    "    params[\"objective\"]= \"binary:logistic\"\n",
    "    params[\"booster\"]= \"gbtree\"\n",
    "    params[\"eval_metric\"]= \"auc\"\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"tree_method\"]='exact'\n",
    "    params[\"max_depth\"]= max_depth\n",
    "    params[\"subsample\"] =subsample\n",
    "    params[\"colsample_bytree\"]= colsample_bytree\n",
    "    params[\"silent\"] =1\n",
    "    params[\"seed\"] =random_state\n",
    "    params[\"gamma\"] =0.1\n",
    "    params[\"min_child_weight\"] =2\n",
    "    params[\"scale_pos_weight\"]=2\n",
    "    params[\"seed\"]=27\n",
    "\n",
    "    xgboost_to_xgb={\n",
    "    \n",
    "    \"learning_rate\" : \"eta\",\n",
    "    \"reg_alpha\" : \"alpha\",\n",
    "   \n",
    "    \"reg_lambda\" : \"lambda\" }\n",
    "\n",
    "#    Parameters from previous run, if any\n",
    "\n",
    "    if type(params_model) is OrderedDict:\n",
    "        \n",
    "        for item in params_model:\n",
    "        \n",
    "            params[item]=params_model[item]\n",
    "            \n",
    "        print(params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#   Using best parameters to train model \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    if CSPkey1 and mode!=0:\n",
    "        \n",
    "        seq_chosen=seq_number\n",
    "        csp_components=csp_n\n",
    "        \n",
    "        print('test length ',test.shape[0])\n",
    "        \n",
    "        for item in range(test.shape[0]):\n",
    "            \n",
    "            if item==0:\n",
    "                \n",
    "                test_3d_pre=test[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "                test_3d=np.reshape(test_3d_pre,newshape=(num_features, channels), order='F')\n",
    "                continue\n",
    "                \n",
    "            test_3d_pre = test[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "            test_red=np.reshape(test_3d_pre,newshape=(num_features, channels), order='F')\n",
    "            \n",
    "            test_3d=np.dstack((test_3d,test_red))\n",
    "          \n",
    "        \n",
    "        \n",
    "        csp_test=test_3d.transpose((2,1,0))\n",
    "        print('csp_test',csp_test.shape) \n",
    "                        \n",
    "    if GridSearch and CSPkey:\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    if GridSearch and CSPkey==False and CSPkey1==False:\n",
    "        \n",
    "        pass\n",
    "            \n",
    "            \n",
    "    if GridSearch and CSPkey1 and mode!=0:\n",
    "        \n",
    "        for item in range(train.shape[0]):\n",
    "            \n",
    "            if item==0:\n",
    "                \n",
    "                train_3d_pre=train[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "                train_3d=np.reshape(train_3d_pre,newshape=(num_features, channels), order='F')\n",
    "                continue\n",
    "                \n",
    "            train_3d_pre = train[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "            train_red=np.reshape(train_3d_pre,newshape=(num_features, channels), order='F')\n",
    "            \n",
    "            train_3d=np.dstack((train_3d,test_red))\n",
    "          \n",
    "        \n",
    "        \n",
    "        csp_train_gs=train_3d.transpose((2,1,0))\n",
    "        print('csp_train_gs',csp_train_gs.shape, csp_train_gs )\n",
    "        \n",
    "        CSPtest=CSP(n_components=csp_components, transform_into='csp_space')\n",
    "        \n",
    "        target_gs=train[target].values.astype(np.int64)\n",
    "        \n",
    "        print('target gs',target_gs.shape, target_gs)\n",
    "        \n",
    "        CSPtest.fit(csp_train_gs,target_gs)\n",
    "        \n",
    "        csp_train_gs_final=CSPtest.transform(csp_train_gs)\n",
    "\n",
    "        #csp_train_gs_final=CSPtest.fit_transform(train[fea.values,target_gs)\n",
    "        \n",
    "        print('csp_train_gs_final',csp_train_gs_final.shape, csp_train_gs_final)\n",
    "        \n",
    "        \n",
    "        train_Id=train['Id'].values.astype(np.int64)\n",
    "        train_seq_Id=train['sequence_id'].values.astype(np.int64)\n",
    "        train_patient_Id=train['patient_id'].values.astype(np.int64)\n",
    "        train_result=train[target].values.astype(np.int64)\n",
    "        \n",
    "        \n",
    "        csp_train_gs_final_index=np.column_stack((train_Id, train_seq_Id, train_patient_Id,\n",
    "                                                  csp_train_gs_final, train_result,\n",
    "                                                  train.index.values.astype(np.int64)))\n",
    "        \n",
    "        print('csp_train_gs_final_index',csp_train_gs_final_index)\n",
    "        \n",
    "        csp_train_gs_f_index=csp_train_gs_final_index[~np.any(np.isinf(csp_train_gs_final_index), axis=1)]\n",
    "        \n",
    "        #print('csp_train_gs_f_index',csp_train_gs_f_index)\n",
    "        index_csp_train_gs=csp_train_gs_f_index[:,csp_train_gs_f_index.shape[1]-1].astype(np.int64)\n",
    "        \n",
    "        #print('index_csp_train_gs',index_csp_train_gs)\n",
    "        \n",
    "        csp_train_gs_f=np.delete(csp_train_gs_f_index,csp_train_gs_f_index.shape[1]-1, 1)\n",
    "        \n",
    "        #print('csp_train_gs_f',csp_train_gs_f)\n",
    "        \n",
    "        features_names=['feature'+str(i) for i in range(channels)]\n",
    "        \n",
    "        train_gs_columns=['Id','sequence_id', 'patient_id']+features_names+['result']\n",
    "        \n",
    "        \n",
    "        train_gs_f=pd.DataFrame(csp_train_gs_f, index=index_csp_train_gs, columns =train_gs_columns)\n",
    "        \n",
    "        #print('train gs f',train_gs_f)\n",
    "    \n",
    "        splitKF=kf.split(unique_seq_X, unique_seq_y)\n",
    "        \n",
    "        #print('splitKF',splitKF)\n",
    "    \n",
    "        best_param=param_search_embedded(nfolds, train_gs_columns, target, splitKF, unique_sequences, \n",
    "                                         unique_sequences_fold,train_gs_f, PCAkeyGS)\n",
    "    \n",
    "        print('after best_param', best_param)\n",
    "        \n",
    "        \n",
    "        for key in best_param:\n",
    "            if key in xgboost_to_xgb:   \n",
    "                best_param[xgboost_to_xgb[key]]=best_param[key]\n",
    "                del best_param[key]\n",
    "                \n",
    "        #print ('substitution', best_param)\n",
    "        params={key : best_param.get(key, value) for key, value in params.items()}\n",
    "        \n",
    "        print (params)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    if CSPkey and mode!=0:\n",
    "        \n",
    "        seq_chosen=seq_number\n",
    "        csp_components=csp_n\n",
    "\n",
    "#        train_seq_Id=train['sequence_id'].values.astype(np.int64)\n",
    "        \n",
    "#        files_id_test=test['Id'].tolist()\n",
    "#        \n",
    "#        print('files_number',len(files_id_test))\n",
    "#        \n",
    "#        file_name_test=[]\n",
    "#        \n",
    "#        for i,f_id in enumerate(files_id_test):\n",
    "#            \n",
    "##            if train_seq_Id[i]==seq_number:\n",
    "#                \n",
    "#            real_f_id=f_id % 100000   \n",
    "#            file_name_test.append(\"./test_\"+str(mode)+'_new/new_'+str(mode)+'_'+str(real_f_id)+'.mat')        \n",
    "#\n",
    "#\n",
    "#        print('files_id_test',len(files_id_test),'file_name_test',len(file_name_test))\n",
    "#        \n",
    "#        test_div=20\n",
    "#        \n",
    "#        parts_test=int(len(file_name_test)/test_div)+1\n",
    "#        \n",
    "#        print('parts_test', parts_test)\n",
    "#        \n",
    "#        csp_test=[]\n",
    "#        \n",
    "#        for k in range(parts_test):\n",
    "#        \n",
    "#            track1=0\n",
    "#            \n",
    "#            ki=k\n",
    "#            if k==(parts_test-1):\n",
    "#                kfin=len(file_name_test)\n",
    "#                print('kfin',kfin)\n",
    "#            else:\n",
    "#                kfin=(k+1)*test_div\n",
    "#                print('kfin',kfin)\n",
    "#\n",
    "#            for i, fl1 in enumerate(file_name_test[ki*test_div:kfin]):\n",
    "#            #    print(i)\n",
    "#\n",
    "#                if i==track1:\n",
    "#        \n",
    "#                    print('checking')\n",
    "#     \n",
    "#\n",
    "#                    tables, sequence_from_mat, samp_freq = mat_to_pandas(fl1)\n",
    "#                \n",
    "#                    csp_left=int(csp_init*10*60*samp_freq)\n",
    "#                    csp_right=int(csp_end*10*60*samp_freq)\n",
    "#                \n",
    "#                    print('csp left right', csp_left, csp_right)\n",
    "#        \n",
    "#        \n",
    "#\n",
    "#                    data_csp_test=np.transpose(tables.values[csp_left:csp_right,:])\n",
    "#                \n",
    "#                \n",
    "#          \n",
    "#                    print('done!')\n",
    "#        \n",
    "#                    continue\n",
    "#              \n",
    "#   \n",
    "#                try:\n",
    "#                    tables1, sequence_from_mat1, samp_freq1 = mat_to_pandas(fl1)\n",
    "#                except:\n",
    "#                    print('Some error here {}...'.format(fl1))\n",
    "#                    continue\n",
    "#                \n",
    "#                csp_left=int(csp_init*10*60*samp_freq1)\n",
    "#                csp_right=int(csp_end*10*60*samp_freq1)\n",
    "#            \n",
    "#                print('csp left right', csp_left, csp_right)\n",
    "#            \n",
    "#        \n",
    "#                temp_matrix=np.transpose(tables1.values[csp_left:csp_right,:])\n",
    "#                data_csp_test=np.dstack((data_csp_test,temp_matrix))\n",
    "#                print('data_csp_test',data_csp_test.shape)\n",
    "#\n",
    "##            csp_test.append(data_csp_test.transpose((2,0,1)))\n",
    "#\n",
    "#            outfile = \"csp_test_\"+str(mode) +\"_\"+ \"part_\"+str(k)+ \".txt\"\n",
    "#            data_csp_test.transpose((2,0,1)).tofile(outfile)\n",
    "#\n",
    "#            del data_csp_test\n",
    "            \n",
    "#            print('csp_test',len(csp_test))     \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        if GridSearch:\n",
    "            \n",
    "            seq_chosen=seq_number\n",
    "            csp_components=csp_n\n",
    "        \n",
    "\n",
    "        \n",
    "            files_id_train=train['Id'].tolist()\n",
    "            results_id_train=train[target].tolist()\n",
    "        \n",
    "            print('files_number',len(files_id_train))\n",
    "            \n",
    "       \n",
    "            file_name_train=[]\n",
    "        \n",
    "            for i,f_id in enumerate(files_id_train):\n",
    "                \n",
    "                if (f_id % 1000) % 6 == 0:\n",
    "                     train_seq_Id=6\n",
    "                else:\n",
    "                     train_seq_Id=(f_id % 1000) % 6\n",
    "                \n",
    "                if train_seq_Id==seq_chosen:\n",
    "                \n",
    "                    real_f_id=f_id % 100000   \n",
    "                    file_name_train.append(\"./train_\"+str(mode)+'/'+str(mode)+'_'\n",
    "                                      +str(real_f_id)+'_'+str(results_id_train[i])+'.mat')        \n",
    "\n",
    "\n",
    "            print('files_id_train',len(files_id_train),'file_name_train',len(file_name_train))\n",
    "        \n",
    "       \n",
    "        \n",
    "            track1=0\n",
    "\n",
    "            for i, fl1 in enumerate(file_name_train):\n",
    "                #    print(i)\n",
    "\n",
    "                if i==track1:\n",
    "        \n",
    "                    print('checking')\n",
    "     \n",
    "\n",
    "                    tables, sequence_from_mat, samp_freq = mat_to_pandas(fl1)\n",
    "                \n",
    "                    csp_left=int(csp_init*10*60*samp_freq)\n",
    "                    csp_right=int(csp_end*10*60*samp_freq)\n",
    "                \n",
    "                    print('csp left right', csp_left, csp_right)\n",
    "        \n",
    "        \n",
    "                    if int(sequence_from_mat[0][0][0][0])==seq_chosen:\n",
    "                \n",
    "                        data_csp_train=tables.values[csp_left:csp_right,:]\n",
    "                        print('done!')\n",
    "                        continue\n",
    "                    \n",
    "                    else:\n",
    "                        track1+=1\n",
    "                        continue\n",
    "                \n",
    "   \n",
    "                try:\n",
    "                    tables1, sequence_from_mat1, samp_freq1 = mat_to_pandas(fl1)\n",
    "                except:\n",
    "                    print('Some error here {}...'.format(fl1))\n",
    "                    continue\n",
    "                \n",
    "                csp_left=int(csp_init*10*60*samp_freq1)\n",
    "                csp_right=int(csp_end*10*60*samp_freq1)\n",
    "            \n",
    "                print('csp left right', csp_left, csp_right)\n",
    "                \n",
    "                if int(sequence_from_mat1[0][0][0][0])==seq_chosen:\n",
    "        \n",
    "                    temp_matrix=tables1.values[csp_left:csp_right,:]\n",
    "                    data_csp_train=np.dstack((data_csp_train,temp_matrix))\n",
    "                    print('data_csp_train',data_csp_train.shape)\n",
    "\n",
    "            csp_train_gs=data_csp_train.transpose((2,1,0))\n",
    "\n",
    "            print('csp_test',csp_train.shape, csp_train_gs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            CSPtest=CSP(n_components=csp_components)\n",
    "        \n",
    "            target_gs=train[target].values.astype(np.int64)\n",
    "        \n",
    "            print('target gs',target_gs.shape, target_gs)\n",
    "        \n",
    "            CSPtest.fit(csp_train_gs,target_gs)\n",
    "        \n",
    "            csp_train_gs_final=CSPtest.transform(csp_train_gs)\n",
    "\n",
    "            #csp_train_gs_final=CSPtest.fit_transform(train[fea.values,target_gs)\n",
    "        \n",
    "            print('csp_train_gs_final',csp_train_gs_final.shape, csp_train_gs_final)\n",
    "        \n",
    "        \n",
    "            train_Id=train['Id'].values.astype(np.int64)\n",
    "            train_seq_Id=train['sequence_id'].values.astype(np.int64)\n",
    "            train_patient_Id=train['patient_id'].values.astype(np.int64)\n",
    "            train_result=train[target].values.astype(np.int64)\n",
    "        \n",
    "        \n",
    "            csp_train_gs_final_index=np.column_stack((train_Id, train_seq_Id, train_patient_Id,\n",
    "                                                  csp_train_gs_final, train_result,\n",
    "                                                  train.index.values.astype(np.int64)))\n",
    "        \n",
    "            print('csp_train_gs_final_index',csp_train_gs_final_index)\n",
    "            \n",
    "            csp_train_gs_f_index=csp_train_gs_final_index[~np.any(np.isinf(csp_train_gs_final_index), axis=1)]\n",
    "        \n",
    "            #print('csp_train_gs_f_index',csp_train_gs_f_index)\n",
    "            index_csp_train_gs=csp_train_gs_f_index[:,csp_train_gs_f_index.shape[1]-1].astype(np.int64)\n",
    "        \n",
    "            #print('index_csp_train_gs',index_csp_train_gs)\n",
    "        \n",
    "            csp_train_gs_f=np.delete(csp_train_gs_f_index,csp_train_gs_f_index.shape[1]-1, 1)\n",
    "        \n",
    "            #print('csp_train_gs_f',csp_train_gs_f)\n",
    "        \n",
    "            features_names=['feature'+str(i) for i in range(channels)]\n",
    "        \n",
    "            train_gs_columns=['Id','sequence_id', 'patient_id']+features_names+['result']\n",
    "        \n",
    "        \n",
    "            train_gs_f=pd.DataFrame(csp_train_gs_f, index=index_csp_train_gs, columns =train_gs_columns)\n",
    "            \n",
    "            #print('train gs f',train_gs_f)\n",
    "    \n",
    "            splitKF=kf.split(unique_seq_X, unique_seq_y)\n",
    "        \n",
    "            #print('splitKF',splitKF)\n",
    "    \n",
    "            best_param=param_search_embedded(nfolds, train_gs_columns, target, splitKF, unique_sequences, \n",
    "                                         unique_sequences_fold,train_gs_f, PCAkeyGS)\n",
    "    \n",
    "            print('after best_param', best_param)\n",
    "        \n",
    "        \n",
    "            for key in best_param:\n",
    "                if key in xgboost_to_xgb:   \n",
    "                    best_param[xgboost_to_xgb[key]]=best_param[key]\n",
    "                    del best_param[key]\n",
    "                \n",
    "            #print ('substitution', best_param)\n",
    "            params={key : best_param.get(key, value) for key, value in params.items()}\n",
    "        \n",
    "            print (params)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    for train_seq_index, test_seq_index in kf.split(unique_seq_X, unique_seq_y):\n",
    "        num_fold += 1\n",
    "        print('Start fold {} from {}'.format(num_fold, nfolds))\n",
    "        train_seq = unique_sequences[train_seq_index]\n",
    "        valid_seq = unique_sequences[test_seq_index]\n",
    "        print('Length of train people: {}'.format(len(train_seq)))\n",
    "        print('Length of valid people: {}'.format(len(valid_seq)))\n",
    "        \n",
    "#        print('train_seq',train_seq)\n",
    "#        print('valid_seq',valid_seq)\n",
    "\n",
    "        X_train, X_valid = train[unique_sequences_fold.isin(train_seq)][features], train[unique_sequences_fold.isin(valid_seq)][features]\n",
    "        y_train, y_valid = train[unique_sequences_fold.isin(train_seq)][target], train[unique_sequences_fold.isin(valid_seq)][target]\n",
    "        X_test = test[features]\n",
    "        \n",
    "        X_train_seq, X_valid_seq =train[unique_sequences_fold.isin(train_seq)]['sequence_id'],\\\n",
    "                                    train[unique_sequences_fold.isin(valid_seq)]['sequence_id']\n",
    "        \n",
    "        print('X_train index',X_train.index)\n",
    "        print('X_valid index',y_train.index)\n",
    "        print('X_test index', X_test.index.shape)\n",
    "\n",
    "        print('Length train:', len(X_train))\n",
    "        print('Length valid:', len(X_valid))\n",
    "        \n",
    "        print('X_train_seq', X_train_seq.shape)\n",
    "        print('X_valid_seq', X_valid_seq.shape)\n",
    "        \n",
    "#       Scaling for PCA\n",
    "\n",
    "\n",
    "        scaler = MinMaxScaler()   \n",
    "        \n",
    "        Xtrain_scaled=pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        Xvalid_scaled=pd.DataFrame(scaler.fit_transform(X_valid), columns=X_valid.columns, index=X_valid.index )\n",
    "\n",
    "        Xtest_scaled=pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "\n",
    "        if PCAgraph:\n",
    "            \n",
    "            pcatest=KernelPCA(n_components=20)\n",
    "            pcatest.fit(Xtrain_scaled)\n",
    "            var1=np.cumsum(np.round(pcatest.explained_variance_ratio_, decimals=4)*100)\n",
    "            f1 = plt.figure()\n",
    "            print(var1)\n",
    "            plt.plot(var1)\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "        if PCAkey:      \n",
    "        \n",
    "#       PCA transformation \n",
    "            pcatest=PCA(n_components=20)\n",
    "            X_train_f=pd.DataFrame(pcatest.fit_transform(Xtrain_scaled), index=Xtrain_scaled.index)\n",
    "            X_valid_f=pd.DataFrame(pcatest.fit_transform(Xvalid_scaled), index=Xvalid_scaled.index)\n",
    "\n",
    "            X_test_f=pd.DataFrame(pcatest.fit_transform(Xtest_scaled), index=Xtest_scaled.index)\n",
    "\n",
    "\n",
    "        \n",
    "        if CSPkey1 and mode!=0:\n",
    "            \n",
    "            print('train length ', X_train.shape[0])\n",
    "            \n",
    "            for item in range(X_train.shape[0]):\n",
    "            \n",
    "                if item==0:\n",
    "                \n",
    "                    X_train_3d_pre=X_train[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "                    X_train_3d=np.reshape(X_train_3d_pre,newshape=(num_features, channels), order='F')\n",
    "                    continue\n",
    "                \n",
    "                X_train_3d_pre = X_train[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "                X_train_red=np.reshape(X_train_3d_pre,newshape=(num_features, channels), order='F')\n",
    "                X_train_3d=np.dstack((X_train_3d,X_train_red))\n",
    "          \n",
    "        \n",
    "        \n",
    "            csp_train=X_train_3d.transpose((2,1,0))\n",
    "            \n",
    "            \n",
    "            print('valid length ',X_valid.shape[0])\n",
    "            \n",
    "            for item in range(X_valid.shape[0]):\n",
    "            \n",
    "                if item==0:\n",
    "                \n",
    "                    X_valid_3d_pre=X_valid[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "                    X_valid_3d=np.reshape(X_valid_3d_pre,newshape=(num_features, channels), order='F')\n",
    "                    continue\n",
    "                \n",
    "                X_valid_3d_pre = X_valid[features].iloc[[item]].drop(['file_size','patient_id'],1).values\n",
    "                X_valid_red=np.reshape(X_valid_3d_pre,newshape=(num_features, channels), order='F')\n",
    "                X_valid_3d=np.dstack((X_valid_3d,X_valid_red))\n",
    "          \n",
    "        \n",
    "        \n",
    "            csp_valid=X_valid_3d.transpose((2,1,0))\n",
    "            \n",
    "            y_csp_train=y_train.values\n",
    "            y_csp_valid=y_valid.values\n",
    "            \n",
    "            \n",
    "            print('csp_train',csp_train.shape)\n",
    "            print('csp_valid',csp_valid.shape)\n",
    "            print('csp_test',csp_test.shape) \n",
    "            \n",
    "            CSPtest=CSP(n_components=csp_components)\n",
    "        \n",
    "            csp_train_final=CSPtest.fit_transform(csp_train, y_csp_train)\n",
    "            csp_valid_final=CSPtest.transform(csp_valid)\n",
    "            csp_test_final=CSPtest.transform(csp_test)\n",
    "            \n",
    "            print('csp train final' ,csp_train_final.shape, 'csp valid final', csp_valid_final.shape)\n",
    "            print('csp test final' ,csp_test_final.shape)\n",
    "            \n",
    "            csp_train_final_index=np.column_stack((csp_train_final, X_train.index.values))\n",
    "            csp_valid_final_index=np.column_stack((csp_valid_final, X_valid.index.values))\n",
    "            csp_test_final_index=np.column_stack((csp_test_final, X_test.index.values))\n",
    "            \n",
    "            csp_test_final_index[csp_test_final_index == inf] = 100000\n",
    "            csp_test_final_index[csp_test_final_index == -inf] = 100000\n",
    "            \n",
    "            print('csp train final_index' ,csp_train_final_index.shape, 'csp valid final_index', csp_valid_final_index.shape)\n",
    "            print('csp test final index' ,csp_test_final_index.shape)\n",
    "\n",
    "            \n",
    "            csp_train_f_index=csp_train_final_index[~np.any(np.isinf(csp_train_final_index), axis=1)] \n",
    "            csp_valid_f_index=csp_valid_final_index[~np.any(np.isinf(csp_valid_final_index), axis=1)]\n",
    "            csp_test_f_index=csp_test_final_index[~np.any(np.isinf(csp_test_final_index), axis=1)]\n",
    "            \n",
    "            print('csp train f index' ,csp_train_f_index.shape, 'csp valid f index', csp_valid_f_index.shape)\n",
    "            print('csp test f index' ,csp_test_f_index.shape)\n",
    "            \n",
    "            index_csp_train=csp_train_f_index[:,csp_train_f_index.shape[1]-1].astype(np.int64)\n",
    "            index_csp_valid=csp_valid_f_index[:,csp_valid_f_index.shape[1]-1].astype(np.int64)\n",
    "            index_csp_test=csp_test_f_index[:,csp_test_f_index.shape[1]-1].astype(np.int64)\n",
    "            \n",
    "            print('index csp train' ,index_csp_train.shape, 'index csp valid', index_csp_valid.shape)\n",
    "            print('index_csp_test' ,index_csp_test.shape)\n",
    "            \n",
    "            \n",
    "            csp_train_f=np.delete(csp_train_f_index,csp_train_f_index.shape[1]-1, 1)\n",
    "            csp_valid_f=np.delete(csp_valid_f_index,csp_valid_f_index.shape[1]-1, 1)\n",
    "            csp_test_f=np.delete(csp_test_f_index,csp_test_f_index.shape[1]-1, 1)\n",
    "            \n",
    "            print('csp train f' ,csp_train_f.shape, 'csp valid f', csp_valid_f.shape)\n",
    "            print('csp test f' ,csp_test_f.shape)\n",
    "            \n",
    "            X_train_f=pd.DataFrame(csp_train_f, index=index_csp_train)\n",
    "            X_valid_f=pd.DataFrame(csp_valid_f, index=index_csp_valid)\n",
    "\n",
    "            X_test_f=pd.DataFrame(csp_test_f, index=index_csp_test)\n",
    "            \n",
    "            \n",
    "            #print('X_train_f',X_train_f.shape,'X_valid_f', X_valid_f.shape, 'X_test_f', X_test_f.shape )\n",
    "            #print('y_train',y_train.shape, 'y_valid', y_valid.shape)\n",
    "            \n",
    "            print('X_train_f',X_train_f.shape,'X_valid_f', X_valid_f.shape, 'X_test_f', X_test_f.shape )\n",
    "            print('y_train',y_train.shape, 'y_valid', y_valid.shape)\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            X_train_f=X_train\n",
    "            X_valid_f=X_valid\n",
    "            X_test_f=X_test\n",
    "            \n",
    "            \n",
    "        \n",
    "        if CSPkey and mode!=0:\n",
    "            \n",
    "            seq_chosen=seq_number\n",
    "            csp_components=csp_n            \n",
    "            \n",
    "            #X_train, X_valid = train[(unique_sequences_fold.isin(train_seq)) & ((X_train_seq % 1000) % 6 == seq_chosen)][features],\\\n",
    "                                #train[(unique_sequences_fold.isin(valid_seq))&((X_valid_seq % 1000) % 6 == seq_chosen)][features]\n",
    "            #y_train, y_valid = train[(unique_sequences_fold.isin(train_seq)) & ((X_train_seq % 1000) % 6 == seq_chosen)][target],\\\n",
    "                                #train[(unique_sequences_fold.isin(valid_seq)) &((X_valid_seq % 1000) % 6 == seq_chosen)][target]\n",
    "            \n",
    "            print('X_train csp',X_train.shape)\n",
    "            print('X_valid csp',X_valid.shape)\n",
    "            print('y_train csp', y_train.shape)\n",
    "            print('y_valid csp', y_valid.shape)\n",
    "            \n",
    "            \n",
    "            #taking the 'Id' files from fold\n",
    "            \n",
    "            #files_id_train=train[unique_sequences_fold.isin(train_seq)&((X_train_seq % 1000) % 6 == seq_chosen)]['Id'].tolist()\n",
    "            files_id_train=train[unique_sequences_fold.isin(train_seq)]['Id'].tolist()\n",
    "            results_id_train=y_train.tolist()\n",
    "            \n",
    "            \n",
    "            #files_id_valid=train[unique_sequences_fold.isin(valid_seq)&((X_valid_seq % 1000) % 6 == seq_chosen)]['Id'].tolist()\n",
    "            files_id_valid=train[unique_sequences_fold.isin(valid_seq)]['Id'].tolist()\n",
    "            results_id_valid=y_valid.tolist()\n",
    "            \n",
    "            \n",
    "            print('files_id_train',len(files_id_train),'results_id_train',len(results_id_train))\n",
    "            print('files_id_valid',len(files_id_valid),'results_id_valid',len(results_id_valid))\n",
    "            \n",
    "            file_name_train=[]\n",
    "            file_name_valid=[]\n",
    "            \n",
    "            for i,f_id in enumerate(files_id_train):\n",
    "                \n",
    "#                sequence_validator=(X_train_seq[i] % 1000) % 6\n",
    "                \n",
    "#                print('sequence validator', sequence_validator)\n",
    "                \n",
    "                \n",
    "#                if sequence_validator==seq_chosen:\n",
    "                \n",
    "                real_f_id=f_id % 100000 \n",
    "                file_name_train.append(\"./train_\"+str(mode)+'/'+str(mode)+'_'+str(real_f_id)+'_'\n",
    "                                           +str(results_id_train[i])+'.mat')        \n",
    "\n",
    "\n",
    "            print('files_id_train',len(files_id_train),'results_id_train',len(results_id_train),'file_name_train',len(file_name_train))\n",
    "\n",
    "            \n",
    "            \n",
    "            for i,f_id in enumerate(files_id_valid):\n",
    "                \n",
    "#                sequence_validator=(X_valid_seq[i] % 1000) % 6\n",
    "                \n",
    "#                if sequence_validator==seq_chosen:\n",
    "                \n",
    "                real_f_id=f_id % 100000 \n",
    "                file_name_valid.append(\"./train_\"+str(mode)+'/'+str(mode)+'_'+str(real_f_id)+'_'\n",
    "                                           +str(results_id_valid[i])+'.mat')        \n",
    "\n",
    "\n",
    "            print('files_id_valid',len(files_id_valid),'results_id_valid',len(results_id_valid),'file_name',len(file_name_valid))\n",
    "\n",
    "            \n",
    "    \n",
    "#            result_list_train=[]\n",
    "#            result_list_valid=[]\n",
    "            track1=0\n",
    "        \n",
    "            for i, fl1 in enumerate(file_name_train):\n",
    "                #    print(i)\n",
    "                \n",
    "#                result = results_id_train[i]\n",
    "                if i==track1:\n",
    "                    print('checking')\n",
    "                    \n",
    "                    tables, sequence_from_mat, samp_freq = mat_to_pandas(fl1)\n",
    "                    csp_left=int(csp_init*10*60*samp_freq)\n",
    "                    csp_right=int(csp_end*10*60*samp_freq)\n",
    "\n",
    "                    data_csp_train=np.transpose(tables.values[csp_left:csp_right,:])\n",
    "                    \n",
    "                    if int(sequence_from_mat[0][0][0][0])!=seq_chosen:\n",
    "                        print('error train seq here!',int(sequence_from_mat[0][0][0][0]), seq_chosen, fl1)\n",
    "        \n",
    "#                    result_list_train.append(result)\n",
    "        \n",
    "                    print('done!')\n",
    "        \n",
    "                    continue\n",
    "              \n",
    "   \n",
    "                try:\n",
    "                    tables1, sequence_from_mat1, samp_freq1 = mat_to_pandas(fl1)\n",
    "                    csp_left=int(csp_init*10*60*samp_freq)\n",
    "                    csp_right=int(csp_end*10*60*samp_freq)\n",
    "                    \n",
    "                except:\n",
    "                    print('Some error here {}...'.format(fl))\n",
    "                    continue\n",
    "    \n",
    "#                if sequence_from_mat1==seq_chosen:\n",
    "                if int(sequence_from_mat1[0][0][0][0])!=seq_chosen:\n",
    "                        print('error train seq here!',int(sequence_from_mat1[0][0][0][0]), seq_chosen, fl1)\n",
    "        \n",
    "                temp_matrix=np.transpose(tables1.values[csp_left:csp_right,:])\n",
    "                data_csp_train=np.dstack((data_csp_train,temp_matrix))\n",
    "#                result_list_train.append(result)\n",
    "                \n",
    "#                print(data_csp_train.shape)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            for i, fl1 in enumerate(file_name_valid):\n",
    "                #    print(i)\n",
    "                \n",
    "#                result = results_id_valid[i]\n",
    "                if i==track1:\n",
    "        \n",
    "                    print('checking')\n",
    "                  \n",
    "        \n",
    "                    tables, sequence_from_mat, samp_freq = mat_to_pandas(fl1)\n",
    "                    csp_left=int(csp_init*10*60*samp_freq)\n",
    "                    csp_right=int(csp_end*10*60*samp_freq)\n",
    "\n",
    "                    data_csp_valid=np.transpose(tables.values[csp_left:csp_right,:])\n",
    "                \n",
    "                    if int(sequence_from_mat[0][0][0][0])!=seq_chosen:\n",
    "                        print('error valid seq here!',int(sequence_from_mat[0][0][0][0]), seq_chosen, fl1)\n",
    "        \n",
    "#                    result_list_valid.append(result)\n",
    "        \n",
    "                    print('done!')\n",
    "        \n",
    "                    continue\n",
    "              \n",
    "   \n",
    "                try:\n",
    "                    tables1, sequence_from_mat1, samp_freq1 = mat_to_pandas(fl1)\n",
    "                    csp_left=int(csp_init*10*60*samp_freq)\n",
    "                    csp_right=int(csp_end*10*60*samp_freq)\n",
    "                except:\n",
    "                    print('Some error here {}...'.format(fl1))\n",
    "                    continue\n",
    "    \n",
    "#                if sequence_from_mat1==seq_chosen:\n",
    "                if int(sequence_from_mat1[0][0][0][0])!=seq_chosen:\n",
    "                    print('error valid seq here!',int(sequence_from_mat1[0][0][0][0]), seq_chosen, fl1)\n",
    "\n",
    "                temp_matrix=np.transpose(tables1.values[csp_left:csp_right,:])\n",
    "                data_csp_valid=np.dstack((data_csp_valid,temp_matrix))\n",
    "#                result_list_valid.append(result)\n",
    "                \n",
    "#                print(data_csp_valid.shape)\n",
    "\n",
    "#            y_csp_train=np.array(result_list_train)\n",
    "#            y_csp_valid=np.array(result_list_valid)\n",
    "\n",
    "            y_csp_train=y_train.values\n",
    "            y_csp_valid=y_valid.values\n",
    "                   \n",
    "            \n",
    "            print('y_csp_train',y_csp_train.shape,'y_csp_valid', y_csp_valid.shape)\n",
    "            print('y_csp_train',y_csp_train,'y_csp_valid', y_csp_valid)\n",
    "\n",
    "\n",
    "            #print(data_csp.shape)    \n",
    "\n",
    "            csp_train=data_csp_train.transpose((2,0,1))\n",
    "            csp_valid=data_csp_valid.transpose((2,0,1))\n",
    "            \n",
    "#            print('csp_train',csp_train.shape,'csp_valid', csp_valid.shape)\n",
    "#            print('csp_train',type(csp_train),'csp_valid', type(csp_valid))\n",
    "\n",
    "            #print(csp_data.shape)     \n",
    "    \n",
    "            CSPtest=CSP(n_components=csp_components)\n",
    "        \n",
    "            csp_train_final=CSPtest.fit_transform(csp_train, y_csp_train)\n",
    "            csp_valid_final=CSPtest.transform(csp_valid)\n",
    "            \n",
    "            \n",
    "            track1=0\n",
    "            \n",
    "            print('starting reading test files bits')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            files_id_test=test['Id'].tolist()\n",
    "        \n",
    "            print('files_number',len(files_id_test))\n",
    "        \n",
    "            file_name_test=[]\n",
    "        \n",
    "            for i,f_id in enumerate(files_id_test):\n",
    "            \n",
    "#            if train_seq_Id[i]==seq_number:\n",
    "                \n",
    "                real_f_id=f_id % 100000   \n",
    "                file_name_test.append(\"./test_\"+str(mode)+'_new/new_'+str(mode)+'_'+str(real_f_id)+'.mat')        \n",
    "\n",
    "\n",
    "            print('files_id_test',len(files_id_test),'file_name_test',len(file_name_test))\n",
    "            \n",
    "            test_div=20\n",
    "            \n",
    "            parts_test=int(len(file_name_test)/test_div)+1\n",
    "            \n",
    "            print('parts_test', parts_test)\n",
    "            \n",
    "            csp_test=[]\n",
    "            \n",
    "            track=0\n",
    "            \n",
    "            for k in range(parts_test):\n",
    "            \n",
    "                track1=0\n",
    "                \n",
    "                \n",
    "                ki=k\n",
    "                if k==(parts_test-1):\n",
    "                    kfin=len(file_name_test)\n",
    "                    print('kfin',kfin)\n",
    "                else:\n",
    "                    kfin=(k+1)*test_div\n",
    "                    print('kfin',kfin)\n",
    "    \n",
    "                for i, fl1 in enumerate(file_name_test[ki*test_div:kfin]):\n",
    "                #    print(i)\n",
    "    \n",
    "                    if i==track1:\n",
    "            \n",
    "                        print('checking')\n",
    "         \n",
    "    \n",
    "                        tables, sequence_from_mat, samp_freq = mat_to_pandas(fl1)\n",
    "                    \n",
    "                        csp_left=int(csp_init*10*60*samp_freq)\n",
    "                        csp_right=int(csp_end*10*60*samp_freq)\n",
    "                    \n",
    " #                       print('csp left right', csp_left, csp_right)\n",
    "            \n",
    "            \n",
    "    \n",
    "                        data_csp_test=np.transpose(tables.values[csp_left:csp_right,:])\n",
    "                    \n",
    "                    \n",
    "              \n",
    "                        print('done!')\n",
    "            \n",
    "                        continue\n",
    "                  \n",
    "       \n",
    "                    try:\n",
    "                        tables1, sequence_from_mat1, samp_freq1 = mat_to_pandas(fl1)\n",
    "                    except:\n",
    "                        print('Some error here {}...'.format(fl1))\n",
    "                        continue\n",
    "                    \n",
    "                    csp_left=int(csp_init*10*60*samp_freq1)\n",
    "                    csp_right=int(csp_end*10*60*samp_freq1)\n",
    "                \n",
    "#                    print('csp left right', csp_left, csp_right)\n",
    "                \n",
    "            \n",
    "                    temp_matrix=np.transpose(tables1.values[csp_left:csp_right,:])\n",
    "                    data_csp_test=np.dstack((data_csp_test,temp_matrix))\n",
    "#                    print('data_csp_test',data_csp_test.shape)\n",
    "                    \n",
    "                    del temp_matrix\n",
    "                \n",
    "                temp_test_csp=CSPtest.transform(data_csp_test.transpose((2,0,1)))\n",
    "                \n",
    "#                print('test shape bit',temp_test_csp.shape)\n",
    "                \n",
    "                if k==track:\n",
    "                    csp_test_final=temp_test_csp\n",
    "                    continue\n",
    "                    \n",
    "                csp_test_final=np.concatenate((csp_test_final,temp_test_csp))\n",
    "    \n",
    "    #            csp_test.append(data_csp_test.transpose((2,0,1)))\n",
    "    \n",
    "#                outfile = \"csp_test_\"+str(mode) +\"_\"+ \"part_\"+str(k)+ \".txt\"\n",
    "#                data_csp_test.transpose((2,0,1)).tofile(outfile)\n",
    "    \n",
    "                del data_csp_test\n",
    "                del temp_test_csp\n",
    "            \n",
    "#            print('csp_test',len(csp_test))             \n",
    "        \n",
    "            \n",
    "            \n",
    "                                 \n",
    "#            for k in range(parts_test):\n",
    "#                \n",
    "#                if track1==k:\n",
    "#                    \n",
    "#                    outfile = \"csp_test_\"+str(mode) +\"_\"+ \"part_\"+str(k)+ \".txt\"\n",
    "#                    temp_test_csp=np.fromfile(outfile)\n",
    "#                    \n",
    "#                    csp_test_final=CSPtest.transform(temp_test_csp)\n",
    "#                    continue\n",
    "#                    \n",
    "#                outfile = \"csp_test_\"+str(mode) +\"_\"+ str(patient_id) + \"part_\"+str(k)+ \".csv\"\n",
    "#                temp_test_csp=np.fromfile(outfile)\n",
    "#                temp_test_csp1=CSPtest.transform(temp_test_csp)\n",
    "#                csp_test_final=np.concatenate((csp_test_final,temp_test_csp1))\n",
    "                print('done! ',k)\n",
    "            \n",
    "            print('csp train final' ,csp_train_final.shape,\n",
    "                  'csp valid final', csp_valid_final.shape, 'csp_test_final', csp_test_final.shape)\n",
    "            \n",
    "            csp_train_final_index=np.column_stack((csp_train_final, y_train.values, X_train.index.values))\n",
    "            csp_valid_final_index=np.column_stack((csp_valid_final, y_valid.values, X_valid.index.values))\n",
    "            csp_test_final_index=np.column_stack((csp_test_final, X_test.index.values))\n",
    "            \n",
    "#            print('csp train final_index' ,csp_train_final_index, 'csp valid final_index', csp_valid_final_index)\n",
    "            \n",
    "            \n",
    "            csp_train_f_index=csp_train_final_index[~np.any(np.isinf(csp_train_final_index), axis=1)] \n",
    "            csp_valid_f_index=csp_valid_final_index[~np.any(np.isinf(csp_valid_final_index), axis=1)]\n",
    " #           csp_test_f_index=csp_test_final_index[~np.any(np.isinf(csp_test_final_index), axis=1)]\n",
    "            csp_test_final_index[np.isinf(csp_test_final_index)]=0\n",
    "            csp_test_f_index=csp_test_final_index\n",
    "\n",
    "#           replace_test_csp=np.isinf()\n",
    "#           print('post_test_csp',replace_test_csp)\n",
    "\n",
    "            \n",
    "            \n",
    "#            print('csp train f index' ,csp_train_f_index, 'csp valid f index', csp_valid_f_index)\n",
    "            \n",
    "            index_csp_train=csp_train_f_index[:,csp_train_f_index.shape[1]-1]\n",
    "            index_csp_valid=csp_valid_f_index[:,csp_valid_f_index.shape[1]-1]\n",
    "            index_csp_test=csp_test_f_index[:,csp_test_f_index.shape[1]-1]\n",
    "            \n",
    "#            print('index csp train' ,index_csp_train, 'index csp valid', index_csp_valid)\n",
    "\n",
    "            y_train=csp_train_f_index[:,csp_train_f_index.shape[1]-2]\n",
    "            y_valid=csp_valid_f_index[:,csp_valid_f_index.shape[1]-2]\n",
    "          \n",
    "#            print('index csp train' ,index_csp_train, 'index csp valid', index_csp_valid)\n",
    "            \n",
    "            \n",
    "            csp_train_f=np.delete(csp_train_f_index,np.s_[csp_train_f_index.shape[1]-2,csp_train_f_index.shape[1]-1], 1)\n",
    "            csp_valid_f=np.delete(csp_valid_f_index,np.s_[csp_train_f_index.shape[1]-2,csp_train_f_index.shape[1]-1], 1)\n",
    "            csp_test_f=np.delete(csp_test_f_index,csp_test_f_index.shape[1]-1, 1)\n",
    "            \n",
    "            print('csp train f' ,csp_train_f.shape, 'csp valid f', csp_valid_f.shape, 'csp_test_f', csp_test_f.shape)\n",
    "            \n",
    "            X_train_f=pd.DataFrame(csp_train_f, index=np.int64(index_csp_train))\n",
    "            X_valid_f=pd.DataFrame(csp_valid_f, index=np.int64(index_csp_valid))\n",
    "\n",
    "            X_test_f=pd.DataFrame(csp_test_f, index=np.int64(index_csp_test))\n",
    "            \n",
    "            \n",
    "            print('X_train_f',X_train_f.shape,'X_valid_f', X_valid_f.shape, 'X_test_f', X_test_f.shape )\n",
    "            print('y_train',y_train.shape, 'y_valid', y_valid.shape)\n",
    "            \n",
    "            #print('X_train_f',X_train_f,'X_valid_f', X_valid_f, 'X_test_f', X_test_f )\n",
    "            #print('y_train',y_train, 'y_valid', y_valid)\n",
    "            \n",
    "\n",
    "#       SMOTE oversampling\n",
    "        \n",
    "#        print('Original dataset shape {}'.format(Counter(y_train)))\n",
    "#        print('Original dataset shape {}'.format(Counter(X_train_f)))\n",
    "#        print(X_train_f)\n",
    "#        print(y_train)\n",
    "\n",
    "        if Oversampling:\n",
    "        \n",
    "            sm1 = SMOTETomek(random_state=42)\n",
    "            X_res,y_res = sm1.fit_sample(X_train_f,y_train)\n",
    "            X_train_f=pd.DataFrame(X_res, columns=X_train_f.columns)\n",
    "            y_train=pd.Series(y_res)# Does it need the index of y_train?\n",
    "        \n",
    "#        print('Resampled dataset shape {}'.format(Counter(y_train)))\n",
    "#        print(X_train_f)\n",
    "#        print(y_train)\n",
    "    \n",
    "    \n",
    "                        \n",
    "        \n",
    "        \n",
    "        \n",
    "#       Preparation for XGB training\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_f, y_train)\n",
    "        dvalid = xgb.DMatrix(X_valid_f, y_valid)\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]       \n",
    "        \n",
    "        gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n",
    "        yhat = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "\n",
    "#       Each time store portion of precicted data in train predicted values\n",
    "\n",
    "        for i in range(len(X_valid_f.index)):\n",
    "            yfull_train[X_valid_f.index[i]] = yhat[i]\n",
    "            \n",
    "        print(\"Validating...\")\n",
    "        check = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "        score = roc_auc_score(y_valid.tolist(), check)\n",
    "        print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "        print(\"Predict test set...\")\n",
    "        test_prediction1 = gbm.predict(xgb.DMatrix(X_test_f), ntree_limit=gbm.best_iteration+1)\n",
    "        print('test prediction', test_prediction1.shape, 'yfull_test', yfull_test.shape)\n",
    "        \n",
    "#        for item in replace_test_csp.tolist():\n",
    "#            np.insert(test_prediction1, item, 0)\n",
    "        print('test_prediction1 shape',test_prediction1.shape)\n",
    "        yfull_test['kfold_' + str(num_fold)] = test_prediction1\n",
    "        \n",
    "              \n",
    "\n",
    "    print('iteration finished')\n",
    "    # Copy dict to list\n",
    "    train_res = []\n",
    "    \n",
    "#    print('train.index',train.index, train.index[0], 'train shape', train.shape)\n",
    "    print('yfull_train', len(yfull_train), list(yfull_train.keys())[0], list(yfull_train.keys())\n",
    "         ,type(list(yfull_train.keys())[0]))\n",
    "    \n",
    "#    print('train_indexes',train.index.values.tolist())\n",
    "    \n",
    "    iterator_train=train.index.values.tolist()\n",
    "    for i in iterator_train:\n",
    "        if i in yfull_train:\n",
    "            train_res.append(yfull_train[i])\n",
    "            \n",
    "        else:\n",
    "            print('this index is missing! ', i)\n",
    "            miss_index=i\n",
    "            \n",
    "            print('miss_index', miss_index)\n",
    "            row_miss=train.loc[[i]]\n",
    "#            print('row_miss', row_miss)\n",
    "        \n",
    "            missing_id=row_miss.index.values[0]\n",
    "\n",
    "            \n",
    "            print('missing_id', missing_id)\n",
    "#            print('missing_result', missing_result)\n",
    "            train=train[(train.index != missing_id)]\n",
    "            \n",
    "            print('train shape', train.shape)\n",
    "            print('train_res shape', len(train_res))\n",
    "            \n",
    "#    print('test indexes', test.index.values.tolist())\n",
    "    \n",
    "    \n",
    "#    print('test.index',test.index, test.index[0], 'test shape', test.shape)\n",
    "#    print('X_test_f index', X_test_f.index.values, 'X test shape', X_test_f.shape)\n",
    "    \n",
    "    iterator_test=test.index.values.tolist()\n",
    "    \n",
    "    for j in iterator_test :\n",
    "    \n",
    "        \n",
    "        if j in X_test_f.index.values.tolist():\n",
    "            pass\n",
    "        else:\n",
    "            missing_test_index=j\n",
    "            \n",
    "            row_test_miss=test.loc[[j]]\n",
    "            \n",
    "            missing_test_id=row_miss['Id'].values[0]\n",
    "            \n",
    "            \n",
    "            test=test[test['Id'] != missing_test_id]\n",
    "            \n",
    "            print('test shape', test.shape)\n",
    "            \n",
    "\n",
    "     \n",
    "    \n",
    "    print('train shape', train.shape)\n",
    "\n",
    "    score = roc_auc_score(train[target], np.array(train_res))\n",
    "    print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "    # Find mean for KFolds on test\n",
    "    merge = []\n",
    "    for i in range(1, nfolds+1):\n",
    "        merge.append('kfold_' + str(i))\n",
    "    yfull_test['mean'] = yfull_test[merge].mean(axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "\n",
    "\n",
    "    #Pred_per_patient currently in development. Is deactivated and should not be used.\n",
    "    if pred_per_patient:\n",
    "    \n",
    "        total_results=yfull_test['mean'].values\n",
    "        hist, bins = np.histogram(total_results, bins=50)\n",
    "        width = 0.7 * (bins[1] - bins[0])\n",
    "        center = (bins[:-1] + bins[1:]) / 2\n",
    "        plt.bar(center, hist, align='center', width=width)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "        total_results_train=np.array(train_res)\n",
    "        hist, bins = np.histogram(total_results_train, bins=50)\n",
    "        width = 0.7 * (bins[1] - bins[0])\n",
    "        center = (bins[:-1] + bins[1:]) / 2\n",
    "        plt.bar(center, hist, align='center', width=width)\n",
    "        plt.show()\n",
    "        \n",
    "    #Saving module and xgboost parameters to JSON file\n",
    "    \n",
    "    xgboost_params=params\n",
    "    \n",
    "    parameters=[]\n",
    "    \n",
    "    parameters.append(xgboost_params)\n",
    "    parameters.append(function_params)\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    parameter_file_name=str('parameter-file-'+'mode-'+str(mode)+'-'+now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    \n",
    "    json.dump(parameters, open(parameter_file_name+\".txt\",'w'), indent=4)\n",
    "#    read_params = json.load(open(parameter_file_name+\".txt\"), object_pairs_hook=OrderedDict)\n",
    "    \n",
    "#    print (read_params)\n",
    "    \n",
    "#    print('yfull_test shape', yfull_test.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    return yfull_test['mean'].values, score, yfull_train, train_res, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Training and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Grid Search\n",
    "\n",
    "### This is not currently being used. Will probably be deleted in future versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_param_search(nfolds, train, test, features, target, random_state=2016,  PCAkey=False, SEQoriginal=False):\n",
    "\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "\n",
    "    \n",
    "    yfull_train = dict()\n",
    "    yfull_test = copy.deepcopy(test[['Id']].astype(object))\n",
    "    print('train sequences',train['sequence_id'])\n",
    "    \n",
    "    \n",
    "    unique_sequences = np.array(train['sequence_id'].unique())\n",
    "#    print('unique sequences', unique_sequences, len(unique_sequences))\n",
    "    \n",
    "    groups1=np.fix(unique_sequences/1000)\n",
    "    \n",
    "    groups2=groups1.astype(int)\n",
    "#    print('groups', groups2)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    test1=gkf.split(unique_sequences, groups=groups2)\n",
    "    test2=gkf.split(unique_sequences, groups=groups2)\n",
    "    \n",
    "    \n",
    "    if SEQoriginal:\n",
    "        sequences_full=np.mod(train['sequence_id'].values,1000)\n",
    "        unique_sequences2=np.mod(unique_sequences,1000)\n",
    "        unique_sequences_fold=pd.Series(sequences_full, index=train['sequence_id'].index)\n",
    "#        print('unique_sequences_fold', unique_sequences_fold)\n",
    "    \n",
    "        unique_sequences = np.unique(unique_sequences2)\n",
    "\n",
    "    else:\n",
    "        unique_sequences_fold=pd.Series(train['sequence_id'], index=train['sequence_id'].index)\n",
    "\n",
    "    \n",
    "    kf = KFold(len(unique_sequences), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    num_fold1=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#   param_test1 = {'max_depth': [1,3,5]}\n",
    "#   param_test1 = {'max_depth': [1,3,5,7,9], 'min_child_weight':[1,3,5,7]}\n",
    "#   param_test1 = {'gamma':[i/10.0 for i in range(0,7)]}\n",
    "#   param_test1 = { 'subsample':[i/10.0 for i in range(6,10)],'colsample_bytree':[i/10.0 for i in range(6,10)]}  \n",
    "#   param_test1 = {'max_depth': [1,3], 'min_child_weight':[6,7,8,9,10]}\n",
    "#    param_test1 = {'scale_pos_weight':[1,2,3,4,5], 'max_delta_step':[0,1,2,3,4,5]}\n",
    "\n",
    "    for train_seq_index1, test_seq_index1 in kf:\n",
    "        num_fold1 += 1\n",
    "        print('this is creation of Kfold iterator')\n",
    "        print('Start fold {} from {}'.format(num_fold1, nfolds))\n",
    "    \n",
    "        train_seq1 = unique_sequences[train_seq_index1]\n",
    "        valid_seq1 = unique_sequences[test_seq_index1]\n",
    "        \n",
    "        print(train_seq1)\n",
    "        print(valid_seq1)\n",
    "\n",
    "        train_index = train[unique_sequences_fold.isin(train_seq1)].index.values\n",
    "        test_index = train[unique_sequences_fold.isin(valid_seq1)].index.values\n",
    "\n",
    "        print(train_index, type(train_index))\n",
    "        print(test_index, type(test_index))\n",
    "       \n",
    "        train_index_group.append(train_index)\n",
    "        test_index_group.append(test_index)\n",
    "        \n",
    "    \n",
    "    print('train index group',train_index_group)\n",
    "\n",
    "    custom_cv = [(train_index_group[i], test_index_group[i]) for i in range(0,3) ]\n",
    "    \n",
    "#    custom_cv=GroupShuffleSplit(n_splits=nfolds, test_size=0.5, random_state=0)\n",
    "\n",
    "#    custom_cv = list(zip(train_index_group, test_index_group))\n",
    "\n",
    "    print('custom cv', custom_cv)\n",
    "                   \n",
    "\n",
    "#    Scaling and PCA\n",
    "\n",
    "    if PCAkey:\n",
    "\n",
    "        scaler1 = MinMaxScaler() \n",
    "    \n",
    "        train_features=train[features]\n",
    "        train_target=train[target]\n",
    "            \n",
    "        train_scaled=pd.DataFrame(scaler1.fit_transform(train_features), columns=train_features.columns, index=train_features.index)\n",
    "\n",
    "\n",
    "        pcatest=KernelPCA(kernel='poly')\n",
    "        train_features_f=pd.DataFrame(pcatest.fit_transform(train_scaled), index=train_scaled.index)\n",
    "\n",
    "        dmfeatures=train_features_f\n",
    "        dmtarget=train_target        \n",
    "\n",
    "    else:\n",
    "    \n",
    "        dmfeatures=train[features]\n",
    "        dmtarget=train[target]\n",
    "    \n",
    "#   GridSearch\n",
    "    \n",
    "    classifier1=XGBClassifier( learning_rate =0.2, n_estimators=1000, max_depth=1,\n",
    "        min_child_weight=3, gamma=0, subsample=0.6, colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = classifier1, param_grid = param_test1, scoring='roc_auc',n_jobs=-1,iid=False, cv=custom_cv)\n",
    "  \n",
    "    \n",
    "#    gsearch1.fit(dmfeatures,dmtarget,groups=train['sequence_id'])\n",
    "    gsearch1.fit(dmfeatures,dmtarget)\n",
    "\n",
    "    print('best parameters, scores')    \n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "#    classifier2 = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Grid Search\n",
    "\n",
    "### Used by Module for Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def param_search_embedded(n_folds,features, target, kf, unique_sequences, unique_sequences_fold, train, \n",
    "                          PCAkey=False):\n",
    "\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "    num_fold1=0\n",
    "    \n",
    "    print(train)\n",
    "    print(train.shape)\n",
    "    print(train.columns.values)\n",
    "    \n",
    "\n",
    "    param_test1={}\n",
    "#    param_test1['learning_rate']= [i/10.0 for i in range(1,5)]\n",
    "    param_test1['max_depth']= [1,2,3,4,5,6,7]\n",
    "    param_test1['min_child_weight']=[1,2,3]\n",
    "#   param_test1['gamma']=[i/10.0 for i in range(0,7)]\n",
    "#    param_test1['subsample']=[i/10.0 for i in range(6,10)]\n",
    "#    param_test1['colsample_bytree']=[i/10.0 for i in range(6,10)]\n",
    "#    param_test1['scale_pos_weight']=[1,2,3,4,5]\n",
    "#    param_test1['max_delta_step']=[0,1,2,3,4,5]\n",
    "\n",
    "    for train_seq_index1, test_seq_index1 in kf:\n",
    "        num_fold1 += 1\n",
    "        print('this is creation of Kfold iterator')\n",
    "        print('Start fold {} from {}'.format(num_fold1, n_folds))\n",
    "    \n",
    "        train_seq1 = unique_sequences[train_seq_index1]\n",
    "        valid_seq1 = unique_sequences[test_seq_index1]\n",
    "        \n",
    "#        print(train_seq1)\n",
    "#        print(valid_seq1)\n",
    "\n",
    "        train_index = train[unique_sequences_fold.isin(train_seq1)].index.values\n",
    "        test_index = train[unique_sequences_fold.isin(valid_seq1)].index.values\n",
    "\n",
    "#        print(train_index, type(train_index))\n",
    "#        print(test_index, type(test_index))\n",
    "       \n",
    "        train_index_group.append(train_index)\n",
    "        test_index_group.append(test_index)\n",
    "        \n",
    "    \n",
    "#    print('train index group',train_index_group)\n",
    "\n",
    "    custom_cv = [(train_index_group[i], test_index_group[i]) for i in range(0,n_folds) ]\n",
    "    \n",
    "#    custom_cv=GroupShuffleSplit(n_splits=nfolds, test_size=0.5, random_state=0)\n",
    "\n",
    "#    custom_cv = list(zip(train_index_group, test_index_group))\n",
    "\n",
    "    print('custom cv', np.array(custom_cv).shape)\n",
    "                   \n",
    "\n",
    "#    Scaling and PCA\n",
    "\n",
    "    if PCAkey:\n",
    "\n",
    "        scaler1 = MinMaxScaler() \n",
    "    \n",
    "        train_features=train[features]\n",
    "        train_target=train[target]\n",
    "            \n",
    "        train_scaled=pd.DataFrame(scaler1.fit_transform(train_features), columns=train_features.columns, index=train_features.index)\n",
    "\n",
    "\n",
    "        pcatest=PCA(20)\n",
    "        train_features_f=pd.DataFrame(pcatest.fit_transform(train_scaled), index=train_scaled.index)\n",
    "\n",
    "        dmfeatures=train_features_f\n",
    "        dmtarget=train_target        \n",
    "\n",
    "    else:\n",
    "    \n",
    "        dmfeatures=train[features]\n",
    "        dmtarget=train[target]\n",
    "    \n",
    "#   GridSearch\n",
    "\n",
    "    print('start grid search')\n",
    "    \n",
    "    classifier1=XGBClassifier( learning_rate =0.2, n_estimators=1000, max_depth=4,\n",
    "        min_child_weight=2, gamma=0.1, subsample=0.9, colsample_bytree=0.9,\n",
    "        objective= 'binary:logistic', nthread=4, scale_pos_weight=2, seed=27, max_delta_step=0)\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = classifier1, param_grid = param_test1, scoring='roc_auc',n_jobs=-1,iid=False, cv=custom_cv)\n",
    "  \n",
    "    \n",
    "#    gsearch1.fit(dmfeatures,dmtarget,groups=train['sequence_id'])\n",
    "    gsearch1.fit(dmfeatures,dmtarget)\n",
    "\n",
    "    print('best parameters, scores')    \n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "#    classifier2 = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n",
    "    return gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_param_search(3, train, test, features, 'result', SEQoriginal=True, PCAkey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Training, Prediction and Creating Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode=3\n",
    "feature_model=2\n",
    "short_size=False\n",
    "num_features=6\n",
    "new_test=True\n",
    "fini=\n",
    "fend=\n",
    "fovr=\n",
    "\n",
    "params_training='parameter-file-mode-3-2016-11-22-14-16 (copy).txt'\n",
    "\n",
    "read_params = json.load(open(params_training), object_pairs_hook=OrderedDict)\n",
    "\n",
    "\n",
    "#Here checks if file with filter parameters exist.\n",
    "feature_file='_'+str(feature_model)+'_short_'+str(short_dataset)+'_new_test_'+ str(new_test)\\\n",
    "            +'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)\n",
    "\n",
    "file_name_train=\"simple_train_\" + str(patient_id) + feature_file + \".csv\"\n",
    "file_name_test=\"simple_train_\" + str(patient_id) + feature_file + \".csv\"\n",
    "\n",
    "import os  \n",
    "\n",
    "if os.path.isfile('./'+file_name_train):\n",
    "    \n",
    "    pass\n",
    "\n",
    "#Here creates files with filters parameters\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    generating_files(feature_model, short_size, num_features, new_test,fini, fend, fovr):\n",
    "\n",
    "\n",
    "time.sleep(10)\n",
    "#Here read data from generated csv files\n",
    "all_data=read_test_train_per_patient(feature_model, short_size=False, new_test=True)\n",
    "\n",
    "if mode != 0:\n",
    "    \n",
    "    train, test, features=all_data[mode-1][0],all_data[mode-1][1],all_data[mode-1][2]\n",
    "\n",
    "\n",
    "params_model=read_params[0]\n",
    "prm=read_params[1]\n",
    "\n",
    "#print(type(params_model) is OrderedDict)\n",
    "    \n",
    "score_list=[]\n",
    "    \n",
    "for item in range(1):\n",
    "    \n",
    "    folds_iter=item+3\n",
    "    \n",
    "    prediction, score, train_result1, train_result2 = run_train_predict(train, test, features, 'result', params_model,\n",
    "                                                  nfolds=prm['nfolds'], random_state=prm['random_state'],\n",
    "                                                  mode=mode, SEQoriginal=prm['SEQoriginal'], PCAkey=prm['PCAkey'], \n",
    "                                                  PCAgraph=prm['PCAgraph'], PCAkeyGS=prm['PCAkeyGS'],\n",
    "                                                  Oversampling=prm['Oversampling'], GridSearch=True,\n",
    "                                                  pred_per_patient=False)\n",
    "\n",
    "    score_list.append(score)\n",
    "#prediction, score = run_train_predict(train, test, features, 'result', read_params[0],**prm,\n",
    "#                                      pred_per_patient=False)\n",
    "\n",
    "print(score_list)\n",
    "\n",
    "#create_submission(score, test, prediction, feature_model, short_size, new_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Analysis per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode=3\n",
    "feature_model=5\n",
    "short_size=False\n",
    "num_features=9\n",
    "new_test=True\n",
    "channels=16\n",
    "#fini=\n",
    "#fend=\n",
    "#fovr=\n",
    "\n",
    "#params_training='parameter-file-mode-3-2016-11-22-14-16 (copy).txt'\n",
    "params_training='parameter-file-mode-3-2016-11-27-16-29.txt'\n",
    "\n",
    "\n",
    "read_params = json.load(open(params_training), object_pairs_hook=OrderedDict)\n",
    "\n",
    "all_data=read_test_train_per_patient(feature_model, short_size=False, new_test=True, fini=4, fend=40, fovr=0)\n",
    "\n",
    "if mode != 0:\n",
    "    \n",
    "    train, test, features=all_data[mode-1][0],all_data[mode-1][1],all_data[mode-1][2]\n",
    "\n",
    "\n",
    "params_model=read_params[0]\n",
    "prm=read_params[1]\n",
    "\n",
    "#print(type(params_model) is OrderedDict)\n",
    "    \n",
    "score_list=[]\n",
    "test_seq=[]\n",
    "prediction_list=[]\n",
    "train_result2_list=[]\n",
    "\n",
    "#print('before all', train)\n",
    "\n",
    "\n",
    "for part_iter in range(2):\n",
    "    \n",
    "    for item in range(6):\n",
    "    \n",
    "        folds_iter=4\n",
    "    \n",
    "        random_iter=2016\n",
    "    \n",
    "        seq_iter=item+1\n",
    "        \n",
    "        if part_iter==0:\n",
    "            csp_init=0\n",
    "            csp_end=0.5\n",
    "        \n",
    "        else:\n",
    "            csp_init=0.5\n",
    "            csp_end=1\n",
    "    \n",
    "        prediction, score, train_result1, train_result2, test_out = run_train_predict(train, test, features, 'result', \n",
    "                                               params_model,num_features, channels, csp_n=16,seq_number=seq_iter,\n",
    "                                                csp_init=0,csp_end=0.5, \n",
    "                                                  nfolds=folds_iter, random_state=random_iter,\n",
    "                                                  mode=mode, SEQoriginal=prm['SEQoriginal'], PCAkey=prm['PCAkey'], \n",
    "                                                  PCAgraph=prm['PCAgraph'], PCAkeyGS=prm['PCAkeyGS'],\n",
    "                                                  Oversampling=prm['Oversampling'], GridSearch=False,\n",
    "                                                  pred_per_patient=False, CSPkey=True, CSPkey1=False)\n",
    "        prediction_list.append(prediction)\n",
    "        test_seq.append(test_out)\n",
    "        score_list.append(score)\n",
    "        train_result2_list.append(train_result2)\n",
    "#prediction, score = run_train_predict(train, test, features, 'result', read_params[0],**prm,\n",
    "#                                      pred_per_patient=False)\n",
    "\n",
    "print(score_list)\n",
    "print(prediction.shape)\n",
    "\n",
    "create_submission(score, test_out, prediction, feature_model, short_size, new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_train_pred=np.zeros(len(train_result2_list[0]))\n",
    "final_test_pred=np.zeros(len(prediction_list[0]))\n",
    "for i in prediction_list:\n",
    "    final_test_pred+=i\n",
    "final_test_pred=np.divide(final_test_pred,6)\n",
    "\n",
    "for i in train_result2_list:\n",
    "    final_train_pred+=i\n",
    "final_train_pred=np.divide(final_test_pred,6)\n",
    "#print(final_array)\n",
    "print(final_train_pred)\n",
    "print(final_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = np.random.rand(5,5)\n",
    "\n",
    "#print (x)\n",
    "\n",
    "#test11=x[:,x.shape[1]-1]\n",
    "\n",
    "#print(test11,type(test11))\n",
    "\n",
    "#test12=np.int64(test11)\n",
    "#print(test12)\n",
    "#z=x.shape[0]-1\n",
    "\n",
    "#y=np.delete(x,x.shape[1]-1, 1)\n",
    "\n",
    "#print(y)\n",
    "#x[0] = -np.inf\n",
    "#print(x)\n",
    "\n",
    "#xf=x[~np.isinf(x)]  # 1D array with NaNs removed\n",
    "#print(xf)\n",
    "#xf=x[~np.any(np.isinf(x), axis=1)]  # 2D array with rows with NaN removed\n",
    "\n",
    "#print (xf)\n",
    "\n",
    "#a = np.arange(6).reshape((3, 2), order='F')\n",
    "\n",
    "print(a)\n",
    "\n",
    "new_test = test[features].iloc[[0]].drop(['file_size','patient_id'],1).values\n",
    "\n",
    "print(new_test.astype(np.int64))\n",
    "\n",
    "out1=np.reshape(new_test,newshape=(9, 16), order='F')\n",
    "\n",
    "\n",
    "print(out1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC analysis for 1 patient at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_id=all_data[mode-1][0]['Id']\n",
    "\n",
    "train_true = all_data[mode-1][0][all_data[mode-1][0]['result'] == 1]['Id']\n",
    "\n",
    "print(train_true.shape)\n",
    "\n",
    "train_false = all_data[mode-1][0][all_data[mode-1][0]['result'] == 0]['Id']\n",
    "      \n",
    "print(train_false.shape)\n",
    "        \n",
    "train_result2_final=np.array(train_result2)\n",
    "#train_result2_1 = (train_result2_1 - np.median(train_result2_1)) * 0.5 + 0.5\n",
    "\n",
    "print(train_result2_final.shape)\n",
    "\n",
    "      \n",
    "#roc1=np.column_stack((train1_id,train_result2_1))\n",
    "roc=pd.DataFrame({'id':train_id,'prob':train_result2_final})\n",
    "\n",
    "#print(type(train1_true),type(roc1))\n",
    "\n",
    "#histograms\n",
    "\n",
    "hist_true=roc[roc.index.isin(train_true.index)]['prob'].values\n",
    "hist_false=roc[roc.index.isin(train_false.index)]['prob'].values\n",
    "\n",
    "#scaling\n",
    "\n",
    "#hist_true_1 = (hist_true_1 - np.median(hist_true_1)) * 0.5 + 0.5\n",
    "#hist_false_1 = (hist_false_1 - np.median(hist_false_1)) * 0.5 + 0.5\n",
    "#print(hist_true_1.shape)\n",
    "\n",
    "histogram=plt.figure()\n",
    "\n",
    "bins = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.hist(hist_true, bins,color='b', log=True, alpha=0.5)\n",
    "plt.hist(hist_false, bins, log=True,color='g', alpha=0.5)\n",
    "plt.hist(prediction, bins, log=True,color='r', alpha=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Grid Search, Training and Prediction per patient - All patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_model=4\n",
    "short_size=False\n",
    "num_features=9\n",
    "new_test=True\n",
    "all_data=read_test_train_per_patient(feature_model, short_size=False, new_test=True)\n",
    "rescale=False\n",
    "\n",
    "\n",
    "params_training1='parameter-file-mode-1-2016-11-21-19-41.txt'\n",
    "params_training2='parameter-file-mode-2-2016-11-21-22-47.txt'\n",
    "params_training3='parameter-file-mode-3-2016-11-22-01-59.txt'\n",
    "\n",
    "read_params1 = json.load(open(params_training1), object_pairs_hook=OrderedDict)\n",
    "\n",
    "read_params2 = json.load(open(params_training2), object_pairs_hook=OrderedDict)\n",
    "\n",
    "read_params3 = json.load(open(params_training3), object_pairs_hook=OrderedDict)\n",
    "\n",
    "all_params=[]\n",
    "all_params.append(read_params1)\n",
    "all_params.append(read_params2)\n",
    "all_params.append(read_params3)\n",
    "\n",
    "print (all_params)\n",
    "\n",
    "\n",
    "#print(len(all_data), len(all_data[0]), all_data[0][1].shape)\n",
    "\n",
    "train_result1=[]\n",
    "train_result2=[]\n",
    "prediction=[]\n",
    "score=[]\n",
    "for i, item in enumerate(all_data):\n",
    "    \n",
    "    read_params_patient=all_params[i]\n",
    "    params_model=read_params_patient[0]\n",
    "    prm=read_params_patient[1]\n",
    "    \n",
    "#    print(item[1])\n",
    "    # mode indicates '0':Global training (all patients); '1' training and testing on patient one;\n",
    "    #'2' training and testing on patient two...\n",
    "    mode=i+1\n",
    "    \n",
    "    temp1, temp2, temp3, temp4 = run_train_predict(item[0], item[1], item[2], 'result',\n",
    "                                            params_model,num_features, channels, csp_n=16,seq_number=1,csp_init=0,csp_end=1, \n",
    "                                            nfolds=prm['nfolds'], random_state=prm['random_state'],\n",
    "                                            mode=mode, SEQoriginal=prm['SEQoriginal'], PCAkey=prm['PCAkey'], \n",
    "                                            PCAgraph=prm['PCAgraph'], PCAkeyGS=prm['PCAkeyGS'],\n",
    "                                            Oversampling=prm['Oversampling'], GridSearch=False,\n",
    "                                            pred_per_patient=False, CSPkey=False, CSPkey1=True)\n",
    "    \n",
    "   \n",
    "    \n",
    "    train_result1.append(temp3)\n",
    "    train_result2.append(temp4)\n",
    "    prediction.append(temp1)\n",
    "    score.append(temp2)\n",
    "\n",
    "#median rescale\n",
    "\n",
    "if rescale:\n",
    "#    for i in range(len(all_data)):\n",
    "#        prediction[i] = (prediction[i] - np.median(prediction[i])) * 0.5 + 0.5\n",
    "    prediction[i]=np.divide(prediction[i],np.amax(prediction[i]))\n",
    "    \n",
    "# Concatenation of AUC scores per patient\n",
    "\n",
    "predict_total=np.concatenate((prediction[0],prediction[1],prediction[2]))\n",
    "score_total=sum(score)/len(score)\n",
    "test=pd.concat([all_data[0][1],all_data[1][1],all_data[2][1]])\n",
    "    \n",
    "    \n",
    "create_submission(score_total, test, predict_total, feature_model, short_size, new_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train1_id,train2_id,train3_id=all_data[0][0]['Id'],all_data[1][0]['Id'],all_data[2][0]['Id']\n",
    "\n",
    "train1_true, train2_true, train3_true = all_data[0][0][all_data[0][0]['result'] == 1]['Id'],\\\n",
    "                                        all_data[1][0][all_data[1][0]['result'] == 1]['Id'],\\\n",
    "                                        all_data[2][0][all_data[2][0]['result'] == 1]['Id']\n",
    "        \n",
    "train1_false, train2_false, train3_false = all_data[0][0][all_data[0][0]['result'] == 0]['Id'],\\\n",
    "                                        all_data[1][0][all_data[1][0]['result'] == 0]['Id'],\\\n",
    "                                        all_data[2][0][all_data[2][0]['result'] == 0]['Id']        \n",
    "                \n",
    "        \n",
    "train_result2_1=np.array(train_result2[0])\n",
    "#train_result2_1 = (train_result2_1 - np.median(train_result2_1)) * 0.5 + 0.5\n",
    "\n",
    "\n",
    "train_result2_2=np.array(train_result2[1])\n",
    "#train_result2_2 = (train_result2_2 - np.median(train_result2_2)) * 0.5 + 0.5\n",
    "\n",
    "\n",
    "train_result2_3=np.array(train_result2[2])\n",
    "#train_result2_3 = (train_result2_3 - np.median(train_result2_3)) * 0.5 + 0.5\n",
    "\n",
    "\n",
    "        \n",
    "#roc1=np.column_stack((train1_id,train_result2_1))\n",
    "roc1=pd.DataFrame({'id':train1_id,'prob':train_result2_1})\n",
    "roc2=pd.DataFrame({'id':train2_id,'prob':train_result2_2})\n",
    "roc3=pd.DataFrame({'id':train3_id,'prob':train_result2_3})\n",
    "#print(type(train1_true),type(roc1))\n",
    "\n",
    "#histograms\n",
    "\n",
    "hist_true_1=roc1[roc1.index.isin(train1_true.index)]['prob'].values\n",
    "hist_false_1=roc1[roc1.index.isin(train1_false.index)]['prob'].values\n",
    "\n",
    "#scaling\n",
    "\n",
    "#hist_true_1 = (hist_true_1 - np.median(hist_true_1)) * 0.5 + 0.5\n",
    "#hist_false_1 = (hist_false_1 - np.median(hist_false_1)) * 0.5 + 0.5\n",
    "#print(hist_true_1.shape)\n",
    "\n",
    "\n",
    "#histograms\n",
    "\n",
    "hist_true_2=roc2[roc2.index.isin(train2_true.index)]['prob'].values\n",
    "hist_false_2=roc2[roc2.index.isin(train2_false.index)]['prob'].values\n",
    "\n",
    "#scaling\n",
    "\n",
    "#hist_true_2 = (hist_true_2 - np.median(hist_true_2)) * 0.5 + 0.5\n",
    "#hist_false_2 = (hist_false_2 - np.median(hist_false_2)) * 0.5 + 0.5\n",
    "\n",
    "\n",
    "#histograms\n",
    "\n",
    "hist_true_3=roc3[roc3.index.isin(train3_true.index)]['prob'].values\n",
    "hist_false_3=roc3[roc3.index.isin(train3_false.index)]['prob'].values\n",
    "\n",
    "\n",
    "#scaling\n",
    "\n",
    "#hist_true_3 = (hist_true_3 - np.median(hist_true_3)) * 0.5 + 0.5\n",
    "#hist_false_3 = (hist_false_3 - np.median(hist_false_3)) * 0.5 + 0.5\n",
    "\n",
    "#print(roc1.isin(np.array(train1_true))['id'].shape)\n",
    "#print(prediction[0], prediction[0].shape)\n",
    "#print(train1_id.shape)\n",
    "#print(train_result2_1.shape, train1_id.shape)\n",
    "#print(roc1)\n",
    "#print(len(all_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histogram=plt.figure()\n",
    "\n",
    "bins = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.hist(hist_true_1, bins,color='b', log=True, alpha=0.5)\n",
    "plt.hist(hist_false_1, bins, log=True,color='g', alpha=0.5)\n",
    "plt.hist(prediction[0], bins, log=True,color='r', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "histogram1=plt.figure()\n",
    "\n",
    "bins = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.hist(hist_true_2, bins, color='b', log=True, alpha=0.5)\n",
    "plt.hist(hist_false_2, bins,log=True,color='g', alpha=0.5)\n",
    "plt.hist(prediction[1], bins, log=True,color='r', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "histogra3=plt.figure()\n",
    "\n",
    "bins = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.hist(hist_true_3, bins,log=True,color='b', alpha=0.5)\n",
    "plt.hist(hist_false_3, bins,log=True,color='g', alpha=0.5)\n",
    "plt.hist(prediction[2], bins, log=True,color='r', alpha=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification per patient with filter customization (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_model=3\n",
    "short_size=False\n",
    "num_features=6\n",
    "new_test=True\n",
    "all_data=read_test_train_per_patient(feature_model=3, short_size=False, new_test=True)\n",
    "rescale=False\n",
    "\n",
    "\n",
    "params_training1='parameter-file-mode-1-2016-11-22-14-16 (copy).txt'\n",
    "params_training2='parameter-file-mode-2-2016-11-22-14-16 (copy).txt'\n",
    "params_training3='parameter-file-mode-3-2016-11-22-14-16 (copy).txt'\n",
    "\n",
    "read_params1 = json.load(open(params_training1), object_pairs_hook=OrderedDict)\n",
    "\n",
    "read_params2 = json.load(open(params_training2), object_pairs_hook=OrderedDict)\n",
    "\n",
    "read_params3 = json.load(open(params_training3), object_pairs_hook=OrderedDict)\n",
    "\n",
    "all_params=[]\n",
    "all_params.append(read_params1)\n",
    "all_params.append(read_params2)\n",
    "all_params.append(read_params3)\n",
    "\n",
    "print (all_params)\n",
    "\n",
    "\n",
    "#print(len(all_data), len(all_data[0]), all_data[0][1].shape)\n",
    "\n",
    "if param_filter not in folder_results:\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        print('XGBoost: {}'.format(xgb.__version__))\n",
    "        if 1:\n",
    "            # Do reading and processing of MAT files in parallel\n",
    "            p = dict()\n",
    "    #        p[1] = Process(target=create_simple_csv_train, args=(1,feature_model,num_features,short_size,new_test))\n",
    "    #        p[1].start()\n",
    "    #        p[2] = Process(target=create_simple_csv_train, args=(2,feature_model,num_features,short_size,new_test))\n",
    "    #        p[2].start()\n",
    "    #        p[3] = Process(target=create_simple_csv_train, args=(3,feature_model,num_features,short_size,new_test))\n",
    "    #        p[3].start()\n",
    "    #        p[4] = Process(target=create_simple_csv_test, args=(1,feature_model,num_features,short_size,new_test))\n",
    "    #        p[4].start()\n",
    "    #        p[5] = Process(target=create_simple_csv_test, args=(2,feature_model,num_features,short_size,new_test))\n",
    "    #        p[5].start()\n",
    "            p[6] = Process(target=create_simple_csv_test, args=(3,feature_model,num_features,short_size,new_test))\n",
    "            p[6].start()\n",
    "    #        p[1].join()\n",
    "    #        p[2].join()\n",
    "    #        p[3].join()\n",
    "    #        p[4].join()\n",
    "    #        p[5].join()\n",
    "            p[6].join()\n",
    "\n",
    "        \n",
    "time.sleep(5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    \n",
    "    train, test, features = read_test_train(feature_model, short_size, new_test)\n",
    "    print('Length of train: ', len(train))\n",
    "    print('Length of test: ', len(test))\n",
    "    print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_result1=[]\n",
    "train_result2=[]\n",
    "prediction=[]\n",
    "score=[]\n",
    "for i, item in enumerate(all_data):\n",
    "    \n",
    "    read_params_patient=all_params[i]\n",
    "    params_model=read_params_patient[0]\n",
    "    prm=read_params_patient[1]\n",
    "    \n",
    "    \n",
    "    # mode indicates '0':Global training (all patients); '1' training and testing on patient one;\n",
    "    #'2' training and testing on patient two...\n",
    "    mode=i+1\n",
    "    \n",
    "    temp1, temp2, temp3, temp4 = run_train_predict(item[0], item[1], item[2], 'result', params_model, nfolds=prm['nfolds'],\n",
    "                                      random_state=prm['random_state'],\n",
    "                                      mode=mode, SEQoriginal=prm['SEQoriginal'], PCAkey=prm['PCAkey'], \n",
    "                                      PCAgraph=prm['PCAgraph'], PCAkeyGS=prm['PCAkeyGS'],\n",
    "                                      Oversampling=prm['Oversampling'], GridSearch=False,\n",
    "                                      pred_per_patient=False)\n",
    "    \n",
    "    \n",
    "    train_result1.append(temp3)\n",
    "    train_result2.append(temp4)\n",
    "    prediction.append(temp1)\n",
    "    score.append(temp2)\n",
    "\n",
    "#median rescale\n",
    "\n",
    "if rescale:\n",
    "    for i in range(len(all_data)):\n",
    "        prediction[i] = (prediction[i] - np.median(prediction[i])) * 0.5 + 0.5\n",
    "    \n",
    "    \n",
    "# Concatenation of AUC scores per patient\n",
    "\n",
    "predict_total=np.concatenate((prediction[0],prediction[1],prediction[2]))\n",
    "score_total=sum(score)/len(score)\n",
    "test=pd.concat([all_data[0][1],all_data[1][1],all_data[2][1]])\n",
    "    \n",
    "    \n",
    "create_submission(score_total, test, predict_total, feature_model, short_size, new_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules for PCA Graphic Analysis (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "almost_black = '#262626'\n",
    "palette = sns.color_palette()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.scatter(X_vis[y == 0, 0], X_vis[y == 0, 1], label=\"Class #0\", alpha=0.5,\n",
    "            edgecolor=almost_black, facecolor=palette[0], linewidth=0.15)\n",
    "ax1.scatter(X_vis[y == 1, 0], X_vis[y == 1, 1], label=\"Class #1\", alpha=0.5,\n",
    "            edgecolor=almost_black, facecolor=palette[2], linewidth=0.15)\n",
    "ax1.set_title('Original set')\n",
    "\n",
    "ax2.scatter(X_res_vis[y_resampled == 0, 0], X_res_vis[y_resampled == 0, 1],\n",
    "            label=\"Class #0\", alpha=.5, edgecolor=almost_black,\n",
    "            facecolor=palette[0], linewidth=0.15)\n",
    "ax2.scatter(X_res_vis[y_resampled == 1, 0], X_res_vis[y_resampled == 1, 1],\n",
    "            label=\"Class #1\", alpha=.5, edgecolor=almost_black,\n",
    "            facecolor=palette[2], linewidth=0.15)\n",
    "ax2.set_title('SMOTE svm')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mne.decoding import CSP\n",
    "\n",
    "pcatest=CSP(n_components=4)\n",
    "X_test=pcatest.fit(csp_data,y)\n",
    "\n",
    "ydata=np.array(train['result'].values)\n",
    "#y_resampled=y\n",
    "\n",
    "Xdata=np.array(train[features])\n",
    "\n",
    "print(X.ndim)\n",
    "        \n",
    "#       PCA transformation \n",
    "print(X_test)\n",
    "\n",
    "newX=X_test.fit_transform(Xdata,ydata)\n",
    "#X_vis=X_test[:,0:2]\n",
    "#X_res_vis=X_test[:,2:4]\n",
    "\n",
    "#print(X_vis.shape,X_res_vis.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newX=X_test.fit_transform(csp_data,y)\n",
    "\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.decoding import CSP\n",
    "\n",
    "source_dir=\"./train_\"\n",
    "patient_id=1\n",
    "files = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "print ('train files'+ str(patient_id), len(files))  \n",
    "\n",
    "fl=files[0]\n",
    "\n",
    "\n",
    "tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "\n",
    "data_csp=np.transpose(tables.values)\n",
    "\n",
    "files1=files[0:100]\n",
    "\n",
    "print(len(files1))\n",
    "\n",
    "\n",
    "new_train = pd.read_csv('train_and_test_data_labels_safe'+'.csv')\n",
    "new_data = new_train['image']\n",
    "    \n",
    "selection = new_train[new_train['safe'] == 1].drop('safe', axis=1)\n",
    "    \n",
    "result_list=[]\n",
    "count=0\n",
    "track1=0\n",
    "        \n",
    "for i, fl1 in enumerate(files1):\n",
    "#    print(i)\n",
    "    if i==track1:\n",
    "        \n",
    "        print('checking')\n",
    "        if os.path.basename(fl1) not in selection['image'].values:\n",
    "            track1+=1\n",
    "            print('still')\n",
    "            continue\n",
    "        \n",
    "        id_str = os.path.basename(fl1)[:-4]\n",
    "        arr = id_str.split(\"_\")  \n",
    "        result = int(arr[2])\n",
    "        \n",
    "        fl=files[0]\n",
    "\n",
    "        tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "\n",
    "        data_csp=np.transpose(tables.values)\n",
    "        \n",
    "        result_list.append(result)\n",
    "        \n",
    "        print('done!')\n",
    "        \n",
    "        continue\n",
    "        \n",
    "        \n",
    "    if os.path.basename(fl1) not in selection['image'].values:\n",
    "        continue\n",
    "        \n",
    "    id_str = os.path.basename(fl1)[:-4]\n",
    "    arr = id_str.split(\"_\")  \n",
    "    result = int(arr[2])\n",
    "\n",
    "    \n",
    "    try:\n",
    "            tables1, sequence_from_mat1, samp_freq1 = mat_to_pandas(fl1)\n",
    "    except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "    \n",
    "    if sequence_from_mat1==1:\n",
    "        \n",
    "        temp_matrix=np.transpose(tables1.values)\n",
    "        data_csp=np.dstack((data_csp,temp_matrix))\n",
    "        result_list.append(result)\n",
    "\n",
    "y=np.array(result_list)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "#print(data_csp.shape)    \n",
    "\n",
    "csp_data=data_csp.transpose((2,0,1))\n",
    "\n",
    "#print(csp_data.shape)     \n",
    "    \n",
    "CSPtest=CSP(n_components=4)\n",
    "#X_test=CSPtest.fit(csp_data,y)\n",
    "newX=CSPtest.fit_transform(csp_data,y)\n",
    "\n",
    "print(newX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=np.array(result_list)\n",
    "print(y.shape)\n",
    "print(data_csp[0][0][3])\n",
    "\n",
    "print(data_csp.shape)    \n",
    "\n",
    "csp_data=data_csp.transpose((2,0,1))\n",
    "\n",
    "print(csp_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#csp_train = csp_data.copy().crop(tmin=1., tmax=600.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ch_names=[str(i) for i in tables.columns.values]\n",
    "eeg_matrix=np.transpose(tables.values)\n",
    "\n",
    "print(type(ch_names))\n",
    "\n",
    "print(type(eeg_matrix), eeg_matrix.shape)\n",
    "\n",
    "info_eeg=mne.create_info(ch_names, samp_freq, ch_types=None, montage=None)\n",
    "\n",
    "\n",
    "raw=mne.io.RawArray(eeg_matrix, info_eeg)\n",
    "\n",
    "#events=mne.find_events(raw, stim_channel='STI 014', output='onset', consecutive='increasing', min_duration=0,\n",
    " #               shortest_event=2, mask=None, uint_cast=False, mask_type=None, verbose=None)\n",
    "\n",
    "#tmin, tmax = -1., 4.\n",
    "#event_id = dict(hands=2, feet=3)\n",
    "#subject = 1\n",
    "#runs = [6, 10, 14]  # motor imagery: hands vs feet\n",
    "\n",
    "#raw_fnames = eegbci.load_data(subject, runs)\n",
    "#raw_files = [read_raw_edf(f, preload=True) for f in raw_fnames]\n",
    "#raw = concatenate_raws(raw_files)\n",
    "\n",
    "## strip channel names of \".\" characters\n",
    "#raw.rename_channels(lambda x: x.strip('.'))\n",
    "\n",
    "## Apply band-pass filter\n",
    "#raw.filter(7., 30., method='iir')\n",
    "\n",
    "#events = find_events(raw, shortest_event=0, stim_channel='STI 014')\n",
    "\n",
    "#picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "#                   exclude='bads')\n",
    "\n",
    "## Read epochs (train will be done only between 1 and 2s)\n",
    "## Testing will be done with a running classifier\n",
    "\n",
    "#events = np.array([\n",
    "#    [0, 1, 1],\n",
    "#    [1, 1, 2],\n",
    "#    [2, 1, 1],\n",
    "#    [3, 1, 2],\n",
    "#    [4, 1, 1],\n",
    "#    [5, 1, 2],\n",
    "#    [6, 1, 1],\n",
    "#    [7, 1, 2],\n",
    "#    [8, 1, 1],\n",
    "#    [9, 1, 2],\n",
    "#])\n",
    "\n",
    "#event_id = dict(smiling=1, frowning=2)\n",
    "\n",
    "#epochs = mne.Epochs(raw, events, event_id=None, tmin=-0.2, tmax=0.5, baseline=(None, 0),\n",
    "#                    picks=None, name='Unknown', preload=False, reject=None, flat=None, proj=True,\n",
    "#                    decim=1, reject_tmin=None, reject_tmax=None, detrend=None, add_eeg_ref=None,\n",
    "#                    on_missing='error', reject_by_annotation=True, verbose=None)\n",
    "#epochs_train = epochs.copy().crop(tmin=1., tmax=2.)\n",
    "#labels = epochs.events[:, -1] - 2\n",
    "\n",
    "\n",
    "\n",
    "#mne.decoding.CSP(n_components=4, reg=None, log=None, cov_est='concat', transform_into='average_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_test = dict(reduce_dim__n_components=[2, 5, 10],\n",
    "               clf__C=[0.1, 10, 100])\n",
    "\n",
    "print(params_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating arrays for CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modules to read train and test data.\n",
    "#Short_dataset can be False or TRue. It decides whether to use the lon or short sample size.\n",
    "\n",
    "def create_simple_csv_train(patient_id, feature_model, num_features, fini, fend, fovr,\n",
    "                            short_dataset=False, new_test=False):\n",
    "    \n",
    "    feature_file='_'+str(feature_model)+'_short_'+str(short_dataset)+'_new_test_'+ str(new_test)\\\n",
    "            +'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)\n",
    "    \n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/train_\"\n",
    "    else:\n",
    "        source_dir=\"./train_\"\n",
    "    \n",
    "    new_label=''\n",
    "    if new_test:\n",
    "        \n",
    "        new_label='_new'\n",
    "\n",
    "    out = open(\"simple_train_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,sequence_id,patient_id,\")\n",
    "  \n",
    "    columns=''\n",
    "    for i in range(16):\n",
    "        for j in range(num_features):\n",
    "            columns+= 'ch_'+str(i)+'_'+\"band_\"+str(j)+\",\"        \n",
    "\n",
    "    out.write(columns+\"file_size,result\\n\")\n",
    "\n",
    "    # TRAIN (0)\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "    print ('train files'+ str(patient_id), len(files))    \n",
    "    pos1=0\n",
    "    neg1=0\n",
    "    sequence_id_pre = int(patient_id)*1000\n",
    "    sequence_id_inter = int(patient_id)*1000\n",
    "    total_pre = 0\n",
    "    total_inter=0\n",
    "    seq1=0\n",
    "    \n",
    "    new_train = pd.read_csv('train_and_test_data_labels_safe'+'.csv')\n",
    "    new_data = new_train['image']\n",
    "    \n",
    "    selection = new_train[new_train['safe'] == 1].drop('safe', axis=1)\n",
    "    \n",
    "    for fl in files:\n",
    "        \n",
    "        # print('Go for ' + fl)\n",
    "               \n",
    "        if os.path.basename(fl) not in selection['image'].values:\n",
    "            continue\n",
    "        \n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        result = int(arr[2])\n",
    "        \n",
    "        if result == 1:\n",
    "            \n",
    "            total_pre += 1\n",
    "            sequence_id=int(patient_id)*1000+int((total_pre-1) // 6) + int((total_inter-1) // 6) + 1\n",
    "\n",
    "            \n",
    "        elif result == 0:\n",
    "            \n",
    "            total_inter += 1            \n",
    "            sequence_id=int(patient_id)*1000+int((total_pre) // 6) + int((total_inter-1) // 6)\n",
    "\n",
    "        \n",
    "        new_id = int(patient*100000 + id)\n",
    "        try:\n",
    "            tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        print(sequence_id)\n",
    "        out_str += str(new_id) + \",\" + str(sequence_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])       \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f], out_str,feature_model, sizesignal, samp_freq,  fini, fend, fovr,)\n",
    "            \n",
    "            \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \",\" + str(result) + \"\\n\"\n",
    "        #print(sequence_from_mat)\n",
    "        #print(type(sequence_from_mat))\n",
    "        seq1=int(sequence_from_mat[0][0][0][0])\n",
    "        print('total preictal: ', total_pre,' total interictal: ', total_inter,' sequence local: ', seq1)\n",
    "        if (total_pre % 6 == 0) and result == 1:\n",
    "                pos1 += 1\n",
    "                print('Positive ocurrence sequence finished', pos1)\n",
    "                if (seq1==6):\n",
    "                    sequence_id_pre += 1\n",
    "                    print ('sequence preictal next',sequence_id_pre)\n",
    "        \n",
    "        if (total_inter % 6 == 0) and result == 0:                \n",
    "                neg1 += 1\n",
    "                print('Negative ocurrence sequence finished', neg1)\n",
    "                if (seq1==6):\n",
    "                    sequence_id_inter += 1\n",
    "                    print ('sequence interictal next',sequence_id_inter)\n",
    "\n",
    "    out.write(out_str)\n",
    "    \n",
    "    out.close()\n",
    "    print('Train CSV for patient {} has been completed...'.format(patient_id))\n",
    "\n",
    "\n",
    "def create_simple_csv_test(patient_id, feature_model, num_features, fini, fend, fovr,\n",
    "                           short_dataset=False, new_test=False):\n",
    "    \n",
    "    feature_file='_'+str(feature_model)+'_short_'+str(short_dataset)+'_new_test_'+str(new_test)\\\n",
    "            +'_fini_'+str(fini)+'_fend_'+str(fend)+'_fovr_'+str(fovr)\n",
    "    \n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/test_\"\n",
    "    else:\n",
    "        source_dir=\"./test_\"\n",
    "    \n",
    "    new_label=''\n",
    "    \n",
    "    if new_test:\n",
    "        \n",
    "        new_label=\"_new\"\n",
    "\n",
    "    # TEST\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + new_label + \"/*.mat\"), key=natural_key)\n",
    "    print ('test files'+ str(patient_id), len(files))    \n",
    "    out = open(\"simple_test_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,patient_id,\")\n",
    "    \n",
    "    columns=''\n",
    "    for i in range(16):\n",
    "        for j in range(num_features):\n",
    "            columns+= 'ch_'+str(i)+'_'+\"band_\"+str(j)+\",\"        \n",
    "    \n",
    "    out.write(columns+\"file_size\\n\")\n",
    "    \n",
    "        \n",
    "    for fl in files:\n",
    "        # print('Go for ' + fl)\n",
    "        id_str = os.path.basename(fl)[4:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        new_id = int(patient*100000 + id)\n",
    "        try:\n",
    "            tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        out_str += str(new_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f], out_str,feature_model, sizesignal, samp_freq, fini, fend, fovr,)\n",
    "                        \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \"\\n\"\n",
    "        # break\n",
    "\n",
    "    out.write(out_str)\n",
    "    out.close()\n",
    "    print('Test CSV for patient {} has been completed...'.format(patient_id))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

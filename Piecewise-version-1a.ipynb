{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing all libraries needed.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.io import loadmat\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import Process\n",
    "import copy\n",
    "\n",
    "#Importing old and new Kfold\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold as NewKF\n",
    "\n",
    "#Importing GroupKfold, only available since version 0.18\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "#Importing function for scaling data before PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Importing PCA packages\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "#Importing FFT package\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "#Importing crossvalidation metrics and Gridsearch\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Importing wrapper to use XGB with Gridsearch\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#Importing plotting packages (optional)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "#Defining general modules used in the classification\n",
    "\n",
    "random.seed(2016)\n",
    "np.random.seed(2016)\n",
    "\n",
    "\n",
    "def natural_key(string_):\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_)]\n",
    "\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def get_importance(gbm, features):\n",
    "    create_feature_map(features)\n",
    "    importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "    return importance\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "\n",
    "def print_features_importance(imp):\n",
    "    for i in range(len(imp)):\n",
    "        print(\"# \" + str(imp[i][1]))\n",
    "        print('output.remove(\\'' + imp[i][0] + '\\')')\n",
    "\n",
    "\n",
    "def mat_to_pandas(path):\n",
    "    mat = loadmat(path)\n",
    "    names = mat['dataStruct'].dtype.names\n",
    "    ndata = {n: mat['dataStruct'][n][0, 0] for n in names}\n",
    "    sequence = -1\n",
    "    if 'sequence' in names:\n",
    "        sequence = mat['dataStruct']['sequence']\n",
    "    return pd.DataFrame(ndata['data'], columns=ndata['channelIndices'][0]), sequence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modules to read train and test data.\n",
    "\n",
    "\n",
    "def create_simple_csv_train(patient_id):\n",
    "\n",
    "    out = open(\"simple_train_\" + str(patient_id) +\"-short\" + \".csv\", \"w\")\n",
    "    out.write(\"Id,sequence_id,patient_id\")\n",
    "    for i in range(16):\n",
    "        out.write(\",avg_\" + str(i) + \",peak1_\" + str(i) + \",peak2_\" + str(i) + \",peak3_\" + str(i) +\",peak4_\" + str(i))\n",
    "    out.write(\",file_size,result\\n\")\n",
    "\n",
    "    # TRAIN (0)\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(\"./data/train_\" + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "    print ('train files'+ str(patient_id), len(files))    \n",
    "    pos1=0\n",
    "    neg1=0\n",
    "    sequence_id = int(patient_id)*1000\n",
    "    total = 0\n",
    "    seq1=0\n",
    "    for fl in files:\n",
    "        total += 1\n",
    "        # print('Go for ' + fl)\n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        result = int(arr[2])\n",
    "        new_id = patient*100000 + id\n",
    "        try:\n",
    "            tables, sequence_from_mat = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        out_str += str(new_id) + \",\" + str(sequence_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])       \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            mean = tables[f].mean()\n",
    "            \n",
    "            yf1 = fft(tables[f])\n",
    "            fftpeak=2.0/sizesignal * np.abs(yf1[0:sizesignal/2])\n",
    " \n",
    "            numberofbands=4\n",
    "\n",
    "            sizeband=20/numberofbands\n",
    "\n",
    "#           for i in range(numberofbands)\n",
    "          \n",
    "            peak1=fftpeak[0:5].mean()            \n",
    "            peak2=fftpeak[5:10].mean()          \n",
    "            peak3=fftpeak[10:15].mean()\n",
    "            peak4=fftpeak[15:20].mean()\n",
    "            \n",
    "            out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4)\n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \",\" + str(result) + \"\\n\"\n",
    "        #print(sequence_from_mat)\n",
    "        #print(type(sequence_from_mat))\n",
    "        seq1=int(sequence_from_mat[0][0][0][0])\n",
    "        print(total, seq1)\n",
    "        if (total % 6 == 0) and (seq1==6):\n",
    "            if result != 0:\n",
    "                pos1 += 1\n",
    "                print('Positive ocurrence sequence finished', pos1)\n",
    "            else:\n",
    "                neg1 += 1\n",
    "                print('Negative ocurrence sequence finished', neg1)\n",
    "                \n",
    "            sequence_id += 1\n",
    "            print ('sequence',sequence_id)\n",
    "\n",
    "    out.write(out_str)\n",
    "    \n",
    "    out.close()\n",
    "    print('Train CSV for patient {} has been completed...'.format(patient_id))\n",
    "\n",
    "\n",
    "def create_simple_csv_test(patient_id):\n",
    "\n",
    "    # TEST\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(\"./data/test_\" + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "    print ('test files'+ str(patient_id), len(files))    \n",
    "    out = open(\"simple_test_\" + str(patient_id) +\"-short\" + \".csv\", \"w\")\n",
    "    out.write(\"Id,patient_id\")\n",
    "    for i in range(16):\n",
    "        out.write(\",avg_\" + str(i) + \",peak1_\" + str(i) + \",peak2_\" + str(i) + \",peak3_\" + str(i) +\",peak4_\" + str(i))\n",
    "    out.write(\",file_size\\n\")\n",
    "    for fl in files:\n",
    "        # print('Go for ' + fl)\n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        new_id = patient*100000 + id\n",
    "        try:\n",
    "            tables, sequence_from_mat = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        out_str += str(new_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])           \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            mean = tables[f].mean()\n",
    "            \n",
    "            yf1 = fft(tables[f])\n",
    "            fftpeak=2.0/sizesignal * np.abs(yf1[0:sizesignal/2])\n",
    "\n",
    "            numberofbands=4\n",
    "\n",
    "            sizeband=20/numberofbands\n",
    "\n",
    "#           for i in range(numberofbands)\n",
    "          \n",
    "            peak1=fftpeak[0:5].mean()            \n",
    "            peak2=fftpeak[5:10].mean()          \n",
    "            peak3=fftpeak[10:15].mean()\n",
    "            peak4=fftpeak[15:20].mean()\n",
    "            \n",
    "            out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4)\n",
    "                        \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \"\\n\"\n",
    "        # break\n",
    "\n",
    "    out.write(out_str)\n",
    "    out.close()\n",
    "    print('Test CSV for patient {} has been completed...'.format(patient_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the main section. It is ordered as follows (it will change with each iteration, including comments format):\n",
    "#\n",
    "#      1. Variables and parameters for XGB training.\n",
    "#      2. GridSearch.\n",
    "#      3. XGB training (not linked to GRidsearch ouput yet)\n",
    "#\n",
    "#\n",
    "\n",
    "def run_kfold(nfolds, train, test, features, target, random_state=2016):\n",
    "\n",
    "    # Parameters and variables used later by xgb (not to be mixed with XGBClassifier used for GridSearch)\n",
    "    \n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "    yfull_train = dict()\n",
    "    yfull_test = copy.deepcopy(test[['Id']].astype(object))\n",
    "    \n",
    "    num_fold = 0\n",
    "    eta = 0.2\n",
    "    max_depth = 5\n",
    "    subsample = 0.7\n",
    "    colsample_bytree = 0.8\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": eta,\n",
    "        \"tree_method\": 'exact',\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "        \"gamma\": 0.3,\n",
    "        \"min_child_weight\": 1\n",
    "    }\n",
    "\n",
    "\n",
    "    unique_sequences = np.array(train['sequence_id'].unique())\n",
    "    print('unique sequences', unique_sequences)\n",
    "    \n",
    "    #This section is largely tentative. Contains variables that are not used rigth now (but could). Is left here\n",
    "    #in case you find a need for them. I will use them in the next iterations of the code.\n",
    "    \n",
    "    groups1=np.fix(unique_sequences/1000)\n",
    "    \n",
    "    groups2=groups1.astype(int)\n",
    "    print('groups', groups2)\n",
    "    \n",
    "    #Here there are a set of options for the split.\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    test1=gkf.split(unique_sequences, groups=groups2)\n",
    "    test2=gkf.split(unique_sequences, groups=groups2)\n",
    "    \n",
    "    kf = KFold(len(unique_sequences), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    \n",
    "    #Here starts the GRid search code. Most of it is not used for this script version. Again, still testing.\n",
    "    #For grid search this script is using GroupShuffleSplit using the sequences as groups(iterator is assigned to 'custom_cv'\n",
    "    #before performing the Grid Search)\n",
    "\n",
    "    num_fold1=0\n",
    "    param_test1 = {'max_depth': [3,5]}\n",
    "#    param_test1 = {'max_depth': [3,5,7,9], 'min_child_weight':[1,3,5,7]}\n",
    "#    param_test1 = {'max_depth': [3,5,7,9], 'gamma':[i/10.0 for i in range(0,5)]}\n",
    "#    param_test1 = { 'subsample':[i/10.0 for i in range(6,10)],'colsample_bytree':[i/10.0 for i in range(6,10)]}  \n",
    "#    param_test1 = { 'subsample':[i/10.0 for i in range(6,10)],'colsample_bytree':[i/10.0 for i in range(6,10)]}      \n",
    "\n",
    "    for train_seq_index1, test_seq_index1 in kf:\n",
    "        num_fold1 += 1\n",
    "        print('this is creation of Kfold iterator')\n",
    "        print('Start fold {} from {}'.format(num_fold1, nfolds))\n",
    "    \n",
    "        train_seq1 = unique_sequences[train_seq_index1]\n",
    "        valid_seq1 = unique_sequences[test_seq_index1]\n",
    "\n",
    "        train_index = train[train['sequence_id'].isin(train_seq1)].index.values\n",
    "        test_index = train[train['sequence_id'].isin(valid_seq1)].index.values\n",
    "\n",
    "        print(train_index, type(train_index))\n",
    "        print(test_index, type(test_index))\n",
    "        \n",
    "        train_index_group.append(train_index)\n",
    "        test_index_group.append(test_index)\n",
    "    \n",
    "    \n",
    "    print('train index group',train_index_group)\n",
    "    \n",
    "#   custom_cv = [(train_index_group[i], test_index_group[i]) for i in range(0,6) ]\n",
    "\n",
    "    number_groups=int(len(unique_sequences)/nfolds)\n",
    "    print('groups',len(unique_sequences),number_groups)\n",
    "    custom_cv=GroupShuffleSplit(n_splits=nfolds, test_size=0.2, random_state=0)\n",
    "#   custom_cv=lpgo.split(train[features], train[target], groups=train['sequence_id'])\n",
    "\n",
    "    print('custom cv', custom_cv)\n",
    "                   \n",
    "\n",
    "#    Scaling and PCA fro Grid search only\n",
    "\n",
    "\n",
    "    scaler1 = MinMaxScaler() \n",
    "    \n",
    "    train_features=train[features]\n",
    "    train_target=train[target]\n",
    "            \n",
    "    train_scaled=pd.DataFrame(scaler1.fit_transform(train_features), columns=train_features.columns, index=train_features.index)\n",
    "\n",
    "\n",
    "    pcatest=KernelPCA(n_components=30, kernel='poly')\n",
    "    train_features_f=pd.DataFrame(pcatest.fit_transform(train_scaled), index=train_scaled.index)\n",
    "\n",
    "\n",
    "    dmfeatures=train_features_f\n",
    "    dmtarget=train_target\n",
    "    \n",
    "#   Grid search\n",
    "    \n",
    "    classifier1=XGBClassifier( learning_rate =0.2, n_estimators=1000, max_depth=5,\n",
    "        min_child_weight=1, gamma=0.3, subsample=0.7, colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = classifier1, param_grid = param_test1, scoring='roc_auc',n_jobs=-1,iid=False, cv=custom_cv)\n",
    "  \n",
    "    \n",
    "    gsearch1.fit(dmfeatures,dmtarget,groups=train['sequence_id'])\n",
    "\n",
    "    print('best parameters, scores')    \n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "\n",
    "\n",
    "    \n",
    "#   XGB training   \n",
    "\n",
    "#   Here starts the XGB training. Is not being fed the parameters found above with Grid Search.\n",
    "#   Some parameter optimization has been done manually. When you check the final score is rather\n",
    "#   encouraging (72%) which has been attained by tweaking the hyper parameters with grid search manually\n",
    "#   However, is being done on only part of the samples. Next is to apply this same code and manually \n",
    "#   change the same hyper parameters with the total data set. That takes more time.\n",
    "    \n",
    "    \n",
    "    for train_seq_index, test_seq_index in kf:\n",
    "        num_fold += 1\n",
    "        print('Start fold {} from {}'.format(num_fold, nfolds))\n",
    "        train_seq = unique_sequences[train_seq_index]\n",
    "        valid_seq = unique_sequences[test_seq_index]\n",
    "        print('Length of train people: {}'.format(len(train_seq)))\n",
    "        print('Length of valid people: {}'.format(len(valid_seq)))\n",
    "\n",
    "        X_train, X_valid = train[train['sequence_id'].isin(train_seq)][features], train[train['sequence_id'].isin(valid_seq)][features]\n",
    "        y_train, y_valid = train[train['sequence_id'].isin(train_seq)][target], train[train['sequence_id'].isin(valid_seq)][target]\n",
    "        X_test = test[features]\n",
    "\n",
    "        print('Length train:', len(X_train))\n",
    "        print('Length valid:', len(X_valid))\n",
    "        \n",
    "#       Scaling for PCA\n",
    "        \n",
    "        scaler = MinMaxScaler()   \n",
    "        \n",
    "        Xtrain_scaled=pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        Xvalid_scaled=pd.DataFrame(scaler.fit_transform(X_valid), columns=X_valid.columns, index=X_valid.index )\n",
    "\n",
    "        Xtest_scaled=pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)\n",
    "        \n",
    "\n",
    "        \n",
    "#       PCA transformation \n",
    "        pcatest=KernelPCA(n_components=30, kernel='poly')\n",
    "        X_train_f=pd.DataFrame(pcatest.fit_transform(Xtrain_scaled), index=Xtrain_scaled.index)\n",
    "        X_valid_f=pd.DataFrame(pcatest.fit_transform(Xvalid_scaled), index=Xvalid_scaled.index)\n",
    "\n",
    "        X_test_f=pd.DataFrame(pcatest.fit_transform(Xtest_scaled))\n",
    "\n",
    "\n",
    "        y_train_f=y_train\n",
    "        y_valid_f=y_valid\n",
    "\n",
    "        \n",
    "#       Preparation for XGB training\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_f, y_train_f)\n",
    "        dvalid = xgb.DMatrix(X_valid_f, y_valid_f)\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]       \n",
    "        \n",
    "        gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n",
    "        yhat = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "\n",
    "#       Each time store portion of precicted data in train predicted values\n",
    "\n",
    "        for i in range(len(X_valid_f.index)):\n",
    "            yfull_train[X_valid_f.index[i]] = yhat[i]\n",
    "            \n",
    "        print(\"Validating...\")\n",
    "        check = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "        score = roc_auc_score(y_valid.tolist(), check)\n",
    "        print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "        print(\"Predict test set...\")\n",
    "        test_prediction1 = gbm.predict(xgb.DMatrix(X_test_f), ntree_limit=gbm.best_iteration+1)\n",
    "        yfull_test['kfold_' + str(num_fold)] = test_prediction1\n",
    "        \n",
    "              \n",
    "\n",
    "    # Copy dict to list\n",
    "    train_res = []\n",
    "    \n",
    "    for i in range(len(train.index)):\n",
    "        train_res.append(yfull_train[i])\n",
    "\n",
    "    score = roc_auc_score(train[target], np.array(train_res))\n",
    "    print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "    # Find mean for KFolds on test\n",
    "    merge = []\n",
    "    for i in range(1, nfolds+1):\n",
    "        merge.append('kfold_' + str(i))\n",
    "    yfull_test['mean'] = yfull_test[merge].mean(axis=1)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return yfull_test['mean'].values, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These modules are self-explanatory. They are used by other modules in reading data and creating final submission. \n",
    "# they are largely fixed.\n",
    "\n",
    "def create_submission(score, test, prediction):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('File,Class\\n')\n",
    "    total = 0\n",
    "    for id in test['Id']:\n",
    "        patient = id // 100000\n",
    "        fid = id % 100000\n",
    "        str1 = str(patient) + '_' + str(fid) + '.mat' + ',' + str(prediction[total])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values)\n",
    "    testval = list(test.columns.values)\n",
    "    output = intersect(trainval, testval)\n",
    "    output.remove('Id')\n",
    "    # output.remove('file_size')\n",
    "    return sorted(output)\n",
    "\n",
    "\n",
    "def read_test_train():\n",
    "    print(\"Load train.csv...\")\n",
    "    train1 = pd.read_csv(\"simple_train_1-short.csv\")\n",
    "    train2 = pd.read_csv(\"simple_train_2-short.csv\")\n",
    "    train3 = pd.read_csv(\"simple_train_3-short.csv\")\n",
    "    train = pd.concat([train1, train2, train3])\n",
    "    # Remove all zeroes files\n",
    "    train = train[train['file_size'] > 55000].copy()\n",
    "    # Shuffle rows since they are ordered\n",
    "    train = train.iloc[np.random.permutation(len(train))]\n",
    "    # Reset broken index\n",
    "    train = train.reset_index()\n",
    "    print(\"Load test.csv...\")\n",
    "    test1 = pd.read_csv(\"simple_test_1-short.csv\")\n",
    "    test2 = pd.read_csv(\"simple_test_2-short.csv\")\n",
    "    test3 = pd.read_csv(\"simple_test_3-short.csv\")\n",
    "    test = pd.concat([test1, test2, test3])\n",
    "    print(\"Process tables...\")\n",
    "    features = get_features(train, test)\n",
    "    return train, test, features\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  This section creates the train and test files, then reads them and concatenates them\n",
    "#  in two large train and test file.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    if 1:\n",
    "        # Do reading and processing of MAT files in parallel\n",
    "        p = dict()\n",
    "        p[1] = Process(target=create_simple_csv_train, args=(1,))\n",
    "        p[1].start()\n",
    "        p[2] = Process(target=create_simple_csv_train, args=(2,))\n",
    "        p[2].start()\n",
    "        p[3] = Process(target=create_simple_csv_train, args=(3,))\n",
    "        p[3].start()\n",
    "        p[4] = Process(target=create_simple_csv_test, args=(1,))\n",
    "        p[4].start()\n",
    "        p[5] = Process(target=create_simple_csv_test, args=(2,))\n",
    "        p[5].start()\n",
    "        p[6] = Process(target=create_simple_csv_test, args=(3,))\n",
    "        p[6].start()\n",
    "        p[1].join()\n",
    "        p[2].join()\n",
    "        p[3].join()\n",
    "        p[4].join()\n",
    "        p[5].join()\n",
    "        p[6].join()\n",
    "    train, test, features = read_test_train()\n",
    "    print('Length of train: ', len(train))\n",
    "    print('Length of test: ', len(test))\n",
    "    print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: 0.6\n",
      "Load train.csv...\n",
      "Load test.csv...\n",
      "Process tables...\n",
      "Length of train:  486\n",
      "Length of test:  303\n",
      "Features [82]: ['avg_0', 'avg_1', 'avg_10', 'avg_11', 'avg_12', 'avg_13', 'avg_14', 'avg_15', 'avg_2', 'avg_3', 'avg_4', 'avg_5', 'avg_6', 'avg_7', 'avg_8', 'avg_9', 'file_size', 'patient_id', 'peak1_0', 'peak1_1', 'peak1_10', 'peak1_11', 'peak1_12', 'peak1_13', 'peak1_14', 'peak1_15', 'peak1_2', 'peak1_3', 'peak1_4', 'peak1_5', 'peak1_6', 'peak1_7', 'peak1_8', 'peak1_9', 'peak2_0', 'peak2_1', 'peak2_10', 'peak2_11', 'peak2_12', 'peak2_13', 'peak2_14', 'peak2_15', 'peak2_2', 'peak2_3', 'peak2_4', 'peak2_5', 'peak2_6', 'peak2_7', 'peak2_8', 'peak2_9', 'peak3_0', 'peak3_1', 'peak3_10', 'peak3_11', 'peak3_12', 'peak3_13', 'peak3_14', 'peak3_15', 'peak3_2', 'peak3_3', 'peak3_4', 'peak3_5', 'peak3_6', 'peak3_7', 'peak3_8', 'peak3_9', 'peak4_0', 'peak4_1', 'peak4_10', 'peak4_11', 'peak4_12', 'peak4_13', 'peak4_14', 'peak4_15', 'peak4_2', 'peak4_3', 'peak4_4', 'peak4_5', 'peak4_6', 'peak4_7', 'peak4_8', 'peak4_9']\n"
     ]
    }
   ],
   "source": [
    "# This section only loads and concatenates the data from existing files. Unless you need to add 'features'\n",
    "# or modify columns, this is the section to run to read the data.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    \n",
    "    train, test, features = read_test_train()\n",
    "    print('Length of train: ', len(train))\n",
    "    print('Length of test: ', len(test))\n",
    "    print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique sequences [3014 3006 2005 1006 2007 3012 2004 2000 1003 2001 2014 1013 3000 1008 3007\n",
      " 1004 2006 3008 3009 1017 3013 1014 3005 2015 1002 2012 1009 1001 2003 2008\n",
      " 1000 3001 2002 2009 1005 2010 2013 3015 1012 3004 2011 3003 3017 3002 3010\n",
      " 1010 2016 1007 2017 1011 1015 3016 3011 1016]\n",
      "groups [3 3 2 1 2 3 2 2 1 2 2 1 3 1 3 1 2 3 3 1 3 1 3 2 1 2 1 1 2 2 1 3 2 2 1 2 2\n",
      " 3 1 3 2 3 3 3 3 1 2 1 2 1 1 3 3 1]\n",
      "XGBoost params. ETA: 0.2, MAX_DEPTH: 5, SUBSAMPLE: 0.7, COLSAMPLE_BY_TREE: 0.8\n",
      "this is creation of Kfold iterator\n",
      "Start fold 1 from 6\n",
      "[  0   2   4   5   8   9  10  11  12  13  17  18  19  21  22  24  25  26\n",
      "  28  29  30  31  32  33  35  36  37  38  39  40  41  42  43  45  46  47\n",
      "  48  49  50  51  52  53  55  56  57  59  60  61  62  63  64  66  67  70\n",
      "  71  72  73  74  75  76  77  78  80  81  82  84  85  86  87  89  90  91\n",
      "  92  93  94  96  97  99 100 101 102 103 105 106 108 111 112 114 115 116\n",
      " 117 118 120 122 123 124 125 127 129 131 132 133 134 135 136 138 140 142\n",
      " 143 146 147 148 149 150 151 152 153 154 155 156 158 159 160 161 163 164\n",
      " 165 167 168 169 170 172 173 174 176 178 179 180 181 182 183 184 186 187\n",
      " 188 189 190 192 193 194 195 196 197 198 199 200 201 203 204 205 206 207\n",
      " 208 209 210 212 213 214 215 216 217 218 220 221 222 223 224 225 226 227\n",
      " 228 229 230 231 233 235 236 237 238 240 241 242 244 247 248 249 250 251\n",
      " 252 253 257 259 260 261 263 265 266 267 268 269 270 272 273 275 276 278\n",
      " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 297\n",
      " 298 299 300 301 302 303 304 305 306 307 308 309 310 312 313 315 316 317\n",
      " 318 319 322 325 326 328 329 330 331 332 333 334 335 336 337 338 340 343\n",
      " 345 347 349 350 351 352 354 355 356 357 358 359 360 361 362 363 364 365\n",
      " 366 367 368 369 370 372 373 374 375 377 379 380 381 382 383 384 385 386\n",
      " 387 388 389 390 391 392 393 394 395 396 397 398 401 402 403 404 405 406\n",
      " 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424\n",
      " 425 427 428 429 432 433 434 435 436 437 438 439 440 441 442 444 445 446\n",
      " 447 448 449 450 452 454 455 456 457 458 459 460 461 462 463 464 466 467\n",
      " 468 469 470 471 472 473 474 475 476 478 480 482 483 484] <class 'numpy.ndarray'>\n",
      "[  1   3   6   7  14  15  16  20  23  27  34  44  54  58  65  68  69  79\n",
      "  83  88  95  98 104 107 109 110 113 119 121 126 128 130 137 139 141 144\n",
      " 145 157 162 166 171 175 177 185 191 202 211 219 232 234 239 243 245 246\n",
      " 254 255 256 258 262 264 271 274 277 296 311 314 320 321 323 324 327 339\n",
      " 341 342 344 346 348 353 371 376 378 399 400 426 430 431 443 451 453 465\n",
      " 477 479 481 485] <class 'numpy.ndarray'>\n",
      "this is creation of Kfold iterator\n",
      "Start fold 2 from 6\n",
      "[  1   3   4   5   6   7   8   9  10  11  12  14  15  16  17  20  21  22\n",
      "  23  24  25  26  27  28  30  32  33  34  36  38  39  40  41  42  43  44\n",
      "  45  46  48  49  50  51  52  53  54  55  56  57  58  59  60  61  63  64\n",
      "  65  66  67  68  69  70  71  72  73  75  76  77  78  79  80  82  83  84\n",
      "  85  86  88  89  90  91  92  93  94  95  96  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 121 122 124\n",
      " 125 126 127 128 129 130 133 135 137 138 139 140 141 142 143 144 145 146\n",
      " 148 149 150 151 152 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 169 170 171 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187\n",
      " 188 189 190 191 192 193 195 196 197 198 199 200 201 202 203 204 207 208\n",
      " 209 210 211 212 213 214 215 217 219 221 223 224 225 226 227 228 229 230\n",
      " 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 249 250\n",
      " 251 252 253 254 255 256 258 260 261 262 263 264 266 267 268 269 270 271\n",
      " 272 273 274 275 276 277 278 280 282 283 284 285 286 287 288 289 290 292\n",
      " 293 296 297 298 299 300 301 302 303 304 305 306 307 308 310 311 312 314\n",
      " 315 316 317 318 319 320 321 322 323 324 325 326 327 328 330 331 334 337\n",
      " 338 339 340 341 342 343 344 345 346 347 348 350 352 353 354 355 356 357\n",
      " 359 361 362 363 364 365 366 367 369 370 371 372 374 375 376 377 378 379\n",
      " 380 381 384 385 387 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 414 415 416 417 418 420 421 422\n",
      " 423 424 425 426 428 429 430 431 432 433 434 435 436 437 438 439 440 442\n",
      " 443 444 446 447 448 449 451 452 453 455 458 460 461 462 464 465 466 467\n",
      " 468 469 470 471 473 474 475 476 477 478 479 480 481 482 483 484 485] <class 'numpy.ndarray'>\n",
      "[  0   2  13  18  19  29  31  35  37  47  62  74  81  87  97 120 123 131\n",
      " 132 134 136 147 153 167 168 172 194 205 206 216 218 220 222 231 248 257\n",
      " 259 265 279 281 291 294 295 309 313 329 332 333 335 336 349 351 358 360\n",
      " 368 373 382 383 386 388 412 413 419 427 441 445 450 454 456 457 459 463\n",
      " 472] <class 'numpy.ndarray'>\n",
      "this is creation of Kfold iterator\n",
      "Start fold 3 from 6\n",
      "[  0   1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  41\n",
      "  42  43  44  46  47  49  51  53  54  55  56  57  58  59  60  61  62  63\n",
      "  64  65  66  67  68  69  71  74  75  78  79  81  82  83  84  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98 100 103 104 105 107 108 109 110 112\n",
      " 113 115 116 117 118 119 120 121 122 123 124 126 128 129 130 131 132 133\n",
      " 134 135 136 137 139 140 141 142 143 144 145 146 147 148 149 150 151 152\n",
      " 153 154 156 157 159 160 161 162 163 164 166 167 168 169 170 171 172 174\n",
      " 175 177 178 179 181 182 183 184 185 187 188 189 190 191 193 194 196 197\n",
      " 200 201 202 204 205 206 207 209 210 211 214 215 216 217 218 219 220 221\n",
      " 222 223 225 226 227 228 229 230 231 232 234 235 236 237 238 239 240 241\n",
      " 242 243 245 246 247 248 249 250 251 252 254 255 256 257 258 259 260 262\n",
      " 263 264 265 266 267 268 269 270 271 273 274 275 276 277 278 279 280 281\n",
      " 283 284 285 286 287 289 290 291 292 293 294 295 296 298 299 301 302 303\n",
      " 304 306 307 309 311 312 313 314 315 316 317 318 320 321 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 336 338 339 340 341 342 344 346 347\n",
      " 348 349 350 351 352 353 354 355 358 360 362 363 364 365 366 367 368 370\n",
      " 371 372 373 374 375 376 377 378 379 381 382 383 384 385 386 387 388 390\n",
      " 391 392 393 394 395 396 397 398 399 400 401 403 404 405 406 408 410 411\n",
      " 412 413 415 416 417 418 419 420 421 423 424 425 426 427 428 429 430 431\n",
      " 433 434 435 436 437 438 439 441 442 443 444 445 447 448 449 450 451 453\n",
      " 454 455 456 457 459 462 463 464 465 466 469 470 471 472 474 475 476 477\n",
      " 478 479 480 481 482 483 485] <class 'numpy.ndarray'>\n",
      "[  4  22  26  38  39  40  45  48  50  52  70  72  73  76  77  80  85  86\n",
      "  99 101 102 106 111 114 125 127 138 155 158 165 173 176 180 186 192 195\n",
      " 198 199 203 208 212 213 224 233 244 253 261 272 282 288 297 300 305 308\n",
      " 310 319 322 337 343 345 356 357 359 361 369 380 389 402 407 409 414 422\n",
      " 432 440 446 452 458 460 461 467 468 473 484] <class 'numpy.ndarray'>\n",
      "this is creation of Kfold iterator\n",
      "Start fold 4 from 6\n",
      "[  0   1   2   3   4   6   7   9  10  12  13  14  15  16  17  18  19  20\n",
      "  22  23  25  26  27  28  29  31  34  35  37  38  39  40  41  44  45  47\n",
      "  48  50  52  53  54  55  56  58  59  60  62  63  65  67  68  69  70  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  91\n",
      "  93  94  95  97  98  99 100 101 102 104 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 119 120 121 122 123 124 125 126 127 128 130 131 132 133\n",
      " 134 136 137 138 139 140 141 142 143 144 145 146 147 149 150 151 152 153\n",
      " 154 155 157 158 159 160 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 185 186 187 189 190 191 192 193 194\n",
      " 195 197 198 199 201 202 203 204 205 206 207 208 210 211 212 213 216 217\n",
      " 218 219 220 222 223 224 225 226 228 229 231 232 233 234 235 236 237 238\n",
      " 239 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
      " 258 259 261 262 263 264 265 266 269 270 271 272 273 274 275 276 277 279\n",
      " 280 281 282 284 287 288 289 291 292 293 294 295 296 297 298 299 300 301\n",
      " 302 303 304 305 308 309 310 311 312 313 314 315 316 317 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 335 336 337 339 340 341 342\n",
      " 343 344 345 346 347 348 349 351 353 354 355 356 357 358 359 360 361 362\n",
      " 364 367 368 369 370 371 373 374 376 378 379 380 381 382 383 384 385 386\n",
      " 388 389 390 391 393 395 396 397 399 400 402 403 404 405 407 409 410 411\n",
      " 412 413 414 415 416 417 418 419 422 423 424 426 427 428 429 430 431 432\n",
      " 433 434 436 438 439 440 441 442 443 444 445 446 447 448 450 451 452 453\n",
      " 454 455 456 457 458 459 460 461 463 464 465 467 468 469 472 473 474 477\n",
      " 479 481 482 483 484 485] <class 'numpy.ndarray'>\n",
      "[  5   8  11  21  24  30  32  33  36  42  43  46  49  51  57  61  64  66\n",
      "  71  90  92  96 103 105 118 129 135 148 156 161 183 184 188 196 200 209\n",
      " 214 215 221 227 230 240 260 267 268 278 283 285 286 290 306 307 318 334\n",
      " 338 350 352 363 365 366 372 375 377 387 392 394 398 401 406 408 420 421\n",
      " 425 435 437 449 462 466 470 471 475 476 478 480] <class 'numpy.ndarray'>\n",
      "this is creation of Kfold iterator\n",
      "Start fold 5 from 6\n",
      "[  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  42  43  44  45  46  47  48  49  50  51  52  54  56  57  58\n",
      "  59  61  62  64  65  66  68  69  70  71  72  73  74  75  76  77  78  79\n",
      "  80  81  82  83  85  86  87  88  89  90  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118\n",
      " 119 120 121 123 125 126 127 128 129 130 131 132 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 147 148 153 154 155 156 157 158 159 161 162 163\n",
      " 165 166 167 168 169 171 172 173 174 175 176 177 179 180 181 182 183 184\n",
      " 185 186 188 191 192 193 194 195 196 198 199 200 202 203 204 205 206 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 226 227\n",
      " 228 230 231 232 233 234 235 237 239 240 241 243 244 245 246 247 248 249\n",
      " 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 267 268 270\n",
      " 271 272 274 275 276 277 278 279 281 282 283 284 285 286 288 290 291 293\n",
      " 294 295 296 297 300 301 303 304 305 306 307 308 309 310 311 313 314 318\n",
      " 319 320 321 322 323 324 325 327 329 331 332 333 334 335 336 337 338 339\n",
      " 341 342 343 344 345 346 348 349 350 351 352 353 354 356 357 358 359 360\n",
      " 361 362 363 364 365 366 368 369 370 371 372 373 374 375 376 377 378 380\n",
      " 382 383 384 386 387 388 389 392 394 395 396 398 399 400 401 402 404 405\n",
      " 406 407 408 409 410 411 412 413 414 417 418 419 420 421 422 423 425 426\n",
      " 427 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 445 446\n",
      " 449 450 451 452 453 454 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 470 471 472 473 475 476 477 478 479 480 481 484 485] <class 'numpy.ndarray'>\n",
      "[  9  28  41  53  55  60  63  67  84  91 116 122 124 133 146 149 150 151\n",
      " 152 160 164 170 178 187 189 190 197 201 207 225 229 236 238 242 250 266\n",
      " 269 273 280 287 289 292 298 299 302 312 315 316 317 326 328 330 340 347\n",
      " 355 367 379 381 385 390 391 393 397 403 415 416 424 428 444 447 448 455\n",
      " 469 474 482 483] <class 'numpy.ndarray'>\n",
      "this is creation of Kfold iterator\n",
      "Start fold 6 from 6\n",
      "[  0   1   2   3   4   5   6   7   8   9  11  13  14  15  16  18  19  20\n",
      "  21  22  23  24  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  76  77  79\n",
      "  80  81  83  84  85  86  87  88  90  91  92  95  96  97  98  99 101 102\n",
      " 103 104 105 106 107 109 110 111 113 114 116 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 141 144 145\n",
      " 146 147 148 149 150 151 152 153 155 156 157 158 160 161 162 164 165 166\n",
      " 167 168 170 171 172 173 175 176 177 178 180 183 184 185 186 187 188 189\n",
      " 190 191 192 194 195 196 197 198 199 200 201 202 203 205 206 207 208 209\n",
      " 211 212 213 214 215 216 218 219 220 221 222 224 225 227 229 230 231 232\n",
      " 233 234 236 238 239 240 242 243 244 245 246 248 250 253 254 255 256 257\n",
      " 258 259 260 261 262 264 265 266 267 268 269 271 272 273 274 277 278 279\n",
      " 280 281 282 283 285 286 287 288 289 290 291 292 294 295 296 297 298 299\n",
      " 300 302 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
      " 321 322 323 324 326 327 328 329 330 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 355 356 357 358 359\n",
      " 360 361 363 365 366 367 368 369 371 372 373 375 376 377 378 379 380 381\n",
      " 382 383 385 386 387 388 389 390 391 392 393 394 397 398 399 400 401 402\n",
      " 403 406 407 408 409 412 413 414 415 416 419 420 421 422 424 425 426 427\n",
      " 428 430 431 432 435 437 440 441 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 465 466 467 468 469 470 471\n",
      " 472 473 474 475 476 477 478 479 480 481 482 483 484 485] <class 'numpy.ndarray'>\n",
      "[ 10  12  17  25  56  59  75  78  82  89  93  94 100 108 112 115 117 140\n",
      " 142 143 154 159 163 169 174 179 181 182 193 204 210 217 223 226 228 235\n",
      " 237 241 247 249 251 252 263 270 275 276 284 293 301 303 304 325 331 354\n",
      " 362 364 370 374 384 395 396 404 405 410 411 417 418 423 429 433 434 436\n",
      " 438 439 442 464] <class 'numpy.ndarray'>\n",
      "train index group [array([  0,   2,   4,   5,   8,   9,  10,  11,  12,  13,  17,  18,  19,\n",
      "        21,  22,  24,  25,  26,  28,  29,  30,  31,  32,  33,  35,  36,\n",
      "        37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,  49,  50,\n",
      "        51,  52,  53,  55,  56,  57,  59,  60,  61,  62,  63,  64,  66,\n",
      "        67,  70,  71,  72,  73,  74,  75,  76,  77,  78,  80,  81,  82,\n",
      "        84,  85,  86,  87,  89,  90,  91,  92,  93,  94,  96,  97,  99,\n",
      "       100, 101, 102, 103, 105, 106, 108, 111, 112, 114, 115, 116, 117,\n",
      "       118, 120, 122, 123, 124, 125, 127, 129, 131, 132, 133, 134, 135,\n",
      "       136, 138, 140, 142, 143, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "       154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 167, 168, 169,\n",
      "       170, 172, 173, 174, 176, 178, 179, 180, 181, 182, 183, 184, 186,\n",
      "       187, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200,\n",
      "       201, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215,\n",
      "       216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
      "       230, 231, 233, 235, 236, 237, 238, 240, 241, 242, 244, 247, 248,\n",
      "       249, 250, 251, 252, 253, 257, 259, 260, 261, 263, 265, 266, 267,\n",
      "       268, 269, 270, 272, 273, 275, 276, 278, 279, 280, 281, 282, 283,\n",
      "       284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 297,\n",
      "       298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "       312, 313, 315, 316, 317, 318, 319, 322, 325, 326, 328, 329, 330,\n",
      "       331, 332, 333, 334, 335, 336, 337, 338, 340, 343, 345, 347, 349,\n",
      "       350, 351, 352, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "       364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 377, 379,\n",
      "       380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
      "       393, 394, 395, 396, 397, 398, 401, 402, 403, 404, 405, 406, 407,\n",
      "       408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
      "       421, 422, 423, 424, 425, 427, 428, 429, 432, 433, 434, 435, 436,\n",
      "       437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450,\n",
      "       452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466,\n",
      "       467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 478, 480, 482,\n",
      "       483, 484]), array([  1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  14,  15,\n",
      "        16,  17,  20,  21,  22,  23,  24,  25,  26,  27,  28,  30,  32,\n",
      "        33,  34,  36,  38,  39,  40,  41,  42,  43,  44,  45,  46,  48,\n",
      "        49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n",
      "        63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  75,  76,\n",
      "        77,  78,  79,  80,  82,  83,  84,  85,  86,  88,  89,  90,  91,\n",
      "        92,  93,  94,  95,  96,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "       106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
      "       119, 121, 122, 124, 125, 126, 127, 128, 129, 130, 133, 135, 137,\n",
      "       138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151,\n",
      "       152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
      "       166, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n",
      "       196, 197, 198, 199, 200, 201, 202, 203, 204, 207, 208, 209, 210,\n",
      "       211, 212, 213, 214, 215, 217, 219, 221, 223, 224, 225, 226, 227,\n",
      "       228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
      "       242, 243, 244, 245, 246, 247, 249, 250, 251, 252, 253, 254, 255,\n",
      "       256, 258, 260, 261, 262, 263, 264, 266, 267, 268, 269, 270, 271,\n",
      "       272, 273, 274, 275, 276, 277, 278, 280, 282, 283, 284, 285, 286,\n",
      "       287, 288, 289, 290, 292, 293, 296, 297, 298, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 310, 311, 312, 314, 315, 316, 317,\n",
      "       318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 330, 331,\n",
      "       334, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
      "       350, 352, 353, 354, 355, 356, 357, 359, 361, 362, 363, 364, 365,\n",
      "       366, 367, 369, 370, 371, 372, 374, 375, 376, 377, 378, 379, 380,\n",
      "       381, 384, 385, 387, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "       398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410,\n",
      "       411, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 426,\n",
      "       428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440,\n",
      "       442, 443, 444, 446, 447, 448, 449, 451, 452, 453, 455, 458, 460,\n",
      "       461, 462, 464, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475,\n",
      "       476, 477, 478, 479, 480, 481, 482, 483, 484, 485]), array([  0,   1,   2,   3,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  23,  24,  25,  27,  28,\n",
      "        29,  30,  31,  32,  33,  34,  35,  36,  37,  41,  42,  43,  44,\n",
      "        46,  47,  49,  51,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n",
      "        62,  63,  64,  65,  66,  67,  68,  69,  71,  74,  75,  78,  79,\n",
      "        81,  82,  83,  84,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
      "        96,  97,  98, 100, 103, 104, 105, 107, 108, 109, 110, 112, 113,\n",
      "       115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143,\n",
      "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157,\n",
      "       159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172,\n",
      "       174, 175, 177, 178, 179, 181, 182, 183, 184, 185, 187, 188, 189,\n",
      "       190, 191, 193, 194, 196, 197, 200, 201, 202, 204, 205, 206, 207,\n",
      "       209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "       225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 237, 238,\n",
      "       239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252,\n",
      "       254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 266, 267,\n",
      "       268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
      "       283, 284, 285, 286, 287, 289, 290, 291, 292, 293, 294, 295, 296,\n",
      "       298, 299, 301, 302, 303, 304, 306, 307, 309, 311, 312, 313, 314,\n",
      "       315, 316, 317, 318, 320, 321, 323, 324, 325, 326, 327, 328, 329,\n",
      "       330, 331, 332, 333, 334, 335, 336, 338, 339, 340, 341, 342, 344,\n",
      "       346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 358, 360, 362,\n",
      "       363, 364, 365, 366, 367, 368, 370, 371, 372, 373, 374, 375, 376,\n",
      "       377, 378, 379, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
      "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405,\n",
      "       406, 408, 410, 411, 412, 413, 415, 416, 417, 418, 419, 420, 421,\n",
      "       423, 424, 425, 426, 427, 428, 429, 430, 431, 433, 434, 435, 436,\n",
      "       437, 438, 439, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451,\n",
      "       453, 454, 455, 456, 457, 459, 462, 463, 464, 465, 466, 469, 470,\n",
      "       471, 472, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485]), array([  0,   1,   2,   3,   4,   6,   7,   9,  10,  12,  13,  14,  15,\n",
      "        16,  17,  18,  19,  20,  22,  23,  25,  26,  27,  28,  29,  31,\n",
      "        34,  35,  37,  38,  39,  40,  41,  44,  45,  47,  48,  50,  52,\n",
      "        53,  54,  55,  56,  58,  59,  60,  62,  63,  65,  67,  68,  69,\n",
      "        70,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "        84,  85,  86,  87,  88,  89,  91,  93,  94,  95,  97,  98,  99,\n",
      "       100, 101, 102, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
      "       115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
      "       130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158,\n",
      "       159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
      "       173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187,\n",
      "       189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 201, 202, 203,\n",
      "       204, 205, 206, 207, 208, 210, 211, 212, 213, 216, 217, 218, 219,\n",
      "       220, 222, 223, 224, 225, 226, 228, 229, 231, 232, 233, 234, 235,\n",
      "       236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
      "       250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 262, 263,\n",
      "       264, 265, 266, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279,\n",
      "       280, 281, 282, 284, 287, 288, 289, 291, 292, 293, 294, 295, 296,\n",
      "       297, 298, 299, 300, 301, 302, 303, 304, 305, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325,\n",
      "       326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 339, 340,\n",
      "       341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 353, 354, 355,\n",
      "       356, 357, 358, 359, 360, 361, 362, 364, 367, 368, 369, 370, 371,\n",
      "       373, 374, 376, 378, 379, 380, 381, 382, 383, 384, 385, 386, 388,\n",
      "       389, 390, 391, 393, 395, 396, 397, 399, 400, 402, 403, 404, 405,\n",
      "       407, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 422,\n",
      "       423, 424, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 438,\n",
      "       439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 450, 451, 452,\n",
      "       453, 454, 455, 456, 457, 458, 459, 460, 461, 463, 464, 465, 467,\n",
      "       468, 469, 472, 473, 474, 477, 479, 481, 482, 483, 484, 485]), array([  0,   1,   2,   3,   4,   5,   6,   7,   8,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
      "        27,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
      "        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  54,  56,\n",
      "        57,  58,  59,  61,  62,  64,  65,  66,  68,  69,  70,  71,  72,\n",
      "        73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  85,  86,\n",
      "        87,  88,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99, 100,\n",
      "       101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
      "       114, 115, 117, 118, 119, 120, 121, 123, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "       144, 145, 147, 148, 153, 154, 155, 156, 157, 158, 159, 161, 162,\n",
      "       163, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177,\n",
      "       179, 180, 181, 182, 183, 184, 185, 186, 188, 191, 192, 193, 194,\n",
      "       195, 196, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210,\n",
      "       211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "       224, 226, 227, 228, 230, 231, 232, 233, 234, 235, 237, 239, 240,\n",
      "       241, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255,\n",
      "       256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 268, 270,\n",
      "       271, 272, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285,\n",
      "       286, 288, 290, 291, 293, 294, 295, 296, 297, 300, 301, 303, 304,\n",
      "       305, 306, 307, 308, 309, 310, 311, 313, 314, 318, 319, 320, 321,\n",
      "       322, 323, 324, 325, 327, 329, 331, 332, 333, 334, 335, 336, 337,\n",
      "       338, 339, 341, 342, 343, 344, 345, 346, 348, 349, 350, 351, 352,\n",
      "       353, 354, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n",
      "       368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 380, 382,\n",
      "       383, 384, 386, 387, 388, 389, 392, 394, 395, 396, 398, 399, 400,\n",
      "       401, 402, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414,\n",
      "       417, 418, 419, 420, 421, 422, 423, 425, 426, 427, 429, 430, 431,\n",
      "       432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445,\n",
      "       446, 449, 450, 451, 452, 453, 454, 456, 457, 458, 459, 460, 461,\n",
      "       462, 463, 464, 465, 466, 467, 468, 470, 471, 472, 473, 475, 476,\n",
      "       477, 478, 479, 480, 481, 484, 485]), array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  13,  14,\n",
      "        15,  16,  18,  19,  20,  21,  22,  23,  24,  26,  27,  28,  29,\n",
      "        30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
      "        43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "        57,  58,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
      "        71,  72,  73,  74,  76,  77,  79,  80,  81,  83,  84,  85,  86,\n",
      "        87,  88,  90,  91,  92,  95,  96,  97,  98,  99, 101, 102, 103,\n",
      "       104, 105, 106, 107, 109, 110, 111, 113, 114, 116, 118, 119, 120,\n",
      "       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
      "       134, 135, 136, 137, 138, 139, 141, 144, 145, 146, 147, 148, 149,\n",
      "       150, 151, 152, 153, 155, 156, 157, 158, 160, 161, 162, 164, 165,\n",
      "       166, 167, 168, 170, 171, 172, 173, 175, 176, 177, 178, 180, 183,\n",
      "       184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 195, 196, 197,\n",
      "       198, 199, 200, 201, 202, 203, 205, 206, 207, 208, 209, 211, 212,\n",
      "       213, 214, 215, 216, 218, 219, 220, 221, 222, 224, 225, 227, 229,\n",
      "       230, 231, 232, 233, 234, 236, 238, 239, 240, 242, 243, 244, 245,\n",
      "       246, 248, 250, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262,\n",
      "       264, 265, 266, 267, 268, 269, 271, 272, 273, 274, 277, 278, 279,\n",
      "       280, 281, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
      "       295, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310,\n",
      "       311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
      "       324, 326, 327, 328, 329, 330, 332, 333, 334, 335, 336, 337, 338,\n",
      "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
      "       352, 353, 355, 356, 357, 358, 359, 360, 361, 363, 365, 366, 367,\n",
      "       368, 369, 371, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382,\n",
      "       383, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 397, 398,\n",
      "       399, 400, 401, 402, 403, 406, 407, 408, 409, 412, 413, 414, 415,\n",
      "       416, 419, 420, 421, 422, 424, 425, 426, 427, 428, 430, 431, 432,\n",
      "       435, 437, 440, 441, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
      "       452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 465,\n",
      "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
      "       479, 480, 481, 482, 483, 484, 485])]\n",
      "groups 54 9\n",
      "custom cv GroupShuffleSplit(n_splits=6, random_state=0, test_size=0.2, train_size=None)\n",
      "best parameters, scores\n",
      "[mean: 0.99116, std: 0.00475, params: {'max_depth': 3}, mean: 0.99088, std: 0.00640, params: {'max_depth': 5}] {'max_depth': 3} 0.991162720448\n",
      "Start fold 1 from 6\n",
      "Length of train people: 45\n",
      "Length of valid people: 9\n",
      "Length train: 392\n",
      "Length valid: 94\n",
      "[0]\ttrain-auc:0.938564\teval-auc:0.825\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodolfoxps/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py:662: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[6]\ttrain-auc:0.999941\teval-auc:0.889583\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.889583\n",
      "Predict test set...\n",
      "Start fold 2 from 6\n",
      "Length of train people: 45\n",
      "Length of valid people: 9\n",
      "Length train: 413\n",
      "Length valid: 73\n",
      "[0]\ttrain-auc:0.957497\teval-auc:0.605751\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3]\ttrain-auc:0.994793\teval-auc:0.642788\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.642788\n",
      "Predict test set...\n",
      "Start fold 3 from 6\n",
      "Length of train people: 45\n",
      "Length of valid people: 9\n",
      "Length train: 403\n",
      "Length valid: 83\n",
      "[0]\ttrain-auc:0.962344\teval-auc:0.408805\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[6]\ttrain-auc:0.998072\teval-auc:0.598742\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.598742\n",
      "Predict test set...\n",
      "Start fold 4 from 6\n",
      "Length of train people: 45\n",
      "Length of valid people: 9\n",
      "Length train: 402\n",
      "Length valid: 84\n",
      "[0]\ttrain-auc:0.916823\teval-auc:0.463235\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[5]\ttrain-auc:0.999696\teval-auc:0.630882\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.630882\n",
      "Predict test set...\n",
      "Start fold 5 from 6\n",
      "Length of train people: 45\n",
      "Length of valid people: 9\n",
      "Length train: 410\n",
      "Length valid: 76\n",
      "[0]\ttrain-auc:0.976702\teval-auc:0.618189\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1]\ttrain-auc:0.992554\teval-auc:0.657452\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.657452\n",
      "Predict test set...\n",
      "Start fold 6 from 6\n",
      "Length of train people: 45\n",
      "Length of valid people: 9\n",
      "Length train: 410\n",
      "Length valid: 76\n",
      "[0]\ttrain-auc:0.929102\teval-auc:0.772354\n",
      "Multiple eval metrics have been passed: 'eval-auc' will be used for early stopping.\n",
      "\n",
      "Will train until eval-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[7]\ttrain-auc:0.999871\teval-auc:0.882691\n",
      "\n",
      "Validating...\n",
      "Check error value: 0.882691\n",
      "Predict test set...\n",
      "Check error value: 0.723864\n",
      "Training time: 0.17 minutes\n",
      "Writing submission:  submission_0.723864482502_2016-10-25-18-03.csv\n",
      "version 3acb\n"
     ]
    }
   ],
   "source": [
    "#Here is where the actual prediction and submission are executed and created respectively.\n",
    "\n",
    "prediction, score = run_kfold(6, train, test, features, 'result')\n",
    "\n",
    "create_submission(score, test, prediction)\n",
    "print('version 3acb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Package Import and Simple Module definition\n",
    "\n",
    "Modified from ZFTurbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.io import loadmat\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import Process\n",
    "import copy\n",
    "\n",
    "#Importing old and new Kfold\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold as NewKF\n",
    "from sklearn.model_selection import StratifiedKFold as StratKF\n",
    "\n",
    "#Importing GroupKfold, only available since version 0.18\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "#Importing function for scaling data before PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Importing PCA packages\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "#Importing FFT package\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "#Importing crossvalidation metrics and Gridsearch\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Importing wrapper to use XGB with Gridsearch\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#Importing plotting packages (optional)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "#Oversampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "\n",
    "#Defining general modules used in the classification\n",
    "\n",
    "random.seed(2016)\n",
    "np.random.seed(2016)\n",
    "\n",
    "\n",
    "def natural_key(string_):\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_)]\n",
    "\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def get_importance(gbm, features):\n",
    "    create_feature_map(features)\n",
    "    importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "    importance = sorted(importance.items(), key=itemgetter(1), reverse=True)\n",
    "    return importance\n",
    "\n",
    "\n",
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "\n",
    "def print_features_importance(imp):\n",
    "    for i in range(len(imp)):\n",
    "        print(\"# \" + str(imp[i][1]))\n",
    "        print('output.remove(\\'' + imp[i][0] + '\\')')\n",
    "\n",
    "\n",
    "def mat_to_pandas(path):\n",
    "    mat = loadmat(path)\n",
    "    names = mat['dataStruct'].dtype.names\n",
    "    ndata = {n: mat['dataStruct'][n][0, 0] for n in names}\n",
    "    samp_freq = ndata['iEEGsamplingRate'][0, 0]\n",
    "    sequence = -1\n",
    "    if 'sequence' in names:\n",
    "        sequence = mat['dataStruct']['sequence']\n",
    "    return pd.DataFrame(ndata['data'], columns=ndata['channelIndices'][0]), sequence, samp_freq\n",
    "\n",
    "def create_submission(score, test, prediction, feature_model, short_size, new_test):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + 'model_'+str(feature_model)+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'_'+ str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('File,Class\\n')\n",
    "    total = 0\n",
    "    for id in test['Id']:\n",
    "        patient = id // 100000\n",
    "        fid = id % 100000\n",
    "        str1 = 'new_' + str(patient) + '_' + str(fid) + '.mat' + ',' + str(prediction[total])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def get_features(train, test):\n",
    "    trainval = list(train.columns.values)\n",
    "    testval = list(test.columns.values)\n",
    "    output = intersect(trainval, testval)\n",
    "    output.remove('Id')\n",
    "    # output.remove('file_size')\n",
    "    return sorted(output)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Creating Features and Saving to CSV Files\n",
    "\n",
    "#### One file per patient per test/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modules to read train and test data.\n",
    "#Short_dataset can be False or TRue. It decides whether to use the lon or short sample size.\n",
    "\n",
    "def create_simple_csv_train(patient_id, feature_model, num_features, short_dataset=False, new_test=False):\n",
    "    \n",
    "    feature_file='_'+str(feature_model)+'_short_'+str(short_dataset)+'_new_test_'+ str(new_test)\n",
    "    \n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/train_\"\n",
    "    else:\n",
    "        source_dir=\"./train_\"\n",
    "    \n",
    "    new_label=''\n",
    "    if new_test:\n",
    "        \n",
    "        new_label='_new'\n",
    "\n",
    "    out = open(\"simple_train_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,sequence_id,patient_id,\")\n",
    "  \n",
    "    columns=''\n",
    "    for i in range(16):\n",
    "        for j in range(num_features):\n",
    "            columns+= 'ch_'+str(i)+'_'+\"band_\"+str(j)+\",\"        \n",
    "\n",
    "    out.write(columns+\"file_size,result\\n\")\n",
    "\n",
    "    # TRAIN (0)\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "    print ('train files'+ str(patient_id), len(files))    \n",
    "    pos1=0\n",
    "    neg1=0\n",
    "    sequence_id_pre = int(patient_id)*1000\n",
    "    sequence_id_inter = int(patient_id)*1000\n",
    "    total_pre = 0\n",
    "    total_inter=0\n",
    "    seq1=0\n",
    "    \n",
    "    new_train = pd.read_csv('train_and_test_data_labels_safe'+'.csv')\n",
    "    new_data = new_train['image']\n",
    "    \n",
    "    selection = new_train[new_train['safe'] == 1].drop('safe', axis=1)\n",
    "    \n",
    "    for fl in files:\n",
    "        \n",
    "        # print('Go for ' + fl)\n",
    "               \n",
    "        if os.path.basename(fl) not in selection['image'].values:\n",
    "            continue\n",
    "        \n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        result = int(arr[2])\n",
    "        \n",
    "        if result == 1:\n",
    "            \n",
    "            total_pre += 1\n",
    "            sequence_id=int(patient_id)*1000+int((total_pre-1) // 6) + int((total_inter-1) // 6) + 1\n",
    "\n",
    "            \n",
    "        elif result == 0:\n",
    "            \n",
    "            total_inter += 1            \n",
    "            sequence_id=int(patient_id)*1000+int((total_pre) // 6) + int((total_inter-1) // 6)\n",
    "\n",
    "        \n",
    "        new_id = int(patient*100000 + id)\n",
    "        try:\n",
    "            tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        print(sequence_id)\n",
    "        out_str += str(new_id) + \",\" + str(sequence_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])       \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f], out_str,feature_model, sizesignal, samp_freq)\n",
    "            \n",
    "            \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \",\" + str(result) + \"\\n\"\n",
    "        #print(sequence_from_mat)\n",
    "        #print(type(sequence_from_mat))\n",
    "        seq1=int(sequence_from_mat[0][0][0][0])\n",
    "        print('total preictal: ', total_pre,' total interictal: ', total_inter,' sequence local: ', seq1)\n",
    "        if (total_pre % 6 == 0) and result == 1:\n",
    "                pos1 += 1\n",
    "                print('Positive ocurrence sequence finished', pos1)\n",
    "                if (seq1==6):\n",
    "                    sequence_id_pre += 1\n",
    "                    print ('sequence preictal next',sequence_id_pre)\n",
    "        \n",
    "        if (total_inter % 6 == 0) and result == 0:                \n",
    "                neg1 += 1\n",
    "                print('Negative ocurrence sequence finished', neg1)\n",
    "                if (seq1==6):\n",
    "                    sequence_id_inter += 1\n",
    "                    print ('sequence interictal next',sequence_id_inter)\n",
    "\n",
    "    out.write(out_str)\n",
    "    \n",
    "    out.close()\n",
    "    print('Train CSV for patient {} has been completed...'.format(patient_id))\n",
    "\n",
    "\n",
    "def create_simple_csv_test(patient_id, feature_model, num_features, short_dataset=False, new_test=False):\n",
    "    \n",
    "    feature_file='_'+str(feature_model)+'_short_'+str(short_dataset)+'_new_test_'+str(new_test)\n",
    "    \n",
    "    if short_dataset:\n",
    "        \n",
    "        source_dir=\"./data/test_\"\n",
    "    else:\n",
    "        source_dir=\"./test_\"\n",
    "    \n",
    "    new_label=''\n",
    "    \n",
    "    if new_test:\n",
    "        \n",
    "        new_label=\"_new\"\n",
    "\n",
    "    # TEST\n",
    "    out_str = ''\n",
    "    files = sorted(glob.glob(source_dir + str(patient_id) + new_label + \"/*.mat\"), key=natural_key)\n",
    "    print ('test files'+ str(patient_id), len(files))    \n",
    "    out = open(\"simple_test_\" + str(patient_id) + feature_file + \".csv\", \"w\")\n",
    "    out.write(\"Id,patient_id,\")\n",
    "    \n",
    "    columns=''\n",
    "    for i in range(16):\n",
    "        for j in range(num_features):\n",
    "            columns+= 'ch_'+str(i)+'_'+\"band_\"+str(j)+\",\"        \n",
    "    \n",
    "    out.write(columns+\"file_size\\n\")\n",
    "    \n",
    "        \n",
    "    for fl in files:\n",
    "        print('Go for ' + fl)\n",
    "        id_str = os.path.basename(fl)[:-4]\n",
    "        arr = id_str.split(\"_\")\n",
    "        patient = int(arr[0])\n",
    "        id = int(arr[1])\n",
    "        new_id = int(patient*100000 + id)\n",
    "        try:\n",
    "            tables, sequence_from_mat, samp_freq = mat_to_pandas(fl)\n",
    "        except:\n",
    "            print('Some error here {}...'.format(fl))\n",
    "            continue\n",
    "        out_str += str(new_id) + \",\" + str(patient)\n",
    "\n",
    "        sizesignal=int(tables.shape[0])           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for f in sorted(list(tables.columns.values)):\n",
    "            \n",
    "            out_str=feature_eng(tables[f], out_str,feature_model, sizesignal, samp_freq)\n",
    "                        \n",
    "        out_str += \",\" + str(os.path.getsize(fl)) + \"\\n\"\n",
    "        # break\n",
    "\n",
    "    out.write(out_str)\n",
    "    out.close()\n",
    "    print('Test CSV for patient {} has been completed...'.format(patient_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_subset_train(feature_model):\n",
    "    \n",
    "   \n",
    "    \n",
    "#    filestotal = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "\n",
    "    \n",
    "    folder_list=['train_1','train_2','train_3']\n",
    "    \n",
    "    \n",
    "    \n",
    "    for folder_label in folder_list:\n",
    "        \n",
    "        short_size=False\n",
    "        \n",
    "        patient_id=int(list(filter(str.isdigit, folder_label))[0])\n",
    "        \n",
    "        new_test=True      \n",
    "        if len(folder_label)>7:\n",
    "  \n",
    "            var=6\n",
    "            \n",
    "        else:\n",
    "            var=7\n",
    "        \n",
    "        files = pd.read_csv('simple_'+ folder_label[0:var]+'_'+str(feature_model)+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "\n",
    "        files_index=pd.Series(files['sequence_id'], index=files['sequence_id'].index)\n",
    "\n",
    "        \n",
    "        files_unique =files.drop_duplicates(subset=['sequence_id'])\n",
    "        \n",
    "#       print(files_unique)\n",
    "        \n",
    "\n",
    "        files_1=files_unique[files_unique['result'] == 1]['sequence_id']\n",
    "        files_0=files_unique[files_unique['result'] == 0]['sequence_id']\n",
    "        \n",
    "        print(files_1.shape)\n",
    "        print(files_0.shape)\n",
    "    \n",
    "        n_samples_1=int(len(files_1)/10)\n",
    "        n_samples_0=int(len(files_0)/10)\n",
    "        \n",
    "\n",
    "    \n",
    "        files_1_rand=np.random.choice(files_1, size=n_samples_1)\n",
    "        files_0_rand=np.random.choice(files_0, size=n_samples_0)\n",
    "        \n",
    "        \n",
    "        files_1_rand_seq=files[files_index.isin(files_1_rand)]['Id']\n",
    "        files_0_rand_seq=files[files_index.isin(files_0_rand)]['Id']\n",
    "        \n",
    "        for file in files_1_rand_seq:\n",
    "        \n",
    "            id_file=str(file % 1000)\n",
    "            source_file='./'+folder_label+'/'+str(patient_id)+'_'+id_file+'_1.mat'\n",
    "            target_file='./data_random/' + folder_label+'/'+str(patient_id)+'_'+id_file+'_1.mat'\n",
    "            print(target_file)\n",
    "    \n",
    "            shutil.copyfile(source_file, target_file) \n",
    "\n",
    "        for file in files_0_rand_seq:\n",
    "        \n",
    "            id_file=str(file % 1000)\n",
    "            source_file='./'+folder_label+'/'+str(patient_id)+'_'+id_file+'_0.mat'\n",
    "            target_file='./data_random/' + folder_label+'/'+str(patient_id)+'_'+id_file+'_0.mat'\n",
    "            print(target_file)\n",
    "    \n",
    "            shutil.copyfile(source_file, target_file)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_model=3\n",
    "short_size=False\n",
    "num_features=6\n",
    "new_test=True\n",
    "\n",
    "\n",
    "create_subset_train(feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subset_test(feature_model):\n",
    "    \n",
    "   \n",
    "    \n",
    "#    filestotal = sorted(glob.glob(source_dir + str(patient_id) + \"/*.mat\"), key=natural_key)\n",
    "\n",
    "    \n",
    "    folder_list=['test_1_new','test_2_new','test_3_new']\n",
    "    \n",
    "    \n",
    "    \n",
    "    for folder_label in folder_list:\n",
    "        \n",
    "        short_size=False\n",
    "        \n",
    "        patient_id=int(list(filter(str.isdigit, folder_label))[0])\n",
    "        \n",
    "        new_test=True      \n",
    "        if len(folder_label)>7:\n",
    "  \n",
    "            var=6\n",
    "            \n",
    "        else:\n",
    "            var=7\n",
    "        \n",
    "        files = pd.read_csv('simple_'+ folder_label[0:var]+'_'+str(feature_model)+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "\n",
    "             \n",
    "\n",
    "        files_total=files['Id']\n",
    "\n",
    "    \n",
    "        n_samples=int(len(files_total)/10)\n",
    "\n",
    "    \n",
    "        files_rand=np.random.choice(files_total, size=n_samples)\n",
    "\n",
    "             \n",
    "        \n",
    "        for file in files_rand:\n",
    "        \n",
    "            id_file=str(file % 1000)\n",
    "            source_file='./'+folder_label+'/new_'+str(patient_id)+'_'+id_file+'.mat'\n",
    "            target_file='./data_random/' + folder_label+'/new_'+str(patient_id)+'_'+id_file+'.mat'\n",
    "            print(target_file)\n",
    "    \n",
    "            shutil.copyfile(source_file, target_file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_model=3\n",
    "short_size=False\n",
    "num_features=6\n",
    "new_test=True\n",
    "\n",
    "\n",
    "create_subset_test(feature_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#eng_number is the feature_value that has values 0,1,2,3... infinite, given by the list below.\n",
    "\n",
    "def feature_eng(data_sensor, out_str, eng_number, sizesignal, fs):\n",
    "\n",
    "                \n",
    "    yf1 = fft(data_sensor)\n",
    "    fftpeak=2/sizesignal * np.abs(yf1[0:sizesignal/2])\n",
    " \n",
    "    numberofbands=4\n",
    "\n",
    "    sizeband=20/numberofbands\n",
    "\n",
    "    if eng_number==3:\n",
    "        \n",
    "    \n",
    "        ###Band Frequency filtering###\n",
    "        from scipy.signal import cheby2, butter, lfilter\n",
    "\n",
    "        ##Frequency parameters##\n",
    "        #Start frequency#\n",
    "        fini = 7\n",
    "        #End frequency#\n",
    "        fend = 30\n",
    "        #Frequency band range#\n",
    "        frng = 4\n",
    "        #Frequency overlap#\n",
    "        fovr = 0\n",
    "    \n",
    "        #Frequency band generator#\n",
    "        fbands = [[f, f + frng] for f in range(fini, fend - fovr, frng - fovr)]\n",
    "    \n",
    "        #Filter order#\n",
    "        order = 5\n",
    "        #Filter bandstop attenuation (dB)#\n",
    "        attenuation = 20.0\n",
    "        #Nyquist frequency#\n",
    "        fnyq = fs / 2.0\n",
    "        \n",
    "\n",
    "        for fb in fbands:\n",
    "        \n",
    "            #Create butterworth bandpass filter#\n",
    "            #b, a = butter(order, fb  / fnyq, btype='band')\n",
    "            b, a = cheby2(order, attenuation, fb  / fnyq, btype='band')\n",
    "            \n",
    "            #Apply filter#\n",
    "            data_filter = lfilter(b, a, data_sensor)\n",
    "            \n",
    "            #Band pass 'power'#\n",
    "            band_pwr = np.square(data_filter)\n",
    "            \n",
    "            avg_band_pwr = band_pwr.mean()\n",
    "            \n",
    "            out_str += \",\" + str(avg_band_pwr)\n",
    "            \n",
    "\n",
    "    elif eng_number==2:\n",
    "        \n",
    "        mean = data_sensor.mean()\n",
    "        \n",
    "        peak1=fftpeak[0:3].mean()            \n",
    "        peak2=fftpeak[3:6].mean()          \n",
    "        peak3=fftpeak[6:9].mean()\n",
    "        peak4=fftpeak[9:12].mean()\n",
    "        peak5=fftpeak[12:15].mean()            \n",
    "        peak6=fftpeak[15:18].mean()          \n",
    "        peak7=fftpeak[18:21].mean()\n",
    "        peak8=fftpeak[21:24].mean()\n",
    "        peak9=fftpeak[24:27].mean()            \n",
    "        peak10=fftpeak[27:30].mean()          \n",
    "        peak11=fftpeak[30:33].mean()\n",
    "        peak12=fftpeak[33:36].mean()\n",
    "            \n",
    "        out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4) \\\n",
    "                    +\",\" + str(peak5) + \",\" + str(peak6) + \",\" + str(peak7) +\",\" + str(peak8)+ \",\" + str(peak9) \\\n",
    "                    +\",\" + str(peak10) + \",\" + str(peak11) +\",\" + str(peak12)\n",
    "    \n",
    "    elif eng_number==1:\n",
    "            \n",
    "        mean = data_sensor.mean()   \n",
    "        \n",
    "        peak1=fftpeak[0:5].mean()            \n",
    "        peak2=fftpeak[5:10].mean()          \n",
    "        peak3=fftpeak[10:15].mean()\n",
    "        peak4=fftpeak[15:20].mean()\n",
    "        \n",
    "        out_str += \",\" + str(mean)+ \",\" + str(peak1) + \",\" + str(peak2) + \",\" + str(peak3) +\",\" + str(peak4)\n",
    "    \n",
    "    elif eng_number==0:\n",
    "            \n",
    "        mean = data_sensor.mean()\n",
    "    \n",
    "        out_str += \",\" + str(mean)\n",
    "    \n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Reading Test and Train Feature Files and Concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_test_train(feature_model, short_size=False, new_test=False):\n",
    "    feature_file='_'+str(feature_model)\n",
    "    print(\"Load train.csv...\")\n",
    "    train1 = pd.read_csv('simple_train_1'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    train2 = pd.read_csv('simple_train_2'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    train3 = pd.read_csv('simple_train_3'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    train = pd.concat([train1, train2, train3])\n",
    "    # Remove all zeroes files\n",
    "    train = train[train['file_size'] > 55000].copy()\n",
    "    # Shuffle rows since they are ordered\n",
    "    train = train.iloc[np.random.permutation(len(train))]\n",
    "    # Reset broken index\n",
    "    train = train.reset_index()\n",
    "    print(\"Load test.csv...\")\n",
    "    test1 = pd.read_csv('simple_test_1'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    test2 = pd.read_csv('simple_test_2'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    test3 = pd.read_csv('simple_test_3'+feature_file+'_short_'+str(short_size)+'_new_test_'+str(new_test)+'.csv')\n",
    "    test = pd.concat([test1, test2, test3])\n",
    "    print(\"Process tables...\")\n",
    "    features = get_features(train, test)\n",
    "    return train, test, features\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Creation of Feature Files (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_model=3\n",
    "short_size=True\n",
    "num_features=6\n",
    "new_test=False\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    if 1:\n",
    "        # Do reading and processing of MAT files in parallel\n",
    "        p = dict()\n",
    "        p[1] = Process(target=create_simple_csv_train, args=(1,feature_model,num_features,short_size,new_test))\n",
    "        p[1].start()\n",
    "        p[2] = Process(target=create_simple_csv_train, args=(2,feature_model,num_features,short_size,new_test))\n",
    "        p[2].start()\n",
    "        p[3] = Process(target=create_simple_csv_train, args=(3,feature_model,num_features,short_size,new_test))\n",
    "        p[3].start()\n",
    "        p[4] = Process(target=create_simple_csv_test, args=(1,feature_model,num_features,short_size,new_test))\n",
    "        p[4].start()\n",
    "        p[5] = Process(target=create_simple_csv_test, args=(2,feature_model,num_features,short_size,new_test))\n",
    "        p[5].start()\n",
    "        p[6] = Process(target=create_simple_csv_test, args=(3,feature_model,num_features,short_size,new_test))\n",
    "        p[6].start()\n",
    "        p[1].join()\n",
    "        p[2].join()\n",
    "        p[3].join()\n",
    "        p[4].join()\n",
    "        p[5].join()\n",
    "        p[6].join()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_model=3\n",
    "short_size=True\n",
    "num_features=6\n",
    "new_test=False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('XGBoost: {}'.format(xgb.__version__))\n",
    "    \n",
    "    train, test, features = read_test_train(feature_model, short_size, new_test)\n",
    "    print('Length of train: ', len(train))\n",
    "    print('Length of test: ', len(test))\n",
    "    print('Features [{}]: {}'.format(len(features), sorted(features)))\n",
    "    \n",
    "#   print ('train',train['sequence_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_train_predict(nfolds, train, test, features, target, random_state=2016, PCAkey=False, PCAgraph=False,\n",
    "                      PCAkeyGS=False, SEQoriginal=False,\n",
    "                     Oversampling=False, GridSearch=False):\n",
    "\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "    \n",
    "    unique_seq = train.drop_duplicates(subset=['sequence_id'])\n",
    "    unique_seq_y = unique_seq['result'].values\n",
    "    \n",
    "    print('unique seq y', len(unique_seq_y) )\n",
    "    \n",
    "    n_samples=len(unique_seq_y)\n",
    "    print('length',n_samples)\n",
    "    unique_seq_X = np.zeros(n_samples)\n",
    "    \n",
    "    print('unique seq X', len(unique_seq_X)  )\n",
    "    \n",
    "    \n",
    "    print('train pre', train.shape) \n",
    "\n",
    "    yfull_train = dict()\n",
    "    yfull_test = copy.deepcopy(test[['Id']].astype(object))\n",
    "\n",
    "    unique_sequences = np.array(train['sequence_id'].unique())\n",
    "    print('unique sequences pre', unique_sequences.shape)\n",
    "\n",
    "    groups1=np.fix(unique_sequences/1000)\n",
    "    \n",
    "    groups2=groups1.astype(int)\n",
    "#    print('groups', groups2)\n",
    "    \n",
    "\n",
    "    \n",
    "    if SEQoriginal:\n",
    "        sequences_full=np.mod(train['sequence_id'].values,1000)\n",
    "        print('sequences full', sequences_full.shape)\n",
    "        unique_sequences2=np.mod(unique_sequences,1000)\n",
    "        unique_sequences_fold=pd.Series(sequences_full, index=train['sequence_id'].index)\n",
    "#        print('unique_sequences_fold', unique_sequences_fold)\n",
    "    \n",
    "        unique_sequences = np.unique(unique_sequences2)\n",
    "        print('unique sequences pre', unique_sequences.shape)\n",
    "\n",
    "    else:\n",
    "        unique_sequences_fold=pd.Series(train['sequence_id'], index=train['sequence_id'].index)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    test1=gkf.split(unique_sequences, groups=groups2)\n",
    "    test2=gkf.split(unique_sequences, groups=groups2)\n",
    "    \n",
    "    #random_state=random_state\n",
    "    print('unique sequences', unique_sequences.shape)\n",
    "#    splitKF = KFold(len(unique_sequences), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "#    kf = NewKF(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    kf = StratKF(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    num_fold = 0\n",
    "    num_fold1=0\n",
    "    \n",
    "    eta = 0.2\n",
    "    max_depth = 3\n",
    "    subsample = 0.9\n",
    "    colsample_bytree = 0.9\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": eta,\n",
    "        \"tree_method\": 'exact',\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "        \"gamma\": 0,\n",
    "        \"min_child_weight\": 3,\n",
    "        \"scale_pos_weight\":1,\n",
    "        \"seed\":27\n",
    "    }\n",
    "\n",
    "    \n",
    "#   Using best parameters to train model \n",
    "\n",
    "    if GridSearch:\n",
    "    \n",
    "        splitKF=kf.split(unique_seq_X, unique_seq_y)\n",
    "    \n",
    "        best_param=param_search_embedded(nfolds, features, target, splitKF, unique_sequences, \n",
    "                                         unique_sequences_fold, train, PCAkeyGS)\n",
    "    \n",
    "        print('after best_param', best_param)\n",
    "    \n",
    "    for train_seq_index, test_seq_index in kf.split(unique_seq_X, unique_seq_y):\n",
    "        num_fold += 1\n",
    "        print('Start fold {} from {}'.format(num_fold, nfolds))\n",
    "        train_seq = unique_sequences[train_seq_index]\n",
    "        valid_seq = unique_sequences[test_seq_index]\n",
    "        print('Length of train people: {}'.format(len(train_seq)))\n",
    "        print('Length of valid people: {}'.format(len(valid_seq)))\n",
    "        \n",
    "#        print('train_seq',train_seq)\n",
    "#        print('valid_seq',valid_seq)\n",
    "\n",
    "        X_train, X_valid = train[unique_sequences_fold.isin(train_seq)][features], train[unique_sequences_fold.isin(valid_seq)][features]\n",
    "        y_train, y_valid = train[unique_sequences_fold.isin(train_seq)][target], train[unique_sequences_fold.isin(valid_seq)][target]\n",
    "        X_test = test[features]\n",
    "        \n",
    "#        print('X_train',X_train)\n",
    "#        print('y_train',y_train)\n",
    "\n",
    "        print('Length train:', len(X_train))\n",
    "        print('Length valid:', len(X_valid))\n",
    "        \n",
    "#       Scaling for PCA\n",
    "\n",
    "\n",
    "        scaler = MinMaxScaler()   \n",
    "        \n",
    "        Xtrain_scaled=pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        Xvalid_scaled=pd.DataFrame(scaler.fit_transform(X_valid), columns=X_valid.columns, index=X_valid.index )\n",
    "\n",
    "        Xtest_scaled=pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "\n",
    "        if PCAgraph:\n",
    "            \n",
    "            pcatest=KernelPCA(n_components=20)\n",
    "            pcatest.fit(Xtrain_scaled)\n",
    "            var1=np.cumsum(np.round(pcatest.explained_variance_ratio_, decimals=4)*100)\n",
    "            f1 = plt.figure()\n",
    "            print(var1)\n",
    "            plt.plot(var1)\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "        if PCAkey:      \n",
    "        \n",
    "#       PCA transformation \n",
    "            pcatest=PCA(n_components=20)\n",
    "            X_train_f=pd.DataFrame(pcatest.fit_transform(Xtrain_scaled), index=Xtrain_scaled.index)\n",
    "            X_valid_f=pd.DataFrame(pcatest.fit_transform(Xvalid_scaled), index=Xvalid_scaled.index)\n",
    "\n",
    "            X_test_f=pd.DataFrame(pcatest.fit_transform(Xtest_scaled), index=Xtest_scaled.index)\n",
    "\n",
    "        else:\n",
    "            X_train_f=X_train\n",
    "            X_valid_f=X_valid\n",
    "            X_test_f=X_test\n",
    "                \n",
    "        \n",
    "        y_train_f=y_train\n",
    "        y_valid_f=y_valid\n",
    "    \n",
    "#       SMOTE oversampling\n",
    "        \n",
    "#        print('Original dataset shape {}'.format(Counter(y_train)))\n",
    "#        print('Original dataset shape {}'.format(Counter(X_train_f)))\n",
    "#        print(X_train_f)\n",
    "#        print(y_train)\n",
    "\n",
    "        if Oversampling:\n",
    "        \n",
    "            sm1 = SMOTETomek(random_state=42)\n",
    "            X_res,y_res = sm1.fit_sample(X_train_f,y_train)\n",
    "            X_train_f=pd.DataFrame(X_res, columns=X_train_f.columns)\n",
    "            y_train=pd.Series(y_res)\n",
    "        \n",
    "#        print('Resampled dataset shape {}'.format(Counter(y_train)))\n",
    "#        print(X_train_f)\n",
    "#        print(y_train)\n",
    "    \n",
    "    \n",
    "        \n",
    "#       Preparation for XGB training\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_f, y_train)\n",
    "        dvalid = xgb.DMatrix(X_valid_f, y_valid)\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]       \n",
    "        \n",
    "        gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n",
    "        yhat = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "\n",
    "#       Each time store portion of precicted data in train predicted values\n",
    "\n",
    "        for i in range(len(X_valid_f.index)):\n",
    "            yfull_train[X_valid_f.index[i]] = yhat[i]\n",
    "            \n",
    "        print(\"Validating...\")\n",
    "        check = gbm.predict(xgb.DMatrix(X_valid_f), ntree_limit=gbm.best_iteration+1)\n",
    "        score = roc_auc_score(y_valid.tolist(), check)\n",
    "        print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "        print(\"Predict test set...\")\n",
    "        test_prediction1 = gbm.predict(xgb.DMatrix(X_test_f), ntree_limit=gbm.best_iteration+1)\n",
    "        yfull_test['kfold_' + str(num_fold)] = test_prediction1\n",
    "        \n",
    "              \n",
    "\n",
    "    print('iteration finished')\n",
    "    # Copy dict to list\n",
    "    train_res = []\n",
    "    \n",
    "    for i in range(len(train.index)):\n",
    "        train_res.append(yfull_train[i])\n",
    "\n",
    "    score = roc_auc_score(train[target], np.array(train_res))\n",
    "    print('Check error value: {:.6f}'.format(score))\n",
    "\n",
    "    # Find mean for KFolds on test\n",
    "    merge = []\n",
    "    for i in range(1, nfolds+1):\n",
    "        merge.append('kfold_' + str(i))\n",
    "    yfull_test['mean'] = yfull_test[merge].mean(axis=1)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return yfull_test['mean'].values, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_param_search(nfolds, train, test, features, target, random_state=2016,  PCAkey=False, SEQoriginal=False):\n",
    "\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "\n",
    "    \n",
    "    yfull_train = dict()\n",
    "    yfull_test = copy.deepcopy(test[['Id']].astype(object))\n",
    "    print('train sequences',train['sequence_id'])\n",
    "    \n",
    "    \n",
    "    unique_sequences = np.array(train['sequence_id'].unique())\n",
    "#    print('unique sequences', unique_sequences, len(unique_sequences))\n",
    "    \n",
    "    groups1=np.fix(unique_sequences/1000)\n",
    "    \n",
    "    groups2=groups1.astype(int)\n",
    "#    print('groups', groups2)\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    test1=gkf.split(unique_sequences, groups=groups2)\n",
    "    test2=gkf.split(unique_sequences, groups=groups2)\n",
    "    \n",
    "    \n",
    "    if SEQoriginal:\n",
    "        sequences_full=np.mod(train['sequence_id'].values,1000)\n",
    "        unique_sequences2=np.mod(unique_sequences,1000)\n",
    "        unique_sequences_fold=pd.Series(sequences_full, index=train['sequence_id'].index)\n",
    "#        print('unique_sequences_fold', unique_sequences_fold)\n",
    "    \n",
    "        unique_sequences = np.unique(unique_sequences2)\n",
    "\n",
    "    else:\n",
    "        unique_sequences_fold=pd.Series(train['sequence_id'], index=train['sequence_id'].index)\n",
    "\n",
    "    \n",
    "    kf = KFold(len(unique_sequences), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    num_fold1=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#   param_test1 = {'max_depth': [3,5]}\n",
    "#   param_test1 = {'max_depth': [1,3,5,7,9], 'min_child_weight':[1,3,5,7]}\n",
    "#   param_test1 = {'gamma':[i/10.0 for i in range(0,7)]}\n",
    "#   param_test1 = { 'subsample':[i/10.0 for i in range(6,10)],'colsample_bytree':[i/10.0 for i in range(6,10)]}  \n",
    "#   param_test1 = {'max_depth': [1,3], 'min_child_weight':[6,7,8,9,10]}\n",
    "    param_test1 = {'scale_pos_weight':[1,2,3,4,5], 'max_delta_step':[0,1,2,3,4,5]}\n",
    "\n",
    "    for train_seq_index1, test_seq_index1 in kf:\n",
    "        num_fold1 += 1\n",
    "        print('this is creation of Kfold iterator')\n",
    "        print('Start fold {} from {}'.format(num_fold1, nfolds))\n",
    "    \n",
    "        train_seq1 = unique_sequences[train_seq_index1]\n",
    "        valid_seq1 = unique_sequences[test_seq_index1]\n",
    "        \n",
    "        print(train_seq1)\n",
    "        print(valid_seq1)\n",
    "\n",
    "        train_index = train[unique_sequences_fold.isin(train_seq1)].index.values\n",
    "        test_index = train[unique_sequences_fold.isin(valid_seq1)].index.values\n",
    "\n",
    "        print(train_index, type(train_index))\n",
    "        print(test_index, type(test_index))\n",
    "       \n",
    "        train_index_group.append(train_index)\n",
    "        test_index_group.append(test_index)\n",
    "        \n",
    "    \n",
    "    print('train index group',train_index_group)\n",
    "\n",
    "    custom_cv = [(train_index_group[i], test_index_group[i]) for i in range(0,3) ]\n",
    "    \n",
    "#    custom_cv=GroupShuffleSplit(n_splits=nfolds, test_size=0.5, random_state=0)\n",
    "\n",
    "#    custom_cv = list(zip(train_index_group, test_index_group))\n",
    "\n",
    "    print('custom cv', custom_cv)\n",
    "                   \n",
    "\n",
    "#    Scaling and PCA\n",
    "\n",
    "    if PCAkey:\n",
    "\n",
    "        scaler1 = MinMaxScaler() \n",
    "    \n",
    "        train_features=train[features]\n",
    "        train_target=train[target]\n",
    "            \n",
    "        train_scaled=pd.DataFrame(scaler1.fit_transform(train_features), columns=train_features.columns, index=train_features.index)\n",
    "\n",
    "\n",
    "        pcatest=KernelPCA(kernel='poly')\n",
    "        train_features_f=pd.DataFrame(pcatest.fit_transform(train_scaled), index=train_scaled.index)\n",
    "\n",
    "        dmfeatures=train_features_f\n",
    "        dmtarget=train_target        \n",
    "\n",
    "    else:\n",
    "    \n",
    "        dmfeatures=train[features]\n",
    "        dmtarget=train[target]\n",
    "    \n",
    "#   GridSearch\n",
    "    \n",
    "    classifier1=XGBClassifier( learning_rate =0.2, n_estimators=1000, max_depth=1,\n",
    "        min_child_weight=3, gamma=0, subsample=0.6, colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = classifier1, param_grid = param_test1, scoring='roc_auc',n_jobs=-1,iid=False, cv=custom_cv)\n",
    "  \n",
    "    \n",
    "#    gsearch1.fit(dmfeatures,dmtarget,groups=train['sequence_id'])\n",
    "    gsearch1.fit(dmfeatures,dmtarget)\n",
    "\n",
    "    print('best parameters, scores')    \n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "#    classifier2 = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def param_search_embedded(n_folds,features, target, kf, unique_sequences, unique_sequences_fold, train, \n",
    "                          PCAkey=False):\n",
    "\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 50\n",
    "    \n",
    "    train_index_group=[]\n",
    "    test_index_group=[]\n",
    "    \n",
    "    num_fold1=0\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#    param_test1 = {'max_depth': [3,5]}\n",
    "    param_test1 = {'max_depth': [1,3,5,7,9], 'min_child_weight':[1,3,5,7]}\n",
    "#   param_test1 = {'gamma':[i/10.0 for i in range(0,7)]}\n",
    "#   param_test1 = { 'subsample':[i/10.0 for i in range(6,10)],'colsample_bytree':[i/10.0 for i in range(6,10)]}  \n",
    "#   param_test1 = {'max_depth': [1,3], 'min_child_weight':[6,7,8,9,10]}\n",
    "#    param_test1 = {'scale_pos_weight':[1,2,3,4,5], 'max_delta_step':[0,1,2,3,4,5]}\n",
    "\n",
    "    for train_seq_index1, test_seq_index1 in kf:\n",
    "        num_fold1 += 1\n",
    "        print('this is creation of Kfold iterator')\n",
    "        print('Start fold {} from {}'.format(num_fold1, n_folds))\n",
    "    \n",
    "        train_seq1 = unique_sequences[train_seq_index1]\n",
    "        valid_seq1 = unique_sequences[test_seq_index1]\n",
    "        \n",
    "#        print(train_seq1)\n",
    "#        print(valid_seq1)\n",
    "\n",
    "        train_index = train[unique_sequences_fold.isin(train_seq1)].index.values\n",
    "        test_index = train[unique_sequences_fold.isin(valid_seq1)].index.values\n",
    "\n",
    "#        print(train_index, type(train_index))\n",
    "#        print(test_index, type(test_index))\n",
    "       \n",
    "        train_index_group.append(train_index)\n",
    "        test_index_group.append(test_index)\n",
    "        \n",
    "    \n",
    "#    print('train index group',train_index_group)\n",
    "\n",
    "    custom_cv = [(train_index_group[i], test_index_group[i]) for i in range(0,n_folds) ]\n",
    "    \n",
    "#    custom_cv=GroupShuffleSplit(n_splits=nfolds, test_size=0.5, random_state=0)\n",
    "\n",
    "#    custom_cv = list(zip(train_index_group, test_index_group))\n",
    "\n",
    "#    print('custom cv', custom_cv)\n",
    "                   \n",
    "\n",
    "#    Scaling and PCA\n",
    "\n",
    "    if PCAkey:\n",
    "\n",
    "        scaler1 = MinMaxScaler() \n",
    "    \n",
    "        train_features=train[features]\n",
    "        train_target=train[target]\n",
    "            \n",
    "        train_scaled=pd.DataFrame(scaler1.fit_transform(train_features), columns=train_features.columns, index=train_features.index)\n",
    "\n",
    "\n",
    "        pcatest=PCA(20)\n",
    "        train_features_f=pd.DataFrame(pcatest.fit_transform(train_scaled), index=train_scaled.index)\n",
    "\n",
    "        dmfeatures=train_features_f\n",
    "        dmtarget=train_target        \n",
    "\n",
    "    else:\n",
    "    \n",
    "        dmfeatures=train[features]\n",
    "        dmtarget=train[target]\n",
    "    \n",
    "#   GridSearch\n",
    "\n",
    "    print('start grid search')\n",
    "    \n",
    "    classifier1=XGBClassifier( learning_rate =0.2, n_estimators=1000, max_depth=3,\n",
    "        min_child_weight=1, gamma=0, subsample=0.9, colsample_bytree=0.9,\n",
    "        objective= 'binary:logistic', nthread=4, scale_pos_weight=2, seed=27)\n",
    "\n",
    "    gsearch1 = GridSearchCV(estimator = classifier1, param_grid = param_test1, scoring='roc_auc',n_jobs=-1,iid=False, cv=custom_cv)\n",
    "  \n",
    "    \n",
    "#    gsearch1.fit(dmfeatures,dmtarget,groups=train['sequence_id'])\n",
    "    gsearch1.fit(dmfeatures,dmtarget)\n",
    "\n",
    "    print('best parameters, scores')    \n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "#    classifier2 = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=500)\n",
    "\n",
    "    return gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_param_search(3, train, test, features, 'result', SEQoriginal=True, PCAkey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Training, Prediction and Creating Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction, score = run_train_predict(7, train, test, features, 'result', SEQoriginal=False, PCAkey=False, \n",
    "                                      PCAgraph=False, PCAkeyGS=False,\n",
    "                                      Oversampling=False, GridSearch=False)\n",
    "\n",
    "create_submission(score, test, prediction, feature_model, short_size, new_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
